{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "749e343f-640a-4383-89a4-3f753dd1a5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f80fab60-ae23-447e-98d0-71ff11588188",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (2.1.2+cu121)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (0.16.2+cu121)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (3.10.0)\n",
      "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (2.18.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (1.5.3)\n",
      "Collecting wandb (from -r requirements.txt (line 6))\n",
      "  Using cached wandb-0.22.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (2024.9.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (2.1.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->-r requirements.txt (line 2)) (1.23.5)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->-r requirements.txt (line 2)) (2.32.3)\n",
      "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision->-r requirements.txt (line 2))\n",
      "  Using cached pillow-11.3.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (4.55.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 3)) (2.9.0.post0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 4)) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 4)) (1.69.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 4)) (3.7)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 4)) (5.29.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 4)) (49.4.0)\n",
      "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 4)) (1.17.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 4)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r requirements.txt (line 4)) (3.1.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 5)) (2024.2)\n",
      "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 6)) (8.1.8)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb->-r requirements.txt (line 6))\n",
      "  Using cached gitpython-3.1.45-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 6)) (4.3.6)\n",
      "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 6)) (2.10.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 6)) (6.0.2)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 6)) (2.41.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 6))\n",
      "  Using cached gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3->wandb->-r requirements.txt (line 6)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3->wandb->-r requirements.txt (line 6)) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->-r requirements.txt (line 2)) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->-r requirements.txt (line 2)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->-r requirements.txt (line 2)) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->-r requirements.txt (line 2)) (2024.12.14)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 4)) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 6)) (5.0.2)\n",
      "Using cached wandb-0.22.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.9 MB)\n",
      "Using cached gitpython-3.1.45-py3-none-any.whl (208 kB)\n",
      "Using cached pillow-11.3.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n",
      "Using cached gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Installing collected packages: pillow, gitdb, gitpython, wandb\n",
      "\u001b[33m  WARNING: The scripts wandb and wb are installed in '/home/alopand/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed gitdb-4.0.12 gitpython-3.1.45 pillow-11.3.0 wandb-0.22.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1d24bb3-5db9-4893-a193-a0511c6115fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n",
      "100%|███████████████████████| 170498071/170498071 [00:03<00:00, 43412281.25it/s]\n",
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.02006 train_loss=1.5788 acc=0.4117 val_loss=1.3804 acc=0.5204\n",
      "[Epoch 002] lr=0.02011 train_loss=1.1618 acc=0.5868 val_loss=1.1168 acc=0.6162\n",
      "[Epoch 003] lr=0.02017 train_loss=0.9875 acc=0.6543 val_loss=1.0258 acc=0.6372\n",
      "[Epoch 004] lr=0.02023 train_loss=0.8747 acc=0.6965 val_loss=0.8149 acc=0.7136\n",
      "[Epoch 005] lr=0.02028 train_loss=0.7932 acc=0.7271 val_loss=0.7468 acc=0.7376\n",
      "[Epoch 006] lr=0.02034 train_loss=0.7315 acc=0.7495 val_loss=0.7934 acc=0.7322\n",
      "[Epoch 007] lr=0.02040 train_loss=0.6801 acc=0.7683 val_loss=0.7301 acc=0.7462\n",
      "[Epoch 008] lr=0.02045 train_loss=0.6353 acc=0.7852 val_loss=0.6536 acc=0.7810\n",
      "[Epoch 009] lr=0.02051 train_loss=0.5972 acc=0.7979 val_loss=0.5738 acc=0.7926\n",
      "[Epoch 010] lr=0.02057 train_loss=0.5555 acc=0.8123 val_loss=0.6144 acc=0.7870\n",
      "[Epoch 011] lr=0.02063 train_loss=0.5217 acc=0.8218 val_loss=0.6380 acc=0.7846\n",
      "[Epoch 012] lr=0.02068 train_loss=0.4960 acc=0.8315 val_loss=0.5433 acc=0.8148\n",
      "[Epoch 013] lr=0.02074 train_loss=0.4713 acc=0.8403 val_loss=0.6003 acc=0.7954\n",
      "[Epoch 014] lr=0.02080 train_loss=0.4496 acc=0.8481 val_loss=0.5623 acc=0.8146\n",
      "[Epoch 015] lr=0.02085 train_loss=0.4310 acc=0.8528 val_loss=0.5042 acc=0.8220\n",
      "[Epoch 016] lr=0.02091 train_loss=0.4116 acc=0.8619 val_loss=0.5307 acc=0.8200\n",
      "[Epoch 017] lr=0.02097 train_loss=0.4004 acc=0.8646 val_loss=0.5042 acc=0.8226\n",
      "[Epoch 018] lr=0.02102 train_loss=0.3851 acc=0.8688 val_loss=0.4195 acc=0.8562\n",
      "[Epoch 019] lr=0.02108 train_loss=0.3701 acc=0.8729 val_loss=0.4616 acc=0.8386\n",
      "[Epoch 020] lr=0.02114 train_loss=0.3592 acc=0.8786 val_loss=0.4685 acc=0.8392\n",
      "[Epoch 021] lr=0.02119 train_loss=0.3489 acc=0.8814 val_loss=0.4639 acc=0.8430\n",
      "[Epoch 022] lr=0.02125 train_loss=0.3426 acc=0.8829 val_loss=0.4893 acc=0.8358\n",
      "[Epoch 023] lr=0.02131 train_loss=0.3312 acc=0.8878 val_loss=0.5056 acc=0.8286\n",
      "[Epoch 024] lr=0.02136 train_loss=0.3227 acc=0.8904 val_loss=0.4249 acc=0.8584\n",
      "[Epoch 025] lr=0.02142 train_loss=0.3122 acc=0.8935 val_loss=0.5129 acc=0.8320\n",
      "[Epoch 026] lr=0.02148 train_loss=0.3098 acc=0.8944 val_loss=0.3891 acc=0.8698\n",
      "[Epoch 027] lr=0.02153 train_loss=0.3032 acc=0.8959 val_loss=0.4405 acc=0.8562\n",
      "[Epoch 028] lr=0.02159 train_loss=0.2952 acc=0.9001 val_loss=0.5124 acc=0.8340\n",
      "[Epoch 029] lr=0.02165 train_loss=0.2933 acc=0.9002 val_loss=0.4088 acc=0.8604\n",
      "[Epoch 030] lr=0.02170 train_loss=0.2862 acc=0.9035 val_loss=0.4073 acc=0.8586\n",
      "[Epoch 031] lr=0.02176 train_loss=0.2813 acc=0.9034 val_loss=0.4091 acc=0.8634\n",
      "[Epoch 032] lr=0.02182 train_loss=0.2735 acc=0.9057 val_loss=0.4292 acc=0.8570\n",
      "[Epoch 033] lr=0.02188 train_loss=0.2717 acc=0.9063 val_loss=0.4840 acc=0.8418\n",
      "[Epoch 034] lr=0.02193 train_loss=0.2673 acc=0.9084 val_loss=0.4637 acc=0.8442\n",
      "[Epoch 035] lr=0.02199 train_loss=0.2651 acc=0.9109 val_loss=0.4911 acc=0.8460\n",
      "[Epoch 036] lr=0.02205 train_loss=0.2639 acc=0.9104 val_loss=0.4123 acc=0.8590\n",
      "[Epoch 037] lr=0.02210 train_loss=0.2577 acc=0.9117 val_loss=0.3762 acc=0.8804\n",
      "[Epoch 038] lr=0.02216 train_loss=0.2532 acc=0.9144 val_loss=0.4687 acc=0.8478\n",
      "[Epoch 039] lr=0.02222 train_loss=0.2522 acc=0.9141 val_loss=0.4354 acc=0.8554\n",
      "[Epoch 040] lr=0.02227 train_loss=0.2432 acc=0.9169 val_loss=0.3953 acc=0.8730\n",
      "[Epoch 041] lr=0.02233 train_loss=0.2477 acc=0.9145 val_loss=0.5817 acc=0.8184\n",
      "[Epoch 042] lr=0.02239 train_loss=0.2408 acc=0.9172 val_loss=0.3822 acc=0.8712\n",
      "[Epoch 043] lr=0.02244 train_loss=0.2349 acc=0.9197 val_loss=0.3691 acc=0.8810\n",
      "[Epoch 044] lr=0.02250 train_loss=0.2354 acc=0.9193 val_loss=0.5077 acc=0.8394\n",
      "[Epoch 045] lr=0.02256 train_loss=0.2361 acc=0.9198 val_loss=0.4400 acc=0.8528\n",
      "[Epoch 046] lr=0.02261 train_loss=0.2342 acc=0.9190 val_loss=0.4096 acc=0.8654\n",
      "[Epoch 047] lr=0.02267 train_loss=0.2310 acc=0.9224 val_loss=0.4018 acc=0.8706\n",
      "[Epoch 048] lr=0.02273 train_loss=0.2281 acc=0.9202 val_loss=0.4032 acc=0.8656\n",
      "[Epoch 049] lr=0.02278 train_loss=0.2322 acc=0.9212 val_loss=0.3738 acc=0.8726\n",
      "[Epoch 050] lr=0.02284 train_loss=0.2240 acc=0.9232 val_loss=0.3966 acc=0.8718\n",
      "[Epoch 051] lr=0.02290 train_loss=0.2251 acc=0.9223 val_loss=0.3936 acc=0.8652\n",
      "[Epoch 052] lr=0.02295 train_loss=0.2177 acc=0.9255 val_loss=0.4239 acc=0.8592\n",
      "[Epoch 053] lr=0.02301 train_loss=0.2191 acc=0.9253 val_loss=0.4580 acc=0.8552\n",
      "[Epoch 054] lr=0.02307 train_loss=0.2171 acc=0.9262 val_loss=0.4234 acc=0.8600\n",
      "[Epoch 055] lr=0.02313 train_loss=0.2206 acc=0.9252 val_loss=0.3751 acc=0.8746\n",
      "[Epoch 056] lr=0.02318 train_loss=0.2126 acc=0.9268 val_loss=0.3614 acc=0.8762\n",
      "[Epoch 057] lr=0.02324 train_loss=0.2151 acc=0.9261 val_loss=0.3838 acc=0.8690\n",
      "[Epoch 058] lr=0.02330 train_loss=0.2151 acc=0.9267 val_loss=0.3665 acc=0.8804\n",
      "[Epoch 059] lr=0.02335 train_loss=0.2151 acc=0.9270 val_loss=0.4015 acc=0.8668\n",
      "[Epoch 060] lr=0.02341 train_loss=0.2119 acc=0.9277 val_loss=0.4689 acc=0.8498\n",
      "FINAL TEST: loss=0.3569  top1_acc=0.8853\n"
     ]
    }
   ],
   "source": [
    "!python -m vgg6_cifar.scripts.train_baseline \\\n",
    "  --data_dir ./data --out_dir ./runs/baseline \\\n",
    "  --epochs 60 --batch_size 128 --lr 0.1 --optimizer sgd --momentum 0.9 \\\n",
    "  --weight_decay 5e-4 --label_smoothing 0.0 \\\n",
    "  --aug_hflip --aug_crop --aug_cutout --aug_jitter --amp --seed 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bdb06e5-4fdc-4204-83bd-a475ab8bafb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved plots to: ./runs/baseline\n"
     ]
    }
   ],
   "source": [
    "!python -m vgg6_cifar.scripts.plot_curves \\\n",
    "  --metrics_csv ./runs/baseline/metrics.csv \\\n",
    "  --out_dir     ./runs/baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ade546-a2c9-4101-8fd2-14b65a98dc91",
   "metadata": {},
   "source": [
    "### Question 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b25dd76e-a16a-493b-b39a-51b52d0f4021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /home/alopand/.local/lib/python3.10/site-packages (0.22.2)\n",
      "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.8)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /home/alopand/.local/lib/python3.10/site-packages (from wandb) (3.1.45)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from wandb) (24.2)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.29.3)\n",
      "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.10.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.41.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/alopand/.local/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3->wandb) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.12.14)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2aa472f-813a-45d9-897b-3fcfe81bc99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/alopand/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login(key='6fcaa42db1a1f1abb9dc850add723ed01de1a49b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e5fe0a8-dd2c-425c-bb6a-307a481a86b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: python -m vgg6_cifar.scripts.train_experiment --data_dir ./data --out_dir ./runs/act_relu --activation relu --optimizer sgd --lr 0.1 --batch_size 128 --epochs 40 --use_bn --aug_hflip --aug_crop --aug_cutout --aug_jitter --amp --seed 42 --wandb\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m setting up run mzsq4m9a (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251014_204547-mzsq4m9a\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcomic-snowball-1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/mzsq4m9a\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.02507 train_loss=1.7133 acc=0.3635 val_loss=1.4728 acc=0.4582\n",
      "[Epoch 002] lr=0.02514 train_loss=1.3493 acc=0.5178 val_loss=1.2355 acc=0.5676\n",
      "[Epoch 003] lr=0.02521 train_loss=1.1348 acc=0.6030 val_loss=1.0249 acc=0.6366\n",
      "[Epoch 004] lr=0.02528 train_loss=0.9779 acc=0.6621 val_loss=0.8693 acc=0.6946\n",
      "[Epoch 005] lr=0.02536 train_loss=0.8820 acc=0.6971 val_loss=0.9087 acc=0.6822\n",
      "[Epoch 006] lr=0.02543 train_loss=0.7919 acc=0.7294 val_loss=0.8278 acc=0.7120\n",
      "[Epoch 007] lr=0.02550 train_loss=0.7177 acc=0.7560 val_loss=0.6946 acc=0.7574\n",
      "[Epoch 008] lr=0.02557 train_loss=0.6609 acc=0.7763 val_loss=0.6430 acc=0.7736\n",
      "[Epoch 009] lr=0.02564 train_loss=0.6126 acc=0.7929 val_loss=0.5906 acc=0.7910\n",
      "[Epoch 010] lr=0.02571 train_loss=0.5752 acc=0.8062 val_loss=0.6227 acc=0.7862\n",
      "[Epoch 011] lr=0.02578 train_loss=0.5433 acc=0.8177 val_loss=0.6380 acc=0.7856\n",
      "[Epoch 012] lr=0.02585 train_loss=0.5156 acc=0.8264 val_loss=0.4985 acc=0.8248\n",
      "[Epoch 013] lr=0.02592 train_loss=0.4924 acc=0.8335 val_loss=0.5301 acc=0.8106\n",
      "[Epoch 014] lr=0.02599 train_loss=0.4720 acc=0.8421 val_loss=0.6306 acc=0.7940\n",
      "[Epoch 015] lr=0.02607 train_loss=0.4530 acc=0.8468 val_loss=0.5706 acc=0.8034\n",
      "[Epoch 016] lr=0.02614 train_loss=0.4404 acc=0.8527 val_loss=0.5377 acc=0.8180\n",
      "[Epoch 017] lr=0.02621 train_loss=0.4236 acc=0.8586 val_loss=0.5541 acc=0.8096\n",
      "[Epoch 018] lr=0.02628 train_loss=0.4095 acc=0.8621 val_loss=0.6007 acc=0.7980\n",
      "[Epoch 019] lr=0.02635 train_loss=0.3942 acc=0.8681 val_loss=0.4692 acc=0.8376\n",
      "[Epoch 020] lr=0.02642 train_loss=0.3874 acc=0.8701 val_loss=0.5206 acc=0.8276\n",
      "[Epoch 021] lr=0.02649 train_loss=0.3784 acc=0.8745 val_loss=0.5469 acc=0.8174\n",
      "[Epoch 022] lr=0.02656 train_loss=0.3713 acc=0.8741 val_loss=0.5854 acc=0.8032\n",
      "[Epoch 023] lr=0.02663 train_loss=0.3526 acc=0.8804 val_loss=0.5905 acc=0.8130\n",
      "[Epoch 024] lr=0.02670 train_loss=0.3566 acc=0.8794 val_loss=0.4978 acc=0.8378\n",
      "[Epoch 025] lr=0.02678 train_loss=0.3456 acc=0.8835 val_loss=0.4893 acc=0.8362\n",
      "[Epoch 026] lr=0.02685 train_loss=0.3364 acc=0.8850 val_loss=0.4367 acc=0.8528\n",
      "[Epoch 027] lr=0.02692 train_loss=0.3295 acc=0.8896 val_loss=0.6008 acc=0.8110\n",
      "[Epoch 028] lr=0.02699 train_loss=0.3278 acc=0.8888 val_loss=0.4696 acc=0.8426\n",
      "[Epoch 029] lr=0.02706 train_loss=0.3201 acc=0.8916 val_loss=0.4002 acc=0.8606\n",
      "[Epoch 030] lr=0.02713 train_loss=0.3149 acc=0.8951 val_loss=0.4893 acc=0.8388\n",
      "[Epoch 031] lr=0.02720 train_loss=0.3092 acc=0.8959 val_loss=0.4349 acc=0.8524\n",
      "[Epoch 032] lr=0.02727 train_loss=0.3060 acc=0.8960 val_loss=0.4584 acc=0.8448\n",
      "[Epoch 033] lr=0.02734 train_loss=0.3026 acc=0.8980 val_loss=0.4486 acc=0.8520\n",
      "[Epoch 034] lr=0.02741 train_loss=0.3010 acc=0.8979 val_loss=0.4943 acc=0.8376\n",
      "[Epoch 035] lr=0.02749 train_loss=0.2954 acc=0.8997 val_loss=0.5341 acc=0.8310\n",
      "[Epoch 036] lr=0.02756 train_loss=0.2988 acc=0.8997 val_loss=0.5825 acc=0.8144\n",
      "[Epoch 037] lr=0.02763 train_loss=0.2874 acc=0.9026 val_loss=0.4268 acc=0.8558\n",
      "[Epoch 038] lr=0.02770 train_loss=0.2809 acc=0.9036 val_loss=0.5333 acc=0.8308\n",
      "[Epoch 039] lr=0.02777 train_loss=0.2853 acc=0.9028 val_loss=0.5515 acc=0.8212\n",
      "[Epoch 040] lr=0.02784 train_loss=0.2795 acc=0.9046 val_loss=0.4371 acc=0.8534\n",
      "FINAL TEST: loss=0.3742  top1_acc=0.8783\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc ▁▃▄▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇██████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▆▅▄▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc ▁▃▄▅▅▅▆▆▇▇▇▇▇▇▇▇▇▇█▇▇▇▇███▇███████▇▇█▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss █▆▅▄▄▄▃▃▂▂▃▂▂▃▂▂▂▂▁▂▂▂▂▂▂▁▂▁▁▂▁▁▁▂▂▂▁▂▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.8606\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 40\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.8783\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.02784\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.90456\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 0.27952\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.8534\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 0.43714\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mcomic-snowball-1\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/mzsq4m9a\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251014_204547-mzsq4m9a/logs\u001b[0m\n",
      "Running: python -m vgg6_cifar.scripts.train_experiment --data_dir ./data --out_dir ./runs/act_silu --activation silu --optimizer sgd --lr 0.1 --batch_size 128 --epochs 40 --use_bn --aug_hflip --aug_crop --aug_cutout --aug_jitter --amp --seed 42 --wandb\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m setting up run lw80elmf (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251014_205208-lw80elmf\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mabsurd-shape-2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/lw80elmf\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.02507 train_loss=1.5722 acc=0.4202 val_loss=1.2578 acc=0.5522\n",
      "[Epoch 002] lr=0.02514 train_loss=1.1208 acc=0.6024 val_loss=1.0599 acc=0.6368\n",
      "[Epoch 003] lr=0.02521 train_loss=0.9239 acc=0.6752 val_loss=0.8834 acc=0.6830\n",
      "[Epoch 004] lr=0.02528 train_loss=0.8130 acc=0.7181 val_loss=0.7673 acc=0.7192\n",
      "[Epoch 005] lr=0.02536 train_loss=0.7348 acc=0.7459 val_loss=0.7271 acc=0.7472\n",
      "[Epoch 006] lr=0.02543 train_loss=0.6900 acc=0.7628 val_loss=0.6613 acc=0.7646\n",
      "[Epoch 007] lr=0.02550 train_loss=0.6354 acc=0.7812 val_loss=0.6419 acc=0.7746\n",
      "[Epoch 008] lr=0.02557 train_loss=0.6038 acc=0.7910 val_loss=0.6040 acc=0.7874\n",
      "[Epoch 009] lr=0.02564 train_loss=0.5783 acc=0.8015 val_loss=0.5727 acc=0.7958\n",
      "[Epoch 010] lr=0.02571 train_loss=0.5447 acc=0.8128 val_loss=0.6357 acc=0.7726\n",
      "[Epoch 011] lr=0.02578 train_loss=0.5316 acc=0.8164 val_loss=0.5310 acc=0.8142\n",
      "[Epoch 012] lr=0.02585 train_loss=0.5142 acc=0.8231 val_loss=0.5332 acc=0.8150\n",
      "[Epoch 013] lr=0.02592 train_loss=0.4924 acc=0.8308 val_loss=0.5451 acc=0.8106\n",
      "[Epoch 014] lr=0.02599 train_loss=0.4799 acc=0.8354 val_loss=0.5483 acc=0.8092\n",
      "[Epoch 015] lr=0.02607 train_loss=0.4624 acc=0.8422 val_loss=0.5098 acc=0.8186\n",
      "[Epoch 016] lr=0.02614 train_loss=0.4570 acc=0.8428 val_loss=0.5234 acc=0.8184\n",
      "[Epoch 017] lr=0.02621 train_loss=0.4492 acc=0.8452 val_loss=0.5086 acc=0.8212\n",
      "[Epoch 018] lr=0.02628 train_loss=0.4350 acc=0.8496 val_loss=0.4800 acc=0.8370\n",
      "[Epoch 019] lr=0.02635 train_loss=0.4225 acc=0.8520 val_loss=0.5042 acc=0.8228\n",
      "[Epoch 020] lr=0.02642 train_loss=0.4164 acc=0.8579 val_loss=0.4861 acc=0.8362\n",
      "[Epoch 021] lr=0.02649 train_loss=0.4062 acc=0.8609 val_loss=0.5000 acc=0.8304\n",
      "[Epoch 022] lr=0.02656 train_loss=0.4034 acc=0.8624 val_loss=0.4854 acc=0.8304\n",
      "[Epoch 023] lr=0.02663 train_loss=0.3950 acc=0.8643 val_loss=0.4433 acc=0.8442\n",
      "[Epoch 024] lr=0.02670 train_loss=0.3870 acc=0.8667 val_loss=0.4543 acc=0.8426\n",
      "[Epoch 025] lr=0.02678 train_loss=0.3795 acc=0.8682 val_loss=0.4272 acc=0.8498\n",
      "[Epoch 026] lr=0.02685 train_loss=0.3757 acc=0.8697 val_loss=0.4234 acc=0.8530\n",
      "[Epoch 027] lr=0.02692 train_loss=0.3719 acc=0.8717 val_loss=0.4403 acc=0.8518\n",
      "[Epoch 028] lr=0.02699 train_loss=0.3666 acc=0.8720 val_loss=0.4144 acc=0.8546\n",
      "[Epoch 029] lr=0.02706 train_loss=0.3566 acc=0.8776 val_loss=0.4779 acc=0.8310\n",
      "[Epoch 030] lr=0.02713 train_loss=0.3591 acc=0.8763 val_loss=0.4501 acc=0.8482\n",
      "[Epoch 031] lr=0.02720 train_loss=0.3508 acc=0.8787 val_loss=0.4296 acc=0.8546\n",
      "[Epoch 032] lr=0.02727 train_loss=0.3436 acc=0.8813 val_loss=0.4689 acc=0.8416\n",
      "[Epoch 033] lr=0.02734 train_loss=0.3350 acc=0.8845 val_loss=0.4025 acc=0.8656\n",
      "[Epoch 034] lr=0.02741 train_loss=0.3336 acc=0.8845 val_loss=0.4391 acc=0.8552\n",
      "[Epoch 035] lr=0.02749 train_loss=0.3377 acc=0.8833 val_loss=0.4347 acc=0.8532\n",
      "[Epoch 036] lr=0.02756 train_loss=0.3294 acc=0.8865 val_loss=0.4121 acc=0.8600\n",
      "[Epoch 037] lr=0.02763 train_loss=0.3289 acc=0.8855 val_loss=0.4617 acc=0.8448\n",
      "[Epoch 038] lr=0.02770 train_loss=0.3236 acc=0.8881 val_loss=0.4497 acc=0.8472\n",
      "[Epoch 039] lr=0.02777 train_loss=0.3185 acc=0.8891 val_loss=0.4272 acc=0.8512\n",
      "[Epoch 040] lr=0.02784 train_loss=0.3206 acc=0.8883 val_loss=0.4106 acc=0.8548\n",
      "FINAL TEST: loss=0.3691  top1_acc=0.8783\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc ▁▄▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇█████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc ▁▃▄▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇█▇████▇██▇████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss █▆▅▄▄▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▂▁▁▂▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.8656\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 40\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.8783\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.02784\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.88827\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 0.32064\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.8548\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 0.41059\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mabsurd-shape-2\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/lw80elmf\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251014_205208-lw80elmf/logs\u001b[0m\n",
      "Running: python -m vgg6_cifar.scripts.train_experiment --data_dir ./data --out_dir ./runs/act_gelu --activation gelu --optimizer sgd --lr 0.1 --batch_size 128 --epochs 40 --use_bn --aug_hflip --aug_crop --aug_cutout --aug_jitter --amp --seed 42 --wandb\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m setting up run wsqqttej (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251014_205833-wsqqttej\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfine-hill-3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/wsqqttej\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.02507 train_loss=1.6373 acc=0.3952 val_loss=1.3256 acc=0.5272\n",
      "[Epoch 002] lr=0.02514 train_loss=1.2431 acc=0.5574 val_loss=1.2187 acc=0.5784\n",
      "[Epoch 003] lr=0.02521 train_loss=1.0185 acc=0.6453 val_loss=1.0619 acc=0.6246\n",
      "[Epoch 004] lr=0.02528 train_loss=0.8891 acc=0.6949 val_loss=0.8301 acc=0.7050\n",
      "[Epoch 005] lr=0.02536 train_loss=0.7842 acc=0.7319 val_loss=0.7835 acc=0.7250\n",
      "[Epoch 006] lr=0.02543 train_loss=0.7146 acc=0.7560 val_loss=0.7640 acc=0.7300\n",
      "[Epoch 007] lr=0.02550 train_loss=0.6585 acc=0.7764 val_loss=0.6682 acc=0.7704\n",
      "[Epoch 008] lr=0.02557 train_loss=0.6184 acc=0.7920 val_loss=0.6251 acc=0.7804\n",
      "[Epoch 009] lr=0.02564 train_loss=0.5755 acc=0.8052 val_loss=0.6067 acc=0.7906\n",
      "[Epoch 010] lr=0.02571 train_loss=0.5463 acc=0.8168 val_loss=0.6837 acc=0.7718\n",
      "[Epoch 011] lr=0.02578 train_loss=0.5208 acc=0.8249 val_loss=0.5814 acc=0.8000\n",
      "[Epoch 012] lr=0.02585 train_loss=0.5038 acc=0.8293 val_loss=0.4881 acc=0.8296\n",
      "[Epoch 013] lr=0.02592 train_loss=0.4746 acc=0.8397 val_loss=0.5383 acc=0.8094\n",
      "[Epoch 014] lr=0.02599 train_loss=0.4591 acc=0.8442 val_loss=0.5866 acc=0.8058\n",
      "[Epoch 015] lr=0.02607 train_loss=0.4425 acc=0.8523 val_loss=0.4908 acc=0.8294\n",
      "[Epoch 016] lr=0.02614 train_loss=0.4290 acc=0.8545 val_loss=0.4988 acc=0.8322\n",
      "[Epoch 017] lr=0.02621 train_loss=0.4124 acc=0.8628 val_loss=0.4505 acc=0.8384\n",
      "[Epoch 018] lr=0.02628 train_loss=0.4003 acc=0.8668 val_loss=0.5115 acc=0.8226\n",
      "[Epoch 019] lr=0.02635 train_loss=0.3866 acc=0.8678 val_loss=0.5047 acc=0.8248\n",
      "[Epoch 020] lr=0.02642 train_loss=0.3765 acc=0.8722 val_loss=0.4407 acc=0.8524\n",
      "[Epoch 021] lr=0.02649 train_loss=0.3675 acc=0.8765 val_loss=0.4480 acc=0.8432\n",
      "[Epoch 022] lr=0.02656 train_loss=0.3601 acc=0.8780 val_loss=0.4484 acc=0.8456\n",
      "[Epoch 023] lr=0.02663 train_loss=0.3515 acc=0.8824 val_loss=0.4720 acc=0.8388\n",
      "[Epoch 024] lr=0.02670 train_loss=0.3383 acc=0.8849 val_loss=0.4788 acc=0.8396\n",
      "[Epoch 025] lr=0.02678 train_loss=0.3355 acc=0.8868 val_loss=0.4408 acc=0.8492\n",
      "[Epoch 026] lr=0.02685 train_loss=0.3242 acc=0.8890 val_loss=0.4050 acc=0.8644\n",
      "[Epoch 027] lr=0.02692 train_loss=0.3117 acc=0.8944 val_loss=0.4206 acc=0.8592\n",
      "[Epoch 028] lr=0.02699 train_loss=0.3118 acc=0.8947 val_loss=0.3981 acc=0.8632\n",
      "[Epoch 029] lr=0.02706 train_loss=0.3041 acc=0.8964 val_loss=0.4395 acc=0.8470\n",
      "[Epoch 030] lr=0.02713 train_loss=0.2993 acc=0.8968 val_loss=0.3973 acc=0.8674\n",
      "[Epoch 031] lr=0.02720 train_loss=0.2893 acc=0.9017 val_loss=0.3930 acc=0.8702\n",
      "[Epoch 032] lr=0.02727 train_loss=0.2852 acc=0.9017 val_loss=0.4105 acc=0.8614\n",
      "[Epoch 033] lr=0.02734 train_loss=0.2765 acc=0.9052 val_loss=0.3866 acc=0.8676\n",
      "[Epoch 034] lr=0.02741 train_loss=0.2749 acc=0.9070 val_loss=0.3931 acc=0.8638\n",
      "[Epoch 035] lr=0.02749 train_loss=0.2729 acc=0.9076 val_loss=0.3881 acc=0.8690\n",
      "[Epoch 036] lr=0.02756 train_loss=0.2687 acc=0.9074 val_loss=0.3568 acc=0.8776\n",
      "[Epoch 037] lr=0.02763 train_loss=0.2662 acc=0.9089 val_loss=0.5472 acc=0.8292\n",
      "[Epoch 038] lr=0.02770 train_loss=0.2554 acc=0.9122 val_loss=0.3914 acc=0.8674\n",
      "[Epoch 039] lr=0.02777 train_loss=0.2591 acc=0.9110 val_loss=0.3799 acc=0.8760\n",
      "[Epoch 040] lr=0.02784 train_loss=0.2480 acc=0.9148 val_loss=0.4114 acc=0.8610\n",
      "FINAL TEST: loss=0.3372  top1_acc=0.8886\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc ▁▃▄▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇███████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▆▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc ▁▂▃▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇███▇███████▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss █▇▆▄▄▄▃▃▃▃▃▂▂▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.8776\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 40\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.8886\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.02784\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.9148\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 0.24799\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.861\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 0.41143\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mfine-hill-3\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/wsqqttej\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251014_205833-wsqqttej/logs\u001b[0m\n",
      "Running: python -m vgg6_cifar.scripts.train_experiment --data_dir ./data --out_dir ./runs/act_tanh --activation tanh --optimizer sgd --lr 0.1 --batch_size 128 --epochs 40 --use_bn --aug_hflip --aug_crop --aug_cutout --aug_jitter --amp --seed 42 --wandb\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251014_210458-znjz09kw\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mprime-snowball-4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/znjz09kw\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.02507 train_loss=1.6716 acc=0.3872 val_loss=1.4589 acc=0.4598\n",
      "[Epoch 002] lr=0.02514 train_loss=1.3676 acc=0.5056 val_loss=1.1966 acc=0.5574\n",
      "[Epoch 003] lr=0.02521 train_loss=1.1927 acc=0.5755 val_loss=1.2182 acc=0.5722\n",
      "[Epoch 004] lr=0.02528 train_loss=1.0943 acc=0.6142 val_loss=1.0019 acc=0.6354\n",
      "[Epoch 005] lr=0.02536 train_loss=1.0155 acc=0.6413 val_loss=0.9801 acc=0.6500\n",
      "[Epoch 006] lr=0.02543 train_loss=0.9866 acc=0.6528 val_loss=0.9537 acc=0.6632\n",
      "[Epoch 007] lr=0.02550 train_loss=0.9296 acc=0.6735 val_loss=0.9023 acc=0.6868\n",
      "[Epoch 008] lr=0.02557 train_loss=0.9016 acc=0.6825 val_loss=0.9033 acc=0.6834\n",
      "[Epoch 009] lr=0.02564 train_loss=0.8721 acc=0.6952 val_loss=0.9308 acc=0.6718\n",
      "[Epoch 010] lr=0.02571 train_loss=0.8478 acc=0.7025 val_loss=0.8190 acc=0.7088\n",
      "[Epoch 011] lr=0.02578 train_loss=0.8246 acc=0.7116 val_loss=0.7676 acc=0.7228\n",
      "[Epoch 012] lr=0.02585 train_loss=0.8078 acc=0.7187 val_loss=0.7579 acc=0.7352\n",
      "[Epoch 013] lr=0.02592 train_loss=0.7879 acc=0.7269 val_loss=0.7707 acc=0.7274\n",
      "[Epoch 014] lr=0.02599 train_loss=0.7721 acc=0.7302 val_loss=0.7509 acc=0.7320\n",
      "[Epoch 015] lr=0.02607 train_loss=0.7588 acc=0.7322 val_loss=0.7886 acc=0.7164\n",
      "[Epoch 016] lr=0.02614 train_loss=0.7477 acc=0.7380 val_loss=0.7340 acc=0.7424\n",
      "[Epoch 017] lr=0.02621 train_loss=0.7330 acc=0.7462 val_loss=0.7323 acc=0.7390\n",
      "[Epoch 018] lr=0.02628 train_loss=0.7228 acc=0.7489 val_loss=0.7264 acc=0.7436\n",
      "[Epoch 019] lr=0.02635 train_loss=0.7100 acc=0.7510 val_loss=0.7631 acc=0.7376\n",
      "[Epoch 020] lr=0.02642 train_loss=0.6965 acc=0.7560 val_loss=0.7299 acc=0.7420\n",
      "[Epoch 021] lr=0.02649 train_loss=0.6977 acc=0.7565 val_loss=0.7712 acc=0.7318\n",
      "[Epoch 022] lr=0.02656 train_loss=0.6884 acc=0.7612 val_loss=0.6865 acc=0.7658\n",
      "[Epoch 023] lr=0.02663 train_loss=0.6814 acc=0.7631 val_loss=0.6792 acc=0.7684\n",
      "[Epoch 024] lr=0.02670 train_loss=0.6729 acc=0.7674 val_loss=0.6446 acc=0.7740\n",
      "[Epoch 025] lr=0.02678 train_loss=0.6593 acc=0.7734 val_loss=0.6657 acc=0.7728\n",
      "[Epoch 026] lr=0.02685 train_loss=0.6650 acc=0.7672 val_loss=0.6226 acc=0.7790\n",
      "[Epoch 027] lr=0.02692 train_loss=0.6543 acc=0.7721 val_loss=0.6828 acc=0.7536\n",
      "[Epoch 028] lr=0.02699 train_loss=0.6448 acc=0.7762 val_loss=0.7069 acc=0.7538\n",
      "[Epoch 029] lr=0.02706 train_loss=0.6325 acc=0.7825 val_loss=0.6688 acc=0.7694\n",
      "[Epoch 030] lr=0.02713 train_loss=0.6421 acc=0.7802 val_loss=0.6071 acc=0.7836\n",
      "[Epoch 031] lr=0.02720 train_loss=0.6373 acc=0.7787 val_loss=0.7295 acc=0.7470\n",
      "[Epoch 032] lr=0.02727 train_loss=0.6275 acc=0.7836 val_loss=0.6762 acc=0.7696\n",
      "[Epoch 033] lr=0.02734 train_loss=0.6280 acc=0.7853 val_loss=0.6101 acc=0.7904\n",
      "[Epoch 034] lr=0.02741 train_loss=0.6256 acc=0.7828 val_loss=0.8124 acc=0.7276\n",
      "[Epoch 035] lr=0.02749 train_loss=0.6166 acc=0.7870 val_loss=0.6861 acc=0.7618\n",
      "[Epoch 036] lr=0.02756 train_loss=0.6122 acc=0.7868 val_loss=0.6193 acc=0.7848\n",
      "[Epoch 037] lr=0.02763 train_loss=0.6092 acc=0.7884 val_loss=0.6084 acc=0.7906\n",
      "[Epoch 038] lr=0.02770 train_loss=0.6012 acc=0.7905 val_loss=0.6867 acc=0.7668\n",
      "[Epoch 039] lr=0.02777 train_loss=0.6072 acc=0.7876 val_loss=0.6384 acc=0.7702\n",
      "[Epoch 040] lr=0.02784 train_loss=0.5989 acc=0.7909 val_loss=0.6486 acc=0.7752\n",
      "FINAL TEST: loss=0.5253  top1_acc=0.8206\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc ▁▃▄▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇██████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▆▅▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc ▁▃▃▅▅▅▆▆▅▆▇▇▇▇▆▇▇▇▇▇▇▇████▇▇██▇██▇▇██▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss █▆▆▄▄▄▃▃▄▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂▂▂▁▂▂▁▃▂▁▁▂▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.7906\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 40\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.8206\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.02784\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.79087\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 0.59893\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.7752\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 0.64863\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mprime-snowball-4\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/znjz09kw\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251014_210458-znjz09kw/logs\u001b[0m\n",
      "Running: python -m vgg6_cifar.scripts.train_experiment --data_dir ./data --out_dir ./runs/act_sigmoid --activation sigmoid --optimizer sgd --lr 0.1 --batch_size 128 --epochs 40 --use_bn --aug_hflip --aug_crop --aug_cutout --aug_jitter --amp --seed 42 --wandb\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251014_211118-vq4cuk44\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mabsurd-sky-5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/vq4cuk44\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.02507 train_loss=1.9877 acc=0.2279 val_loss=2.0788 acc=0.2658\n",
      "[Epoch 002] lr=0.02514 train_loss=1.6661 acc=0.3612 val_loss=2.6660 acc=0.2342\n",
      "[Epoch 003] lr=0.02521 train_loss=1.5018 acc=0.4362 val_loss=1.8151 acc=0.3518\n",
      "[Epoch 004] lr=0.02528 train_loss=1.3756 acc=0.4965 val_loss=1.7571 acc=0.3718\n",
      "[Epoch 005] lr=0.02536 train_loss=1.2846 acc=0.5306 val_loss=2.0918 acc=0.3730\n",
      "[Epoch 006] lr=0.02543 train_loss=1.2274 acc=0.5541 val_loss=1.7699 acc=0.4114\n",
      "[Epoch 007] lr=0.02550 train_loss=1.1762 acc=0.5742 val_loss=3.8601 acc=0.2120\n",
      "[Epoch 008] lr=0.02557 train_loss=1.1279 acc=0.5949 val_loss=1.4935 acc=0.5044\n",
      "[Epoch 009] lr=0.02564 train_loss=1.0993 acc=0.6070 val_loss=1.2790 acc=0.5418\n",
      "[Epoch 010] lr=0.02571 train_loss=1.0658 acc=0.6148 val_loss=2.5430 acc=0.3034\n",
      "[Epoch 011] lr=0.02578 train_loss=1.0383 acc=0.6278 val_loss=1.8998 acc=0.4096\n",
      "[Epoch 012] lr=0.02585 train_loss=1.0146 acc=0.6342 val_loss=3.4417 acc=0.2184\n",
      "[Epoch 013] lr=0.02592 train_loss=0.9954 acc=0.6472 val_loss=2.4291 acc=0.3662\n",
      "[Epoch 014] lr=0.02599 train_loss=0.9835 acc=0.6494 val_loss=2.3094 acc=0.3356\n",
      "[Epoch 015] lr=0.02607 train_loss=0.9612 acc=0.6564 val_loss=3.0502 acc=0.3242\n",
      "[Epoch 016] lr=0.02614 train_loss=0.9577 acc=0.6589 val_loss=1.4980 acc=0.4742\n",
      "[Epoch 017] lr=0.02621 train_loss=0.9409 acc=0.6668 val_loss=2.4141 acc=0.3750\n",
      "[Epoch 018] lr=0.02628 train_loss=0.9239 acc=0.6718 val_loss=1.9795 acc=0.3402\n",
      "[Epoch 019] lr=0.02635 train_loss=0.9167 acc=0.6744 val_loss=1.6313 acc=0.5206\n",
      "[Epoch 020] lr=0.02642 train_loss=0.9026 acc=0.6807 val_loss=4.5213 acc=0.1326\n",
      "[Epoch 021] lr=0.02649 train_loss=0.9010 acc=0.6798 val_loss=3.9595 acc=0.3128\n",
      "[Epoch 022] lr=0.02656 train_loss=0.8958 acc=0.6844 val_loss=2.5827 acc=0.3692\n",
      "[Epoch 023] lr=0.02663 train_loss=0.8840 acc=0.6855 val_loss=1.1720 acc=0.5654\n",
      "[Epoch 024] lr=0.02670 train_loss=0.8768 acc=0.6906 val_loss=1.8779 acc=0.4170\n",
      "[Epoch 025] lr=0.02678 train_loss=0.8707 acc=0.6915 val_loss=2.7919 acc=0.2910\n",
      "[Epoch 026] lr=0.02685 train_loss=0.8599 acc=0.6920 val_loss=1.4595 acc=0.4942\n",
      "[Epoch 027] lr=0.02692 train_loss=0.8531 acc=0.7007 val_loss=2.5139 acc=0.3462\n",
      "[Epoch 028] lr=0.02699 train_loss=0.8469 acc=0.7014 val_loss=1.6516 acc=0.4690\n",
      "[Epoch 029] lr=0.02706 train_loss=0.8319 acc=0.7057 val_loss=3.9043 acc=0.3244\n",
      "[Epoch 030] lr=0.02713 train_loss=0.8263 acc=0.7091 val_loss=1.4117 acc=0.5414\n",
      "[Epoch 031] lr=0.02720 train_loss=0.8338 acc=0.7064 val_loss=3.0901 acc=0.2816\n",
      "[Epoch 032] lr=0.02727 train_loss=0.8208 acc=0.7127 val_loss=2.7911 acc=0.3564\n",
      "[Epoch 033] lr=0.02734 train_loss=0.8082 acc=0.7160 val_loss=2.9979 acc=0.3210\n",
      "[Epoch 034] lr=0.02741 train_loss=0.8058 acc=0.7171 val_loss=1.9236 acc=0.4536\n",
      "[Epoch 035] lr=0.02749 train_loss=0.7996 acc=0.7175 val_loss=1.5394 acc=0.5152\n",
      "[Epoch 036] lr=0.02756 train_loss=0.7999 acc=0.7181 val_loss=4.2076 acc=0.2410\n",
      "[Epoch 037] lr=0.02763 train_loss=0.7923 acc=0.7210 val_loss=3.4094 acc=0.3168\n",
      "[Epoch 038] lr=0.02770 train_loss=0.7816 acc=0.7264 val_loss=4.0514 acc=0.2222\n",
      "[Epoch 039] lr=0.02777 train_loss=0.7810 acc=0.7280 val_loss=1.9762 acc=0.4266\n",
      "[Epoch 040] lr=0.02784 train_loss=0.7770 acc=0.7291 val_loss=2.7940 acc=0.3446\n",
      "FINAL TEST: loss=1.1068  top1_acc=0.6030\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc ▁▃▄▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▆▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc ▃▃▅▅▅▆▂▇█▄▅▂▅▄▄▇▅▄▇▁▄▅█▆▄▇▄▆▄█▃▅▄▆▇▃▄▂▆▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss ▃▄▂▂▃▂▇▂▁▄▃▆▄▃▅▂▄▃▂█▇▄▁▂▄▂▄▂▇▂▅▄▅▃▂▇▆▇▃▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.5654\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 40\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.603\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.02784\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.72907\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 0.77696\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.3446\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 2.79395\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mabsurd-sky-5\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/vq4cuk44\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251014_211118-vq4cuk44/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "activations = [\"relu\", \"silu\", \"gelu\", \"tanh\", \"sigmoid\"]\n",
    "\n",
    "for act in activations:\n",
    "    cmd = (\n",
    "        f\"python -m vgg6_cifar.scripts.train_experiment \"\n",
    "        f\"--data_dir ./data --out_dir ./runs/act_{act} \"\n",
    "        f\"--activation {act} --optimizer sgd --lr 0.1 --batch_size 128 --epochs 40 \"\n",
    "        f\"--use_bn \"\n",
    "        f\"--aug_hflip --aug_crop --aug_cutout --aug_jitter --amp --seed 42 --wandb\"\n",
    "    )\n",
    "    print(f\"Running: {cmd}\")\n",
    "    !{cmd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1da7eb6a-05d9-4e1e-86e3-b92868273fd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251014_212100-vq6caxd9\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mabsurd-cloud-6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/vq6caxd9\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.02507 train_loss=1.6373 acc=0.3952 val_loss=1.3256 acc=0.5272\n",
      "[Epoch 002] lr=0.02514 train_loss=1.2431 acc=0.5574 val_loss=1.2187 acc=0.5784\n",
      "[Epoch 003] lr=0.02521 train_loss=1.0185 acc=0.6453 val_loss=1.0619 acc=0.6246\n",
      "[Epoch 004] lr=0.02528 train_loss=0.8891 acc=0.6949 val_loss=0.8301 acc=0.7050\n",
      "[Epoch 005] lr=0.02536 train_loss=0.7842 acc=0.7319 val_loss=0.7835 acc=0.7250\n",
      "[Epoch 006] lr=0.02543 train_loss=0.7146 acc=0.7560 val_loss=0.7640 acc=0.7300\n",
      "[Epoch 007] lr=0.02550 train_loss=0.6585 acc=0.7764 val_loss=0.6682 acc=0.7704\n",
      "[Epoch 008] lr=0.02557 train_loss=0.6184 acc=0.7920 val_loss=0.6251 acc=0.7804\n",
      "[Epoch 009] lr=0.02564 train_loss=0.5755 acc=0.8052 val_loss=0.6067 acc=0.7906\n",
      "[Epoch 010] lr=0.02571 train_loss=0.5463 acc=0.8168 val_loss=0.6837 acc=0.7718\n",
      "[Epoch 011] lr=0.02578 train_loss=0.5208 acc=0.8249 val_loss=0.5814 acc=0.8000\n",
      "[Epoch 012] lr=0.02585 train_loss=0.5038 acc=0.8293 val_loss=0.4881 acc=0.8296\n",
      "[Epoch 013] lr=0.02592 train_loss=0.4746 acc=0.8397 val_loss=0.5383 acc=0.8094\n",
      "[Epoch 014] lr=0.02599 train_loss=0.4591 acc=0.8442 val_loss=0.5866 acc=0.8058\n",
      "[Epoch 015] lr=0.02607 train_loss=0.4425 acc=0.8523 val_loss=0.4908 acc=0.8294\n",
      "[Epoch 016] lr=0.02614 train_loss=0.4290 acc=0.8545 val_loss=0.4988 acc=0.8322\n",
      "[Epoch 017] lr=0.02621 train_loss=0.4124 acc=0.8628 val_loss=0.4505 acc=0.8384\n",
      "[Epoch 018] lr=0.02628 train_loss=0.4003 acc=0.8668 val_loss=0.5115 acc=0.8226\n",
      "[Epoch 019] lr=0.02635 train_loss=0.3866 acc=0.8678 val_loss=0.5047 acc=0.8248\n",
      "[Epoch 020] lr=0.02642 train_loss=0.3765 acc=0.8722 val_loss=0.4407 acc=0.8524\n",
      "[Epoch 021] lr=0.02649 train_loss=0.3675 acc=0.8765 val_loss=0.4480 acc=0.8432\n",
      "[Epoch 022] lr=0.02656 train_loss=0.3601 acc=0.8780 val_loss=0.4484 acc=0.8456\n",
      "[Epoch 023] lr=0.02663 train_loss=0.3515 acc=0.8824 val_loss=0.4720 acc=0.8388\n",
      "[Epoch 024] lr=0.02670 train_loss=0.3383 acc=0.8849 val_loss=0.4788 acc=0.8396\n",
      "[Epoch 025] lr=0.02678 train_loss=0.3355 acc=0.8868 val_loss=0.4408 acc=0.8492\n",
      "[Epoch 026] lr=0.02685 train_loss=0.3242 acc=0.8890 val_loss=0.4050 acc=0.8644\n",
      "[Epoch 027] lr=0.02692 train_loss=0.3117 acc=0.8944 val_loss=0.4206 acc=0.8592\n",
      "[Epoch 028] lr=0.02699 train_loss=0.3118 acc=0.8947 val_loss=0.3981 acc=0.8632\n",
      "[Epoch 029] lr=0.02706 train_loss=0.3041 acc=0.8964 val_loss=0.4395 acc=0.8470\n",
      "[Epoch 030] lr=0.02713 train_loss=0.2993 acc=0.8968 val_loss=0.3973 acc=0.8674\n",
      "[Epoch 031] lr=0.02720 train_loss=0.2893 acc=0.9017 val_loss=0.3930 acc=0.8702\n",
      "[Epoch 032] lr=0.02727 train_loss=0.2852 acc=0.9017 val_loss=0.4105 acc=0.8614\n",
      "[Epoch 033] lr=0.02734 train_loss=0.2765 acc=0.9052 val_loss=0.3866 acc=0.8676\n",
      "[Epoch 034] lr=0.02741 train_loss=0.2749 acc=0.9070 val_loss=0.3931 acc=0.8638\n",
      "[Epoch 035] lr=0.02749 train_loss=0.2729 acc=0.9076 val_loss=0.3881 acc=0.8690\n",
      "[Epoch 036] lr=0.02756 train_loss=0.2687 acc=0.9074 val_loss=0.3568 acc=0.8776\n",
      "[Epoch 037] lr=0.02763 train_loss=0.2662 acc=0.9089 val_loss=0.5472 acc=0.8292\n",
      "[Epoch 038] lr=0.02770 train_loss=0.2554 acc=0.9122 val_loss=0.3914 acc=0.8674\n",
      "[Epoch 039] lr=0.02777 train_loss=0.2591 acc=0.9110 val_loss=0.3799 acc=0.8760\n",
      "[Epoch 040] lr=0.02784 train_loss=0.2480 acc=0.9148 val_loss=0.4114 acc=0.8610\n",
      "FINAL TEST: loss=0.3372  top1_acc=0.8886\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc ▁▃▄▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇███████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▆▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc ▁▂▃▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇███▇███████▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss █▇▆▄▄▄▃▃▃▃▃▂▂▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.8776\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 40\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.8886\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.02784\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.9148\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 0.24799\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.861\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 0.41143\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mabsurd-cloud-6\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/vq6caxd9\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251014_212100-vq6caxd9/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m vgg6_cifar.scripts.train_experiment --data_dir ./data --out_dir ./runs/opt_sgd \\\n",
    "--activation gelu --optimizer sgd \\\n",
    "--lr 0.1   \\\n",
    "--batch_size 128 \\\n",
    "--epochs 40 \\\n",
    "--use_bn \\\n",
    "--aug_hflip \\\n",
    "--aug_crop \\\n",
    "--aug_cutout \\\n",
    "--aug_jitter \\\n",
    "--amp --seed 42 --wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6ad71aa-65d5-4f0f-8803-9b0dbe6fbd25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251014_212910-cotizpbx\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msage-dragon-7\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/cotizpbx\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.02507 train_loss=1.5335 acc=0.4357 val_loss=1.2711 acc=0.5524\n",
      "[Epoch 002] lr=0.02514 train_loss=1.0997 acc=0.6107 val_loss=1.1434 acc=0.6314\n",
      "[Epoch 003] lr=0.02521 train_loss=0.9182 acc=0.6807 val_loss=0.9018 acc=0.6736\n",
      "[Epoch 004] lr=0.02528 train_loss=0.8087 acc=0.7248 val_loss=0.8189 acc=0.7130\n",
      "[Epoch 005] lr=0.02536 train_loss=0.7209 acc=0.7524 val_loss=0.6781 acc=0.7586\n",
      "[Epoch 006] lr=0.02543 train_loss=0.6673 acc=0.7708 val_loss=0.6262 acc=0.7772\n",
      "[Epoch 007] lr=0.02550 train_loss=0.6109 acc=0.7909 val_loss=0.6491 acc=0.7772\n",
      "[Epoch 008] lr=0.02557 train_loss=0.5686 acc=0.8069 val_loss=0.5817 acc=0.7974\n",
      "[Epoch 009] lr=0.02564 train_loss=0.5379 acc=0.8182 val_loss=0.5497 acc=0.8048\n",
      "[Epoch 010] lr=0.02571 train_loss=0.5043 acc=0.8297 val_loss=0.5223 acc=0.8124\n",
      "[Epoch 011] lr=0.02578 train_loss=0.4831 acc=0.8366 val_loss=0.5770 acc=0.8052\n",
      "[Epoch 012] lr=0.02585 train_loss=0.4612 acc=0.8434 val_loss=0.5024 acc=0.8256\n",
      "[Epoch 013] lr=0.02592 train_loss=0.4415 acc=0.8506 val_loss=0.4680 acc=0.8354\n",
      "[Epoch 014] lr=0.02599 train_loss=0.4222 acc=0.8577 val_loss=0.5546 acc=0.8140\n",
      "[Epoch 015] lr=0.02607 train_loss=0.4096 acc=0.8604 val_loss=0.5076 acc=0.8222\n",
      "[Epoch 016] lr=0.02614 train_loss=0.3941 acc=0.8650 val_loss=0.4775 acc=0.8362\n",
      "[Epoch 017] lr=0.02621 train_loss=0.3867 acc=0.8688 val_loss=0.4575 acc=0.8428\n",
      "[Epoch 018] lr=0.02628 train_loss=0.3706 acc=0.8733 val_loss=0.5390 acc=0.8244\n",
      "[Epoch 019] lr=0.02635 train_loss=0.3585 acc=0.8768 val_loss=0.4105 acc=0.8592\n",
      "[Epoch 020] lr=0.02642 train_loss=0.3483 acc=0.8814 val_loss=0.4033 acc=0.8610\n",
      "[Epoch 021] lr=0.02649 train_loss=0.3403 acc=0.8838 val_loss=0.4428 acc=0.8452\n",
      "[Epoch 022] lr=0.02656 train_loss=0.3347 acc=0.8862 val_loss=0.3822 acc=0.8616\n",
      "[Epoch 023] lr=0.02663 train_loss=0.3236 acc=0.8893 val_loss=0.4149 acc=0.8584\n",
      "[Epoch 024] lr=0.02670 train_loss=0.3114 acc=0.8941 val_loss=0.4240 acc=0.8620\n",
      "[Epoch 025] lr=0.02678 train_loss=0.3067 acc=0.8942 val_loss=0.4032 acc=0.8604\n",
      "[Epoch 026] lr=0.02685 train_loss=0.2993 acc=0.8966 val_loss=0.3981 acc=0.8648\n",
      "[Epoch 027] lr=0.02692 train_loss=0.2948 acc=0.8990 val_loss=0.4509 acc=0.8480\n",
      "[Epoch 028] lr=0.02699 train_loss=0.2900 acc=0.9005 val_loss=0.3874 acc=0.8648\n",
      "[Epoch 029] lr=0.02706 train_loss=0.2821 acc=0.9030 val_loss=0.4107 acc=0.8584\n",
      "[Epoch 030] lr=0.02713 train_loss=0.2773 acc=0.9041 val_loss=0.3994 acc=0.8710\n",
      "[Epoch 031] lr=0.02720 train_loss=0.2747 acc=0.9055 val_loss=0.4111 acc=0.8626\n",
      "[Epoch 032] lr=0.02727 train_loss=0.2654 acc=0.9095 val_loss=0.4187 acc=0.8546\n",
      "[Epoch 033] lr=0.02734 train_loss=0.2620 acc=0.9100 val_loss=0.3540 acc=0.8810\n",
      "[Epoch 034] lr=0.02741 train_loss=0.2576 acc=0.9114 val_loss=0.3894 acc=0.8710\n",
      "[Epoch 035] lr=0.02749 train_loss=0.2591 acc=0.9116 val_loss=0.4228 acc=0.8572\n",
      "[Epoch 036] lr=0.02756 train_loss=0.2510 acc=0.9142 val_loss=0.3746 acc=0.8762\n",
      "[Epoch 037] lr=0.02763 train_loss=0.2470 acc=0.9148 val_loss=0.3931 acc=0.8672\n",
      "[Epoch 038] lr=0.02770 train_loss=0.2434 acc=0.9161 val_loss=0.3778 acc=0.8734\n",
      "[Epoch 039] lr=0.02777 train_loss=0.2393 acc=0.9174 val_loss=0.4407 acc=0.8530\n",
      "[Epoch 040] lr=0.02784 train_loss=0.2330 acc=0.9193 val_loss=0.4093 acc=0.8648\n",
      "FINAL TEST: loss=0.3252  top1_acc=0.8927\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc ▁▄▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇███████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▆▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc ▁▃▄▄▅▆▆▆▆▇▆▇▇▇▇▇▇▇██▇█████▇████▇██▇███▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss █▇▅▅▃▃▃▃▂▂▃▂▂▃▂▂▂▂▁▁▂▁▁▂▁▁▂▁▁▁▁▁▁▁▂▁▁▁▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.881\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 40\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.8927\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.02784\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.91929\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 0.23303\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.8648\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 0.40932\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33msage-dragon-7\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/cotizpbx\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251014_212910-cotizpbx/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m vgg6_cifar.scripts.train_experiment \\\n",
    "    --data_dir ./data \\\n",
    "    --out_dir ./runs/opt_nesterov \\\n",
    "    --activation gelu \\\n",
    "    --optimizer nesterov-sgd \\\n",
    "    --lr 0.1 \\\n",
    "    --batch_size 128 \\\n",
    "    --epochs 40 \\\n",
    "    --use_bn \\\n",
    "    --aug_hflip \\\n",
    "    --aug_crop \\\n",
    "    --aug_cutout \\\n",
    "    --aug_jitter \\\n",
    "    --amp \\\n",
    "    --seed 42 \\\n",
    "    --wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "795afcb7-f550-4be2-829e-b595c04ef73a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m setting up run ywnn8wsu (0.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m setting up run ywnn8wsu (0.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251014_213529-ywnn8wsu\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mswept-capybara-8\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/ywnn8wsu\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.00025 train_loss=1.4490 acc=0.4654 val_loss=1.1327 acc=0.5962\n",
      "[Epoch 002] lr=0.00025 train_loss=1.0130 acc=0.6386 val_loss=0.9881 acc=0.6454\n",
      "[Epoch 003] lr=0.00025 train_loss=0.8522 acc=0.7001 val_loss=0.8808 acc=0.6916\n",
      "[Epoch 004] lr=0.00025 train_loss=0.7569 acc=0.7380 val_loss=0.7230 acc=0.7424\n",
      "[Epoch 005] lr=0.00025 train_loss=0.6903 acc=0.7596 val_loss=0.6933 acc=0.7518\n",
      "[Epoch 006] lr=0.00025 train_loss=0.6459 acc=0.7762 val_loss=0.6422 acc=0.7714\n",
      "[Epoch 007] lr=0.00025 train_loss=0.6025 acc=0.7911 val_loss=0.7004 acc=0.7462\n",
      "[Epoch 008] lr=0.00026 train_loss=0.5673 acc=0.8050 val_loss=0.6096 acc=0.7806\n",
      "[Epoch 009] lr=0.00026 train_loss=0.5398 acc=0.8130 val_loss=0.5690 acc=0.7972\n",
      "[Epoch 010] lr=0.00026 train_loss=0.5139 acc=0.8230 val_loss=0.5804 acc=0.7950\n",
      "[Epoch 011] lr=0.00026 train_loss=0.4883 acc=0.8338 val_loss=0.5395 acc=0.8098\n",
      "[Epoch 012] lr=0.00026 train_loss=0.4693 acc=0.8379 val_loss=0.5128 acc=0.8132\n",
      "[Epoch 013] lr=0.00026 train_loss=0.4447 acc=0.8460 val_loss=0.5044 acc=0.8190\n",
      "[Epoch 014] lr=0.00026 train_loss=0.4299 acc=0.8539 val_loss=0.5164 acc=0.8252\n",
      "[Epoch 015] lr=0.00026 train_loss=0.4101 acc=0.8595 val_loss=0.4846 acc=0.8294\n",
      "[Epoch 016] lr=0.00026 train_loss=0.4018 acc=0.8611 val_loss=0.4513 acc=0.8434\n",
      "[Epoch 017] lr=0.00026 train_loss=0.3872 acc=0.8662 val_loss=0.4687 acc=0.8328\n",
      "[Epoch 018] lr=0.00026 train_loss=0.3739 acc=0.8695 val_loss=0.4260 acc=0.8542\n",
      "[Epoch 019] lr=0.00026 train_loss=0.3580 acc=0.8758 val_loss=0.5078 acc=0.8218\n",
      "[Epoch 020] lr=0.00026 train_loss=0.3512 acc=0.8808 val_loss=0.4153 acc=0.8502\n",
      "[Epoch 021] lr=0.00026 train_loss=0.3380 acc=0.8832 val_loss=0.4588 acc=0.8446\n",
      "[Epoch 022] lr=0.00027 train_loss=0.3300 acc=0.8861 val_loss=0.4734 acc=0.8382\n",
      "[Epoch 023] lr=0.00027 train_loss=0.3167 acc=0.8914 val_loss=0.4615 acc=0.8398\n",
      "[Epoch 024] lr=0.00027 train_loss=0.3126 acc=0.8916 val_loss=0.4165 acc=0.8576\n",
      "[Epoch 025] lr=0.00027 train_loss=0.2994 acc=0.8973 val_loss=0.4678 acc=0.8440\n",
      "[Epoch 026] lr=0.00027 train_loss=0.2956 acc=0.8975 val_loss=0.4083 acc=0.8612\n",
      "[Epoch 027] lr=0.00027 train_loss=0.2864 acc=0.8996 val_loss=0.4247 acc=0.8566\n",
      "[Epoch 028] lr=0.00027 train_loss=0.2778 acc=0.9042 val_loss=0.3864 acc=0.8652\n",
      "[Epoch 029] lr=0.00027 train_loss=0.2726 acc=0.9065 val_loss=0.4205 acc=0.8560\n",
      "[Epoch 030] lr=0.00027 train_loss=0.2667 acc=0.9076 val_loss=0.3929 acc=0.8706\n",
      "[Epoch 031] lr=0.00027 train_loss=0.2593 acc=0.9114 val_loss=0.3997 acc=0.8664\n",
      "[Epoch 032] lr=0.00027 train_loss=0.2552 acc=0.9112 val_loss=0.4152 acc=0.8602\n",
      "[Epoch 033] lr=0.00027 train_loss=0.2453 acc=0.9140 val_loss=0.3922 acc=0.8684\n",
      "[Epoch 034] lr=0.00027 train_loss=0.2409 acc=0.9154 val_loss=0.4689 acc=0.8480\n",
      "[Epoch 035] lr=0.00027 train_loss=0.2432 acc=0.9146 val_loss=0.4120 acc=0.8650\n",
      "[Epoch 036] lr=0.00028 train_loss=0.2348 acc=0.9188 val_loss=0.4770 acc=0.8486\n",
      "[Epoch 037] lr=0.00028 train_loss=0.2301 acc=0.9222 val_loss=0.4075 acc=0.8644\n",
      "[Epoch 038] lr=0.00028 train_loss=0.2251 acc=0.9226 val_loss=0.3928 acc=0.8640\n",
      "[Epoch 039] lr=0.00028 train_loss=0.2239 acc=0.9216 val_loss=0.3571 acc=0.8764\n",
      "[Epoch 040] lr=0.00028 train_loss=0.2154 acc=0.9252 val_loss=0.3754 acc=0.8730\n",
      "FINAL TEST: loss=0.3589  top1_acc=0.8835\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc ▁▄▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▆▅▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc ▁▂▃▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇█▇███▇████▇█▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss █▇▆▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▂▁▁▂▁▂▁▂▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.8764\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 40\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.8835\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.00028\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.92518\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 0.21544\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.873\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 0.37538\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mswept-capybara-8\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/ywnn8wsu\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251014_213529-ywnn8wsu/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m vgg6_cifar.scripts.train_experiment \\\n",
    "    --data_dir ./data \\\n",
    "    --out_dir ./runs/opt_adam \\\n",
    "    --activation gelu \\\n",
    "    --optimizer adam \\\n",
    "    --lr 0.001 \\\n",
    "    --batch_size 128 \\\n",
    "    --epochs 40 \\\n",
    "    --use_bn \\\n",
    "    --aug_hflip \\\n",
    "    --aug_crop \\\n",
    "    --aug_cutout \\\n",
    "    --aug_jitter \\\n",
    "    --amp \\\n",
    "    --seed 42 \\\n",
    "    --wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19130895-1904-4d5e-91ba-e403fec00a67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m setting up run b86vq86l (0.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m setting up run b86vq86l (0.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251014_214155-b86vq86l\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mpeachy-spaceship-9\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/b86vq86l\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.00025 train_loss=1.4505 acc=0.4658 val_loss=1.1469 acc=0.5872\n",
      "[Epoch 002] lr=0.00025 train_loss=1.0174 acc=0.6379 val_loss=0.9748 acc=0.6498\n",
      "[Epoch 003] lr=0.00025 train_loss=0.8577 acc=0.6979 val_loss=0.8808 acc=0.6894\n",
      "[Epoch 004] lr=0.00025 train_loss=0.7637 acc=0.7347 val_loss=0.7383 acc=0.7360\n",
      "[Epoch 005] lr=0.00025 train_loss=0.6969 acc=0.7579 val_loss=0.7083 acc=0.7498\n",
      "[Epoch 006] lr=0.00025 train_loss=0.6524 acc=0.7739 val_loss=0.6467 acc=0.7702\n",
      "[Epoch 007] lr=0.00025 train_loss=0.6086 acc=0.7886 val_loss=0.6744 acc=0.7568\n",
      "[Epoch 008] lr=0.00026 train_loss=0.5742 acc=0.8015 val_loss=0.5965 acc=0.7866\n",
      "[Epoch 009] lr=0.00026 train_loss=0.5465 acc=0.8108 val_loss=0.5565 acc=0.8082\n",
      "[Epoch 010] lr=0.00026 train_loss=0.5221 acc=0.8203 val_loss=0.5512 acc=0.8058\n",
      "[Epoch 011] lr=0.00026 train_loss=0.4939 acc=0.8294 val_loss=0.5723 acc=0.7978\n",
      "[Epoch 012] lr=0.00026 train_loss=0.4739 acc=0.8368 val_loss=0.5436 acc=0.8050\n",
      "[Epoch 013] lr=0.00026 train_loss=0.4518 acc=0.8442 val_loss=0.5339 acc=0.8148\n",
      "[Epoch 014] lr=0.00026 train_loss=0.4388 acc=0.8487 val_loss=0.5049 acc=0.8270\n",
      "[Epoch 015] lr=0.00026 train_loss=0.4147 acc=0.8564 val_loss=0.4844 acc=0.8316\n",
      "[Epoch 016] lr=0.00026 train_loss=0.4034 acc=0.8588 val_loss=0.4640 acc=0.8340\n",
      "[Epoch 017] lr=0.00026 train_loss=0.3879 acc=0.8660 val_loss=0.4686 acc=0.8278\n",
      "[Epoch 018] lr=0.00026 train_loss=0.3764 acc=0.8694 val_loss=0.4263 acc=0.8500\n",
      "[Epoch 019] lr=0.00026 train_loss=0.3626 acc=0.8739 val_loss=0.4662 acc=0.8346\n",
      "[Epoch 020] lr=0.00026 train_loss=0.3507 acc=0.8794 val_loss=0.4242 acc=0.8510\n",
      "[Epoch 021] lr=0.00026 train_loss=0.3407 acc=0.8831 val_loss=0.4454 acc=0.8408\n",
      "[Epoch 022] lr=0.00027 train_loss=0.3306 acc=0.8861 val_loss=0.4469 acc=0.8474\n",
      "[Epoch 023] lr=0.00027 train_loss=0.3124 acc=0.8912 val_loss=0.4087 acc=0.8582\n",
      "[Epoch 024] lr=0.00027 train_loss=0.3101 acc=0.8917 val_loss=0.4196 acc=0.8560\n",
      "[Epoch 025] lr=0.00027 train_loss=0.2981 acc=0.8959 val_loss=0.4536 acc=0.8490\n",
      "[Epoch 026] lr=0.00027 train_loss=0.2929 acc=0.8970 val_loss=0.4416 acc=0.8536\n",
      "[Epoch 027] lr=0.00027 train_loss=0.2801 acc=0.9010 val_loss=0.4299 acc=0.8584\n",
      "[Epoch 028] lr=0.00027 train_loss=0.2729 acc=0.9050 val_loss=0.3960 acc=0.8658\n",
      "[Epoch 029] lr=0.00027 train_loss=0.2679 acc=0.9064 val_loss=0.4074 acc=0.8662\n",
      "[Epoch 030] lr=0.00027 train_loss=0.2597 acc=0.9097 val_loss=0.4459 acc=0.8548\n",
      "[Epoch 031] lr=0.00027 train_loss=0.2523 acc=0.9114 val_loss=0.4355 acc=0.8588\n",
      "[Epoch 032] lr=0.00027 train_loss=0.2450 acc=0.9151 val_loss=0.3949 acc=0.8652\n",
      "[Epoch 033] lr=0.00027 train_loss=0.2370 acc=0.9169 val_loss=0.4160 acc=0.8664\n",
      "[Epoch 034] lr=0.00027 train_loss=0.2273 acc=0.9206 val_loss=0.4269 acc=0.8618\n",
      "[Epoch 035] lr=0.00027 train_loss=0.2283 acc=0.9201 val_loss=0.3958 acc=0.8710\n",
      "[Epoch 036] lr=0.00028 train_loss=0.2202 acc=0.9219 val_loss=0.4165 acc=0.8604\n",
      "[Epoch 037] lr=0.00028 train_loss=0.2168 acc=0.9244 val_loss=0.4658 acc=0.8570\n",
      "[Epoch 038] lr=0.00028 train_loss=0.2059 acc=0.9278 val_loss=0.3880 acc=0.8716\n",
      "[Epoch 039] lr=0.00028 train_loss=0.2051 acc=0.9285 val_loss=0.3711 acc=0.8794\n",
      "[Epoch 040] lr=0.00028 train_loss=0.1980 acc=0.9304 val_loss=0.3810 acc=0.8766\n",
      "FINAL TEST: loss=0.3743  top1_acc=0.8842\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc ▁▄▄▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▆▅▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc ▁▂▃▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇██▇██████▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss █▆▆▄▄▃▄▃▃▃▃▃▂▂▂▂▂▁▂▁▂▂▁▁▂▂▂▁▁▂▂▁▁▂▁▁▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.8794\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 40\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.8842\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.00028\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.93038\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 0.19798\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.8766\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 0.38103\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mpeachy-spaceship-9\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/b86vq86l\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251014_214155-b86vq86l/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m vgg6_cifar.scripts.train_experiment \\\n",
    "    --data_dir ./data \\\n",
    "    --out_dir ./runs/opt_adamw \\\n",
    "    --activation gelu \\\n",
    "    --optimizer adamw \\\n",
    "    --lr 0.001 \\\n",
    "    --batch_size 128 \\\n",
    "    --epochs 40 \\\n",
    "    --use_bn \\\n",
    "    --aug_hflip \\\n",
    "    --aug_crop \\\n",
    "    --aug_cutout \\\n",
    "    --aug_jitter \\\n",
    "    --amp \\\n",
    "    --seed 42 \\\n",
    "    --wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a77fee22-cb70-4c94-ba38-c009cf177f58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251014_214818-6332wq7a\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdry-field-10\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/6332wq7a\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.00251 train_loss=6.4460 acc=0.1000 val_loss=14.8097 acc=0.0864\n",
      "[Epoch 002] lr=0.00251 train_loss=2.3575 acc=0.1091 val_loss=2.2745 acc=0.1412\n",
      "[Epoch 003] lr=0.00252 train_loss=2.2281 acc=0.1608 val_loss=2.2966 acc=0.1030\n",
      "[Epoch 004] lr=0.00253 train_loss=1.9948 acc=0.2199 val_loss=1.7961 acc=0.2934\n",
      "[Epoch 005] lr=0.00254 train_loss=1.8331 acc=0.2803 val_loss=1.8461 acc=0.3040\n",
      "[Epoch 006] lr=0.00254 train_loss=1.7463 acc=0.3323 val_loss=1.7404 acc=0.3392\n",
      "[Epoch 007] lr=0.00255 train_loss=1.6714 acc=0.3665 val_loss=2.2793 acc=0.2712\n",
      "[Epoch 008] lr=0.00256 train_loss=1.6408 acc=0.3850 val_loss=1.5764 acc=0.4104\n",
      "[Epoch 009] lr=0.00256 train_loss=1.6138 acc=0.3989 val_loss=2.3957 acc=0.2294\n",
      "[Epoch 010] lr=0.00257 train_loss=1.6126 acc=0.4010 val_loss=4.7603 acc=0.2240\n",
      "[Epoch 011] lr=0.00258 train_loss=1.6076 acc=0.4059 val_loss=1.6859 acc=0.3736\n",
      "[Epoch 012] lr=0.00259 train_loss=1.5704 acc=0.4178 val_loss=1.9194 acc=0.3196\n",
      "[Epoch 013] lr=0.00259 train_loss=1.5610 acc=0.4328 val_loss=1.7439 acc=0.3620\n",
      "[Epoch 014] lr=0.00260 train_loss=1.5750 acc=0.4286 val_loss=1.6894 acc=0.3808\n",
      "[Epoch 015] lr=0.00261 train_loss=1.5452 acc=0.4400 val_loss=1.5614 acc=0.4026\n",
      "[Epoch 016] lr=0.00261 train_loss=1.5419 acc=0.4428 val_loss=1.5515 acc=0.4500\n",
      "[Epoch 017] lr=0.00262 train_loss=1.5495 acc=0.4387 val_loss=1.4873 acc=0.4472\n",
      "[Epoch 018] lr=0.00263 train_loss=1.5170 acc=0.4546 val_loss=1.7184 acc=0.3832\n",
      "[Epoch 019] lr=0.00263 train_loss=1.5175 acc=0.4574 val_loss=1.4968 acc=0.4518\n",
      "[Epoch 020] lr=0.00264 train_loss=1.5292 acc=0.4537 val_loss=1.7031 acc=0.3680\n",
      "[Epoch 021] lr=0.00265 train_loss=1.5040 acc=0.4600 val_loss=1.4718 acc=0.4726\n",
      "[Epoch 022] lr=0.00266 train_loss=1.5038 acc=0.4652 val_loss=1.5275 acc=0.4754\n",
      "[Epoch 023] lr=0.00266 train_loss=1.5161 acc=0.4572 val_loss=1.7563 acc=0.4322\n",
      "[Epoch 024] lr=0.00267 train_loss=1.5335 acc=0.4552 val_loss=1.9437 acc=0.2978\n",
      "[Epoch 025] lr=0.00268 train_loss=1.5223 acc=0.4612 val_loss=1.4427 acc=0.4812\n",
      "[Epoch 026] lr=0.00268 train_loss=1.5224 acc=0.4580 val_loss=1.9213 acc=0.4186\n",
      "[Epoch 027] lr=0.00269 train_loss=1.5000 acc=0.4668 val_loss=1.6909 acc=0.4036\n",
      "[Epoch 028] lr=0.00270 train_loss=1.5056 acc=0.4631 val_loss=1.7513 acc=0.3740\n",
      "[Epoch 029] lr=0.00271 train_loss=1.5191 acc=0.4586 val_loss=1.6805 acc=0.4136\n",
      "[Epoch 030] lr=0.00271 train_loss=1.5073 acc=0.4663 val_loss=1.8801 acc=0.3482\n",
      "[Epoch 031] lr=0.00272 train_loss=1.5053 acc=0.4654 val_loss=1.4235 acc=0.5120\n",
      "[Epoch 032] lr=0.00273 train_loss=1.5079 acc=0.4641 val_loss=3.7845 acc=0.3248\n",
      "[Epoch 033] lr=0.00273 train_loss=1.4997 acc=0.4679 val_loss=1.5377 acc=0.4772\n",
      "[Epoch 034] lr=0.00274 train_loss=1.5235 acc=0.4579 val_loss=1.6204 acc=0.4542\n",
      "[Epoch 035] lr=0.00275 train_loss=1.5192 acc=0.4622 val_loss=1.4873 acc=0.4684\n",
      "[Epoch 036] lr=0.00276 train_loss=1.5269 acc=0.4616 val_loss=1.6991 acc=0.3930\n",
      "[Epoch 037] lr=0.00276 train_loss=1.5066 acc=0.4644 val_loss=1.6506 acc=0.4424\n",
      "[Epoch 038] lr=0.00277 train_loss=1.5107 acc=0.4651 val_loss=2.9005 acc=0.2844\n",
      "[Epoch 039] lr=0.00278 train_loss=1.5000 acc=0.4642 val_loss=1.5649 acc=0.4706\n",
      "[Epoch 040] lr=0.00278 train_loss=1.5140 acc=0.4601 val_loss=2.7955 acc=0.3392\n",
      "FINAL TEST: loss=1.3429  top1_acc=0.5167\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc ▁▁▂▃▄▅▆▆▇▇▇▇▇▇▇█▇███████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc ▁▂▁▄▅▅▄▆▃▃▆▅▆▆▆▇▇▆▇▆▇▇▇▄▇▆▆▆▆▅█▅▇▇▇▆▇▄▇▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss █▁▁▁▁▁▁▁▂▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▂▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 40\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.5167\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.00278\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.46011\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 1.51401\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.3392\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 2.79548\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mdry-field-10\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/6332wq7a\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251014_214818-6332wq7a/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m vgg6_cifar.scripts.train_experiment \\\n",
    "    --data_dir ./data \\\n",
    "    --out_dir ./runs/opt_rmsprop \\\n",
    "    --activation gelu \\\n",
    "    --optimizer rmsprop \\\n",
    "    --lr 0.01 \\\n",
    "    --batch_size 128 \\\n",
    "    --epochs 40 \\\n",
    "    --use_bn \\\n",
    "    --aug_hflip \\\n",
    "    --aug_crop \\\n",
    "    --aug_cutout \\\n",
    "    --aug_jitter \\\n",
    "    --amp \\\n",
    "    --seed 42 \\\n",
    "    --wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e41bc6f0-3dbc-47a7-9a2a-14288e1ad33b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251014_215437-1bv3e0vl\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mearnest-sea-11\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/1bv3e0vl\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.00025 train_loss=1.4220 acc=0.4791 val_loss=1.1828 acc=0.5680\n",
      "[Epoch 002] lr=0.00025 train_loss=0.9870 acc=0.6481 val_loss=1.0663 acc=0.6156\n",
      "[Epoch 003] lr=0.00025 train_loss=0.8289 acc=0.7089 val_loss=0.8965 acc=0.6778\n",
      "[Epoch 004] lr=0.00025 train_loss=0.7385 acc=0.7439 val_loss=0.7573 acc=0.7338\n",
      "[Epoch 005] lr=0.00025 train_loss=0.6690 acc=0.7677 val_loss=0.7592 acc=0.7338\n",
      "[Epoch 006] lr=0.00025 train_loss=0.6260 acc=0.7822 val_loss=0.6464 acc=0.7742\n",
      "[Epoch 007] lr=0.00025 train_loss=0.5845 acc=0.7973 val_loss=0.6133 acc=0.7866\n",
      "[Epoch 008] lr=0.00026 train_loss=0.5506 acc=0.8116 val_loss=0.6172 acc=0.7828\n",
      "[Epoch 009] lr=0.00026 train_loss=0.5210 acc=0.8204 val_loss=0.5381 acc=0.8116\n",
      "[Epoch 010] lr=0.00026 train_loss=0.4961 acc=0.8278 val_loss=0.5505 acc=0.8032\n",
      "[Epoch 011] lr=0.00026 train_loss=0.4673 acc=0.8390 val_loss=0.5387 acc=0.8100\n",
      "[Epoch 012] lr=0.00026 train_loss=0.4512 acc=0.8450 val_loss=0.5625 acc=0.8032\n",
      "[Epoch 013] lr=0.00026 train_loss=0.4281 acc=0.8513 val_loss=0.5115 acc=0.8182\n",
      "[Epoch 014] lr=0.00026 train_loss=0.4122 acc=0.8571 val_loss=0.5581 acc=0.8064\n",
      "[Epoch 015] lr=0.00026 train_loss=0.3968 acc=0.8637 val_loss=0.5408 acc=0.8110\n",
      "[Epoch 016] lr=0.00026 train_loss=0.3868 acc=0.8654 val_loss=0.4563 acc=0.8392\n",
      "[Epoch 017] lr=0.00026 train_loss=0.3702 acc=0.8718 val_loss=0.5059 acc=0.8210\n",
      "[Epoch 018] lr=0.00026 train_loss=0.3617 acc=0.8748 val_loss=0.4473 acc=0.8444\n",
      "[Epoch 019] lr=0.00026 train_loss=0.3491 acc=0.8787 val_loss=0.5496 acc=0.8162\n",
      "[Epoch 020] lr=0.00026 train_loss=0.3373 acc=0.8826 val_loss=0.4354 acc=0.8496\n",
      "[Epoch 021] lr=0.00026 train_loss=0.3257 acc=0.8875 val_loss=0.4190 acc=0.8586\n",
      "[Epoch 022] lr=0.00027 train_loss=0.3172 acc=0.8887 val_loss=0.4596 acc=0.8440\n",
      "[Epoch 023] lr=0.00027 train_loss=0.3067 acc=0.8943 val_loss=0.4286 acc=0.8532\n",
      "[Epoch 024] lr=0.00027 train_loss=0.2966 acc=0.8966 val_loss=0.3879 acc=0.8676\n",
      "[Epoch 025] lr=0.00027 train_loss=0.2893 acc=0.8989 val_loss=0.4323 acc=0.8516\n",
      "[Epoch 026] lr=0.00027 train_loss=0.2839 acc=0.9027 val_loss=0.3884 acc=0.8684\n",
      "[Epoch 027] lr=0.00027 train_loss=0.2787 acc=0.9032 val_loss=0.4305 acc=0.8578\n",
      "[Epoch 028] lr=0.00027 train_loss=0.2716 acc=0.9062 val_loss=0.4038 acc=0.8602\n",
      "[Epoch 029] lr=0.00027 train_loss=0.2651 acc=0.9070 val_loss=0.4522 acc=0.8510\n",
      "[Epoch 030] lr=0.00027 train_loss=0.2576 acc=0.9110 val_loss=0.3828 acc=0.8766\n",
      "[Epoch 031] lr=0.00027 train_loss=0.2540 acc=0.9112 val_loss=0.3884 acc=0.8730\n",
      "[Epoch 032] lr=0.00027 train_loss=0.2454 acc=0.9148 val_loss=0.4448 acc=0.8552\n",
      "[Epoch 033] lr=0.00027 train_loss=0.2404 acc=0.9164 val_loss=0.3948 acc=0.8708\n",
      "[Epoch 034] lr=0.00027 train_loss=0.2354 acc=0.9187 val_loss=0.4242 acc=0.8616\n",
      "[Epoch 035] lr=0.00027 train_loss=0.2341 acc=0.9176 val_loss=0.4000 acc=0.8648\n",
      "[Epoch 036] lr=0.00028 train_loss=0.2278 acc=0.9212 val_loss=0.4200 acc=0.8632\n",
      "[Epoch 037] lr=0.00028 train_loss=0.2267 acc=0.9211 val_loss=0.3945 acc=0.8644\n",
      "[Epoch 038] lr=0.00028 train_loss=0.2173 acc=0.9236 val_loss=0.4283 acc=0.8606\n",
      "[Epoch 039] lr=0.00028 train_loss=0.2133 acc=0.9266 val_loss=0.3948 acc=0.8634\n",
      "[Epoch 040] lr=0.00028 train_loss=0.2116 acc=0.9278 val_loss=0.3775 acc=0.8754\n",
      "FINAL TEST: loss=0.3662  top1_acc=0.8802\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc ▁▄▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇█████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▅▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc ▁▂▃▅▅▆▆▆▇▆▆▆▇▆▇▇▇▇▇▇█▇▇█▇███▇███████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss █▇▆▄▄▃▃▃▂▃▂▃▂▃▂▂▂▂▂▂▁▂▁▁▁▁▁▁▂▁▁▂▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.8766\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 40\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.8802\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.00028\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.92782\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 0.21162\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.8754\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 0.37747\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mearnest-sea-11\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/1bv3e0vl\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251014_215437-1bv3e0vl/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m vgg6_cifar.scripts.train_experiment \\\n",
    "    --data_dir ./data \\\n",
    "    --out_dir ./runs/opt_nadam \\\n",
    "    --activation gelu \\\n",
    "    --optimizer nadam \\\n",
    "    --lr 0.001 \\\n",
    "    --batch_size 128 \\\n",
    "    --epochs 40 \\\n",
    "    --use_bn \\\n",
    "    --aug_hflip \\\n",
    "    --aug_crop \\\n",
    "    --aug_cutout \\\n",
    "    --aug_jitter \\\n",
    "    --amp \\\n",
    "    --seed 42 \\\n",
    "    --wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80fea0b3-746c-4836-b2ae-50613d86de96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m setting up run msgg26t9 (0.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251014_220057-msgg26t9\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlemon-silence-12\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/msgg26t9\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.01254 train_loss=2.3137 acc=0.2742 val_loss=1.8841 acc=0.3214\n",
      "[Epoch 002] lr=0.01257 train_loss=1.4665 acc=0.4624 val_loss=1.2735 acc=0.5370\n",
      "[Epoch 003] lr=0.01261 train_loss=1.2111 acc=0.5637 val_loss=1.2592 acc=0.5414\n",
      "[Epoch 004] lr=0.01264 train_loss=1.0301 acc=0.6330 val_loss=0.9379 acc=0.6578\n",
      "[Epoch 005] lr=0.01268 train_loss=0.8968 acc=0.6859 val_loss=0.9639 acc=0.6580\n",
      "[Epoch 006] lr=0.01271 train_loss=0.7988 acc=0.7229 val_loss=0.7555 acc=0.7254\n",
      "[Epoch 007] lr=0.01275 train_loss=0.7205 acc=0.7515 val_loss=0.7542 acc=0.7378\n",
      "[Epoch 008] lr=0.01278 train_loss=0.6666 acc=0.7688 val_loss=0.6547 acc=0.7706\n",
      "[Epoch 009] lr=0.01282 train_loss=0.6170 acc=0.7904 val_loss=0.6354 acc=0.7760\n",
      "[Epoch 010] lr=0.01286 train_loss=0.5836 acc=0.8014 val_loss=0.6523 acc=0.7730\n",
      "[Epoch 011] lr=0.01289 train_loss=0.5454 acc=0.8142 val_loss=0.5885 acc=0.7926\n",
      "[Epoch 012] lr=0.01293 train_loss=0.5191 acc=0.8226 val_loss=0.5329 acc=0.8140\n",
      "[Epoch 013] lr=0.01296 train_loss=0.4882 acc=0.8327 val_loss=0.5160 acc=0.8178\n",
      "[Epoch 014] lr=0.01300 train_loss=0.4638 acc=0.8436 val_loss=0.5375 acc=0.8126\n",
      "[Epoch 015] lr=0.01303 train_loss=0.4445 acc=0.8490 val_loss=0.5972 acc=0.7986\n",
      "[Epoch 016] lr=0.01307 train_loss=0.4266 acc=0.8539 val_loss=0.5677 acc=0.7984\n",
      "[Epoch 017] lr=0.01310 train_loss=0.4058 acc=0.8618 val_loss=0.5698 acc=0.8006\n",
      "[Epoch 018] lr=0.01314 train_loss=0.3883 acc=0.8678 val_loss=0.5115 acc=0.8248\n",
      "[Epoch 019] lr=0.01317 train_loss=0.3770 acc=0.8716 val_loss=0.5955 acc=0.7968\n",
      "[Epoch 020] lr=0.01321 train_loss=0.3618 acc=0.8767 val_loss=0.4347 acc=0.8516\n",
      "[Epoch 021] lr=0.01325 train_loss=0.3512 acc=0.8816 val_loss=0.4129 acc=0.8544\n",
      "[Epoch 022] lr=0.01328 train_loss=0.3389 acc=0.8828 val_loss=0.4563 acc=0.8400\n",
      "[Epoch 023] lr=0.01332 train_loss=0.3244 acc=0.8898 val_loss=0.5204 acc=0.8240\n",
      "[Epoch 024] lr=0.01335 train_loss=0.3132 acc=0.8932 val_loss=0.4276 acc=0.8574\n",
      "[Epoch 025] lr=0.01339 train_loss=0.3022 acc=0.8958 val_loss=0.4435 acc=0.8514\n",
      "[Epoch 026] lr=0.01342 train_loss=0.2958 acc=0.8988 val_loss=0.4140 acc=0.8596\n",
      "[Epoch 027] lr=0.01346 train_loss=0.2863 acc=0.9012 val_loss=0.4489 acc=0.8452\n",
      "[Epoch 028] lr=0.01349 train_loss=0.2766 acc=0.9049 val_loss=0.3907 acc=0.8640\n",
      "[Epoch 029] lr=0.01353 train_loss=0.2689 acc=0.9075 val_loss=0.4314 acc=0.8524\n",
      "[Epoch 030] lr=0.01357 train_loss=0.2593 acc=0.9109 val_loss=0.4086 acc=0.8626\n",
      "[Epoch 031] lr=0.01360 train_loss=0.2556 acc=0.9120 val_loss=0.4038 acc=0.8646\n",
      "[Epoch 032] lr=0.01364 train_loss=0.2428 acc=0.9170 val_loss=0.4213 acc=0.8640\n",
      "[Epoch 033] lr=0.01367 train_loss=0.2373 acc=0.9188 val_loss=0.4010 acc=0.8704\n",
      "[Epoch 034] lr=0.01371 train_loss=0.2327 acc=0.9198 val_loss=0.3654 acc=0.8768\n",
      "[Epoch 035] lr=0.01374 train_loss=0.2247 acc=0.9218 val_loss=0.4040 acc=0.8656\n",
      "[Epoch 036] lr=0.01378 train_loss=0.2166 acc=0.9258 val_loss=0.4027 acc=0.8670\n",
      "[Epoch 037] lr=0.01381 train_loss=0.2154 acc=0.9254 val_loss=0.3744 acc=0.8750\n",
      "[Epoch 038] lr=0.01385 train_loss=0.2084 acc=0.9280 val_loss=0.3691 acc=0.8786\n",
      "[Epoch 039] lr=0.01388 train_loss=0.2006 acc=0.9304 val_loss=0.4824 acc=0.8418\n",
      "[Epoch 040] lr=0.01392 train_loss=0.1961 acc=0.9324 val_loss=0.3553 acc=0.8814\n",
      "FINAL TEST: loss=0.3403  top1_acc=0.8922\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Finishing up...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m Finishing up...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m Finishing up...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m Finishing up...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m Finishing up...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m Finishing up...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣟\u001b[0m Finishing up...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc ▁▃▄▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇██████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc ▁▄▄▅▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇██▇▇█████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss █▅▅▄▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.8814\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 40\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.8922\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.01392\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.93236\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 0.19606\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.8814\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 0.35531\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mlemon-silence-12\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/msgg26t9\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251014_220057-msgg26t9/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m vgg6_cifar.scripts.train_experiment \\\n",
    "    --data_dir ./data \\\n",
    "    --out_dir ./runs/opt_adagrad \\\n",
    "    --activation gelu \\\n",
    "    --optimizer adagrad \\\n",
    "    --lr 0.05 \\\n",
    "    --batch_size 128 \\\n",
    "    --epochs 40 \\\n",
    "    --use_bn \\\n",
    "    --aug_hflip \\\n",
    "    --aug_crop \\\n",
    "    --aug_cutout \\\n",
    "    --aug_jitter \\\n",
    "    --amp \\\n",
    "    --seed 42 \\\n",
    "    --wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3025600b-410f-4e24-a0f5-0f303c496101",
   "metadata": {},
   "source": [
    "# combined command for optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2944bdd7-9af2-42de-920f-fc03dd9f161d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers = [\n",
    "    (\"sgd\", \"gelu\", 0.1),\n",
    "    (\"nesterov-sgd\", \"gelu\", 0.1),\n",
    "    (\"adam\", \"gelu\", 0.001),\n",
    "    (\"adamw\", \"gelu\", 0.001),\n",
    "    (\"rmsprop\", \"gelu\", 0.001),\n",
    "    (\"nadam\", \"gelu\", 0.001),\n",
    "    (\"adagrad\", \"gelu\", 0.05)\n",
    "]\n",
    "\n",
    "for opt, activation, lr in optimizers:\n",
    "    cmd = f\"\"\"\n",
    "    python -m vgg6_cifar.scripts.train_experiment \\\n",
    "        --data_dir ./data \\\n",
    "        --out_dir ./runs/opt_{opt} \\\n",
    "        --activation {activation} \\\n",
    "        --optimizer {opt} \\\n",
    "        --lr {lr} \\\n",
    "        --batch_size 128 \\\n",
    "        --epochs 40 \\\n",
    "        --use_bn \\\n",
    "        --aug_hflip \\\n",
    "        --aug_crop \\\n",
    "        --aug_cutout \\\n",
    "        --aug_jitter \\\n",
    "        --amp \\\n",
    "        --seed 42 \\\n",
    "        --wandb\n",
    "    \"\"\"\n",
    "    print(f\"Running: {opt}\")\n",
    "    !{cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646512a7-1393-43cb-8aa8-13b9a6d1fb36",
   "metadata": {},
   "source": [
    "### cmd command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f42141e-45f2-41da-bda8-b9f3a847cd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "for opt in sgd nesterov-sgd adam adamw rmsprop nadam adagrad\n",
    "do\n",
    "  python -m vgg6_cifar.scripts.train_experiment \\\n",
    "    --data_dir ./data \\\n",
    "    --out_dir ./runs/opt_${opt} \\\n",
    "    --activation gelu \\\n",
    "    --optimizer ${opt} \\\n",
    "    --lr 0.1 \\\n",
    "    --batch_size 128 \\\n",
    "    --epochs 40 \\\n",
    "    --use_bn \\\n",
    "    --aug_hflip \\\n",
    "    --aug_crop \\\n",
    "    --aug_cutout \\\n",
    "    --aug_jitter \\\n",
    "    --amp \\\n",
    "    --seed 42 \\\n",
    "    --wandb\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7032cfaa-0b4a-419e-be1d-9c24126f42ab",
   "metadata": {},
   "source": [
    "# Q2(c) Vary batch size, epochs, learning rate, momentum, BN\n",
    "### Two illustrative runs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3133f09f-5ffa-472f-a608-000ee529c7e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m setting up run c2j40iob (0.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251014_221555-c2j40iob\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtough-armadillo-14\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/c2j40iob\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.00200 train_loss=1.4540 acc=0.4667 val_loss=1.5714 acc=0.4534\n",
      "[Epoch 002] lr=0.00201 train_loss=1.1047 acc=0.6042 val_loss=1.0817 acc=0.6098\n",
      "[Epoch 003] lr=0.00201 train_loss=0.9728 acc=0.6558 val_loss=0.9819 acc=0.6516\n",
      "[Epoch 004] lr=0.00201 train_loss=0.8940 acc=0.6848 val_loss=0.8838 acc=0.6840\n",
      "[Epoch 005] lr=0.00201 train_loss=0.8392 acc=0.7060 val_loss=0.8606 acc=0.6942\n",
      "[Epoch 006] lr=0.00202 train_loss=0.8003 acc=0.7185 val_loss=0.7436 acc=0.7410\n",
      "[Epoch 007] lr=0.00202 train_loss=0.7645 acc=0.7339 val_loss=0.7592 acc=0.7270\n",
      "[Epoch 008] lr=0.00202 train_loss=0.7344 acc=0.7441 val_loss=0.8395 acc=0.7008\n",
      "[Epoch 009] lr=0.00203 train_loss=0.7152 acc=0.7507 val_loss=0.6849 acc=0.7584\n",
      "[Epoch 010] lr=0.00203 train_loss=0.6949 acc=0.7596 val_loss=0.6898 acc=0.7512\n",
      "[Epoch 011] lr=0.00203 train_loss=0.6738 acc=0.7672 val_loss=0.7519 acc=0.7344\n",
      "[Epoch 012] lr=0.00203 train_loss=0.6540 acc=0.7722 val_loss=0.6447 acc=0.7704\n",
      "[Epoch 013] lr=0.00204 train_loss=0.6323 acc=0.7808 val_loss=0.6882 acc=0.7506\n",
      "[Epoch 014] lr=0.00204 train_loss=0.6268 acc=0.7816 val_loss=0.6125 acc=0.7798\n",
      "[Epoch 015] lr=0.00204 train_loss=0.6169 acc=0.7875 val_loss=0.6707 acc=0.7636\n",
      "[Epoch 016] lr=0.00205 train_loss=0.6008 acc=0.7920 val_loss=0.7231 acc=0.7510\n",
      "[Epoch 017] lr=0.00205 train_loss=0.5951 acc=0.7938 val_loss=0.6294 acc=0.7786\n",
      "[Epoch 018] lr=0.00205 train_loss=0.5834 acc=0.7981 val_loss=0.5796 acc=0.7930\n",
      "[Epoch 019] lr=0.00205 train_loss=0.5751 acc=0.8024 val_loss=0.5944 acc=0.7852\n",
      "[Epoch 020] lr=0.00206 train_loss=0.5653 acc=0.8037 val_loss=0.5784 acc=0.7964\n",
      "[Epoch 021] lr=0.00206 train_loss=0.5583 acc=0.8082 val_loss=0.5664 acc=0.7938\n",
      "[Epoch 022] lr=0.00206 train_loss=0.5509 acc=0.8099 val_loss=0.5700 acc=0.8000\n",
      "[Epoch 023] lr=0.00207 train_loss=0.5407 acc=0.8134 val_loss=0.5521 acc=0.8078\n",
      "[Epoch 024] lr=0.00207 train_loss=0.5338 acc=0.8157 val_loss=0.5698 acc=0.7982\n",
      "[Epoch 025] lr=0.00207 train_loss=0.5283 acc=0.8170 val_loss=0.5869 acc=0.7896\n",
      "[Epoch 026] lr=0.00207 train_loss=0.5191 acc=0.8209 val_loss=0.5535 acc=0.8058\n",
      "[Epoch 027] lr=0.00208 train_loss=0.5130 acc=0.8233 val_loss=0.5358 acc=0.8070\n",
      "[Epoch 028] lr=0.00208 train_loss=0.5108 acc=0.8241 val_loss=0.5635 acc=0.8014\n",
      "[Epoch 029] lr=0.00208 train_loss=0.5015 acc=0.8263 val_loss=0.5270 acc=0.8128\n",
      "[Epoch 030] lr=0.00209 train_loss=0.4985 acc=0.8290 val_loss=0.5389 acc=0.8094\n",
      "[Epoch 031] lr=0.00209 train_loss=0.4905 acc=0.8305 val_loss=0.5608 acc=0.8012\n",
      "[Epoch 032] lr=0.00209 train_loss=0.4878 acc=0.8324 val_loss=0.5126 acc=0.8194\n",
      "[Epoch 033] lr=0.00209 train_loss=0.4827 acc=0.8328 val_loss=0.5250 acc=0.8148\n",
      "[Epoch 034] lr=0.00210 train_loss=0.4747 acc=0.8363 val_loss=0.5157 acc=0.8182\n",
      "[Epoch 035] lr=0.00210 train_loss=0.4690 acc=0.8403 val_loss=0.4990 acc=0.8264\n",
      "[Epoch 036] lr=0.00210 train_loss=0.4718 acc=0.8385 val_loss=0.5035 acc=0.8230\n",
      "[Epoch 037] lr=0.00211 train_loss=0.4622 acc=0.8401 val_loss=0.4967 acc=0.8244\n",
      "[Epoch 038] lr=0.00211 train_loss=0.4567 acc=0.8420 val_loss=0.5058 acc=0.8196\n",
      "[Epoch 039] lr=0.00211 train_loss=0.4509 acc=0.8470 val_loss=0.5002 acc=0.8236\n",
      "[Epoch 040] lr=0.00211 train_loss=0.4468 acc=0.8468 val_loss=0.5106 acc=0.8188\n",
      "[Epoch 041] lr=0.00212 train_loss=0.4448 acc=0.8465 val_loss=0.5090 acc=0.8244\n",
      "[Epoch 042] lr=0.00212 train_loss=0.4367 acc=0.8490 val_loss=0.4983 acc=0.8254\n",
      "[Epoch 043] lr=0.00212 train_loss=0.4376 acc=0.8483 val_loss=0.5049 acc=0.8236\n",
      "[Epoch 044] lr=0.00213 train_loss=0.4342 acc=0.8496 val_loss=0.4947 acc=0.8282\n",
      "[Epoch 045] lr=0.00213 train_loss=0.4272 acc=0.8516 val_loss=0.4894 acc=0.8256\n",
      "[Epoch 046] lr=0.00213 train_loss=0.4282 acc=0.8533 val_loss=0.4730 acc=0.8334\n",
      "[Epoch 047] lr=0.00213 train_loss=0.4243 acc=0.8542 val_loss=0.4860 acc=0.8348\n",
      "[Epoch 048] lr=0.00214 train_loss=0.4231 acc=0.8542 val_loss=0.4890 acc=0.8262\n",
      "[Epoch 049] lr=0.00214 train_loss=0.4120 acc=0.8586 val_loss=0.4804 acc=0.8302\n",
      "[Epoch 050] lr=0.00214 train_loss=0.4151 acc=0.8564 val_loss=0.4746 acc=0.8380\n",
      "[Epoch 051] lr=0.00214 train_loss=0.4122 acc=0.8572 val_loss=0.4769 acc=0.8330\n",
      "[Epoch 052] lr=0.00215 train_loss=0.4100 acc=0.8582 val_loss=0.4794 acc=0.8304\n",
      "[Epoch 053] lr=0.00215 train_loss=0.4059 acc=0.8587 val_loss=0.4817 acc=0.8294\n",
      "[Epoch 054] lr=0.00215 train_loss=0.4036 acc=0.8602 val_loss=0.5117 acc=0.8204\n",
      "[Epoch 055] lr=0.00216 train_loss=0.4008 acc=0.8617 val_loss=0.4644 acc=0.8386\n",
      "[Epoch 056] lr=0.00216 train_loss=0.3934 acc=0.8643 val_loss=0.4475 acc=0.8422\n",
      "[Epoch 057] lr=0.00216 train_loss=0.3947 acc=0.8650 val_loss=0.4676 acc=0.8340\n",
      "[Epoch 058] lr=0.00216 train_loss=0.3845 acc=0.8687 val_loss=0.5041 acc=0.8278\n",
      "[Epoch 059] lr=0.00217 train_loss=0.3856 acc=0.8670 val_loss=0.4566 acc=0.8398\n",
      "[Epoch 060] lr=0.00217 train_loss=0.3880 acc=0.8656 val_loss=0.5043 acc=0.8234\n",
      "[Epoch 061] lr=0.00217 train_loss=0.3819 acc=0.8682 val_loss=0.4656 acc=0.8354\n",
      "[Epoch 062] lr=0.00218 train_loss=0.3774 acc=0.8696 val_loss=0.4659 acc=0.8380\n",
      "[Epoch 063] lr=0.00218 train_loss=0.3831 acc=0.8674 val_loss=0.4487 acc=0.8388\n",
      "[Epoch 064] lr=0.00218 train_loss=0.3739 acc=0.8722 val_loss=0.4576 acc=0.8376\n",
      "[Epoch 065] lr=0.00218 train_loss=0.3738 acc=0.8712 val_loss=0.4475 acc=0.8390\n",
      "[Epoch 066] lr=0.00219 train_loss=0.3691 acc=0.8731 val_loss=0.5341 acc=0.8136\n",
      "[Epoch 067] lr=0.00219 train_loss=0.3685 acc=0.8726 val_loss=0.4422 acc=0.8434\n",
      "[Epoch 068] lr=0.00219 train_loss=0.3650 acc=0.8754 val_loss=0.4979 acc=0.8262\n",
      "[Epoch 069] lr=0.00220 train_loss=0.3629 acc=0.8754 val_loss=0.4466 acc=0.8428\n",
      "[Epoch 070] lr=0.00220 train_loss=0.3621 acc=0.8750 val_loss=0.4458 acc=0.8428\n",
      "[Epoch 071] lr=0.00220 train_loss=0.3554 acc=0.8763 val_loss=0.4552 acc=0.8400\n",
      "[Epoch 072] lr=0.00220 train_loss=0.3573 acc=0.8773 val_loss=0.4634 acc=0.8404\n",
      "[Epoch 073] lr=0.00221 train_loss=0.3533 acc=0.8786 val_loss=0.4574 acc=0.8412\n",
      "[Epoch 074] lr=0.00221 train_loss=0.3498 acc=0.8799 val_loss=0.4693 acc=0.8382\n",
      "[Epoch 075] lr=0.00221 train_loss=0.3497 acc=0.8790 val_loss=0.4756 acc=0.8362\n",
      "[Epoch 076] lr=0.00222 train_loss=0.3490 acc=0.8810 val_loss=0.4414 acc=0.8422\n",
      "[Epoch 077] lr=0.00222 train_loss=0.3424 acc=0.8814 val_loss=0.4752 acc=0.8334\n",
      "[Epoch 078] lr=0.00222 train_loss=0.3474 acc=0.8777 val_loss=0.4499 acc=0.8462\n",
      "[Epoch 079] lr=0.00222 train_loss=0.3428 acc=0.8810 val_loss=0.4498 acc=0.8404\n",
      "[Epoch 080] lr=0.00223 train_loss=0.3391 acc=0.8822 val_loss=0.4932 acc=0.8288\n",
      "FINAL TEST: loss=0.4143  top1_acc=0.8599\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣟\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc ▁▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▆▅▅▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc ▁▅▅▅▅▇▆▇▇▇▇▇▇▇▇██████████████████▇██████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss █▇▅▅▆▅▄▄▃▃▃▂▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▂▁▁▁▁▁▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.8462\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 80\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.8599\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.00223\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.88218\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 0.33911\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.8288\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 0.49324\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mtough-armadillo-14\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/c2j40iob\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251014_221555-c2j40iob/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Small BS, longer train, smaller LR, momentum 0.9, BN on\n",
    "!python -m vgg6_cifar.scripts.train_experiment \\\n",
    "  --data_dir ./data --out_dir ./runs/hp_bs64_e80_lr005_m09_bnTrue \\\n",
    "  --activation gelu --optimizer adagrad --lr 0.01 --batch_size 64 --epochs 80 \\\n",
    "  --momentum 0.9 --use_bn \\\n",
    "  --aug_hflip --aug_crop --aug_cutout --aug_jitter --amp --seed 42 --wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6623152-79b5-4f0b-a9ba-347bf8c243b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m setting up run b5vw3qau (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251014_225037-b5vw3qau\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msolar-wave-16\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/b5vw3qau\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.00251 train_loss=1.9452 acc=0.2902 val_loss=1.7253 acc=0.3684\n",
      "[Epoch 002] lr=0.00253 train_loss=1.6403 acc=0.3994 val_loss=1.5546 acc=0.4336\n",
      "[Epoch 003] lr=0.00254 train_loss=1.5160 acc=0.4488 val_loss=1.4499 acc=0.4688\n",
      "[Epoch 004] lr=0.00256 train_loss=1.4291 acc=0.4827 val_loss=1.3638 acc=0.5094\n",
      "[Epoch 005] lr=0.00257 train_loss=1.3655 acc=0.5075 val_loss=1.3530 acc=0.5202\n",
      "[Epoch 006] lr=0.00259 train_loss=1.3066 acc=0.5315 val_loss=1.2628 acc=0.5474\n",
      "[Epoch 007] lr=0.00260 train_loss=1.2512 acc=0.5488 val_loss=1.2024 acc=0.5696\n",
      "[Epoch 008] lr=0.00261 train_loss=1.2046 acc=0.5682 val_loss=1.1624 acc=0.5754\n",
      "[Epoch 009] lr=0.00263 train_loss=1.1667 acc=0.5844 val_loss=1.1121 acc=0.6038\n",
      "[Epoch 010] lr=0.00264 train_loss=1.1267 acc=0.5982 val_loss=1.1119 acc=0.5952\n",
      "[Epoch 011] lr=0.00266 train_loss=1.0909 acc=0.6117 val_loss=1.0598 acc=0.6200\n",
      "[Epoch 012] lr=0.00267 train_loss=1.0602 acc=0.6199 val_loss=1.0449 acc=0.6268\n",
      "[Epoch 013] lr=0.00268 train_loss=1.0336 acc=0.6337 val_loss=1.0104 acc=0.6414\n",
      "[Epoch 014] lr=0.00270 train_loss=1.0112 acc=0.6442 val_loss=0.9904 acc=0.6488\n",
      "[Epoch 015] lr=0.00271 train_loss=0.9854 acc=0.6514 val_loss=0.9472 acc=0.6664\n",
      "[Epoch 016] lr=0.00273 train_loss=0.9611 acc=0.6621 val_loss=0.9323 acc=0.6628\n",
      "[Epoch 017] lr=0.00274 train_loss=0.9416 acc=0.6686 val_loss=0.9385 acc=0.6676\n",
      "[Epoch 018] lr=0.00276 train_loss=0.9239 acc=0.6753 val_loss=0.9370 acc=0.6644\n",
      "[Epoch 019] lr=0.00277 train_loss=0.9100 acc=0.6821 val_loss=0.8831 acc=0.6906\n",
      "[Epoch 020] lr=0.00278 train_loss=0.8919 acc=0.6873 val_loss=0.8938 acc=0.6834\n",
      "[Epoch 021] lr=0.00280 train_loss=0.8778 acc=0.6912 val_loss=0.8627 acc=0.6908\n",
      "[Epoch 022] lr=0.00281 train_loss=0.8604 acc=0.6979 val_loss=0.8616 acc=0.6950\n",
      "[Epoch 023] lr=0.00283 train_loss=0.8528 acc=0.7006 val_loss=0.8321 acc=0.7054\n",
      "[Epoch 024] lr=0.00284 train_loss=0.8331 acc=0.7066 val_loss=0.8267 acc=0.7022\n",
      "[Epoch 025] lr=0.00286 train_loss=0.8229 acc=0.7112 val_loss=0.8268 acc=0.7020\n",
      "[Epoch 026] lr=0.00287 train_loss=0.8112 acc=0.7147 val_loss=0.8046 acc=0.7130\n",
      "[Epoch 027] lr=0.00288 train_loss=0.8028 acc=0.7179 val_loss=0.7910 acc=0.7142\n",
      "[Epoch 028] lr=0.00290 train_loss=0.7921 acc=0.7230 val_loss=0.7936 acc=0.7198\n",
      "[Epoch 029] lr=0.00291 train_loss=0.7776 acc=0.7262 val_loss=0.8052 acc=0.7152\n",
      "[Epoch 030] lr=0.00293 train_loss=0.7679 acc=0.7310 val_loss=0.7792 acc=0.7262\n",
      "[Epoch 031] lr=0.00294 train_loss=0.7585 acc=0.7339 val_loss=0.7642 acc=0.7268\n",
      "[Epoch 032] lr=0.00295 train_loss=0.7448 acc=0.7401 val_loss=0.7541 acc=0.7330\n",
      "[Epoch 033] lr=0.00297 train_loss=0.7360 acc=0.7424 val_loss=0.7462 acc=0.7318\n",
      "[Epoch 034] lr=0.00298 train_loss=0.7303 acc=0.7446 val_loss=0.7232 acc=0.7440\n",
      "[Epoch 035] lr=0.00300 train_loss=0.7210 acc=0.7481 val_loss=0.7277 acc=0.7368\n",
      "[Epoch 036] lr=0.00301 train_loss=0.7159 acc=0.7520 val_loss=0.7156 acc=0.7412\n",
      "[Epoch 037] lr=0.00303 train_loss=0.7017 acc=0.7551 val_loss=0.7192 acc=0.7440\n",
      "[Epoch 038] lr=0.00304 train_loss=0.6975 acc=0.7575 val_loss=0.7049 acc=0.7494\n",
      "[Epoch 039] lr=0.00305 train_loss=0.6907 acc=0.7574 val_loss=0.7045 acc=0.7474\n",
      "[Epoch 040] lr=0.00307 train_loss=0.6800 acc=0.7618 val_loss=0.7142 acc=0.7510\n",
      "FINAL TEST: loss=0.6493  top1_acc=0.7734\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc ▁▃▃▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇███████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▆▆▅▅▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc ▁▂▃▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇███████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss █▇▆▆▅▅▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.751\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 40\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.7734\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.00307\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.7618\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 0.68003\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.751\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 0.71421\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33msolar-wave-16\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/b5vw3qau\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251014_225037-b5vw3qau/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Large BS, shorter train, big LR, momentum 0.0, BN off\n",
    "!python -m vgg6_cifar.scripts.train_experiment \\\n",
    "  --data_dir ./data --out_dir ./runs/hp_bs256_e40_lr001_m00_bnFalse \\\n",
    "  --activation gelu --optimizer adagrad --lr 0.01 --batch_size 256 --epochs 40 \\\n",
    "  --momentum 0.0 --no_bn \\\n",
    "  --aug_hflip --aug_crop --aug_cutout --aug_jitter --amp --seed 42 --wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a29e72a7-4bec-4068-a453-ec288992fdd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> \n",
      "            python -m vgg6_cifar.scripts.train_experiment                 --data_dir ./data                 --out_dir ./runs/sweeps_ext/actrelu_bnTrue_bs32_e20_lr0.1_m0.1_sgd                 --activation relu                 --batch_size 32                 --epochs 20                 --lr 0.1                 --momentum 0.1                 --optimizer sgd                 --weight_decay 0.0005                 --label_smoothing 0.0                 --seed 42                 --aug_hflip                 --aug_crop                 --aug_cutout                 --aug_jitter                 --use_bn                 --amp                 --wandb --wandb_project vgg6-cifar10-assignment --wandb_group q2q5-sweeps --run_name i0001\n",
      "            \n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m setting up run ppbwzx4s (0.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251015_023323-ppbwzx4s\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mi0001\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/ppbwzx4s\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.05004 train_loss=1.6800 acc=0.3784 val_loss=1.4687 acc=0.4534\n",
      "[Epoch 002] lr=0.05007 train_loss=1.2905 acc=0.5395 val_loss=1.3416 acc=0.5268\n",
      "[Epoch 003] lr=0.05011 train_loss=1.0658 acc=0.6249 val_loss=0.8880 acc=0.6844\n",
      "[Epoch 004] lr=0.05014 train_loss=0.9249 acc=0.6794 val_loss=1.0222 acc=0.6412\n",
      "[Epoch 005] lr=0.05018 train_loss=0.8330 acc=0.7147 val_loss=0.8569 acc=0.7034\n",
      "[Epoch 006] lr=0.05021 train_loss=0.7557 acc=0.7414 val_loss=0.8517 acc=0.7034\n",
      "[Epoch 007] lr=0.05025 train_loss=0.6942 acc=0.7624 val_loss=0.9514 acc=0.6674\n",
      "[Epoch 008] lr=0.05028 train_loss=0.6386 acc=0.7826 val_loss=0.6552 acc=0.7680\n",
      "[Epoch 009] lr=0.05032 train_loss=0.6002 acc=0.7958 val_loss=0.5837 acc=0.7976\n",
      "[Epoch 010] lr=0.05036 train_loss=0.5640 acc=0.8065 val_loss=0.6948 acc=0.7618\n",
      "[Epoch 011] lr=0.05039 train_loss=0.5328 acc=0.8190 val_loss=0.5702 acc=0.8048\n",
      "[Epoch 012] lr=0.05043 train_loss=0.5067 acc=0.8270 val_loss=0.5285 acc=0.8160\n",
      "[Epoch 013] lr=0.05046 train_loss=0.4758 acc=0.8374 val_loss=0.7520 acc=0.7430\n",
      "[Epoch 014] lr=0.05050 train_loss=0.4597 acc=0.8443 val_loss=0.4669 acc=0.8354\n",
      "[Epoch 015] lr=0.05053 train_loss=0.4402 acc=0.8500 val_loss=0.5065 acc=0.8258\n",
      "[Epoch 016] lr=0.05057 train_loss=0.4328 acc=0.8542 val_loss=0.4631 acc=0.8442\n",
      "[Epoch 017] lr=0.05060 train_loss=0.4098 acc=0.8609 val_loss=0.4522 acc=0.8450\n",
      "[Epoch 018] lr=0.05064 train_loss=0.3957 acc=0.8659 val_loss=0.4699 acc=0.8334\n",
      "[Epoch 019] lr=0.05068 train_loss=0.3847 acc=0.8692 val_loss=0.4687 acc=0.8430\n",
      "[Epoch 020] lr=0.05071 train_loss=0.3753 acc=0.8735 val_loss=0.7894 acc=0.7526\n",
      "FINAL TEST: loss=0.4395  top1_acc=0.8550\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc ▁▃▄▅▆▆▆▇▇▇▇▇▇███████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▆▅▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc ▁▂▅▄▅▅▅▇▇▇▇▇▆██████▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss █▇▄▅▄▄▄▂▂▃▂▂▃▁▁▁▁▁▁▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.845\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.855\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.05071\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.87351\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 0.37532\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.7526\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 0.78938\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mi0001\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/ppbwzx4s\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251015_023323-ppbwzx4s/logs\u001b[0m\n",
      ">>> \n",
      "            python -m vgg6_cifar.scripts.train_experiment                 --data_dir ./data                 --out_dir ./runs/sweeps_ext/actrelu_bnTrue_bs32_e20_lr0.1_m0.1_nesterov-sgd                 --activation relu                 --batch_size 32                 --epochs 20                 --lr 0.1                 --momentum 0.1                 --optimizer nesterov-sgd                 --weight_decay 0.0005                 --label_smoothing 0.0                 --seed 42                 --aug_hflip                 --aug_crop                 --aug_cutout                 --aug_jitter                 --use_bn                 --amp                 --wandb --wandb_project vgg6-cifar10-assignment --wandb_group q2q5-sweeps --run_name i0002\n",
      "            \n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m setting up run oav7nb3p (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251015_023653-oav7nb3p\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mi0002\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/oav7nb3p\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.05004 train_loss=1.7007 acc=0.3713 val_loss=1.8617 acc=0.3558\n",
      "[Epoch 002] lr=0.05007 train_loss=1.2952 acc=0.5373 val_loss=1.1646 acc=0.5764\n",
      "[Epoch 003] lr=0.05011 train_loss=1.0732 acc=0.6222 val_loss=1.1157 acc=0.5984\n",
      "[Epoch 004] lr=0.05014 train_loss=0.9273 acc=0.6776 val_loss=0.9231 acc=0.6570\n",
      "[Epoch 005] lr=0.05018 train_loss=0.8381 acc=0.7113 val_loss=0.9022 acc=0.6918\n",
      "[Epoch 006] lr=0.05021 train_loss=0.7589 acc=0.7380 val_loss=0.9963 acc=0.6722\n",
      "[Epoch 007] lr=0.05025 train_loss=0.6885 acc=0.7629 val_loss=0.8057 acc=0.7152\n",
      "[Epoch 008] lr=0.05028 train_loss=0.6411 acc=0.7806 val_loss=0.7173 acc=0.7554\n",
      "[Epoch 009] lr=0.05032 train_loss=0.6012 acc=0.7937 val_loss=0.6029 acc=0.7846\n",
      "[Epoch 010] lr=0.05036 train_loss=0.5600 acc=0.8101 val_loss=0.6127 acc=0.7848\n",
      "[Epoch 011] lr=0.05039 train_loss=0.5264 acc=0.8212 val_loss=0.9315 acc=0.7094\n",
      "[Epoch 012] lr=0.05043 train_loss=0.5066 acc=0.8267 val_loss=0.8159 acc=0.7368\n",
      "[Epoch 013] lr=0.05046 train_loss=0.4772 acc=0.8369 val_loss=0.6255 acc=0.7894\n",
      "[Epoch 014] lr=0.05050 train_loss=0.4538 acc=0.8454 val_loss=0.5114 acc=0.8226\n",
      "[Epoch 015] lr=0.05053 train_loss=0.4437 acc=0.8490 val_loss=0.6856 acc=0.7778\n",
      "[Epoch 016] lr=0.05057 train_loss=0.4249 acc=0.8551 val_loss=1.6188 acc=0.5574\n",
      "[Epoch 017] lr=0.05060 train_loss=0.4147 acc=0.8596 val_loss=0.8019 acc=0.7438\n",
      "[Epoch 018] lr=0.05064 train_loss=0.3951 acc=0.8657 val_loss=0.5387 acc=0.8144\n",
      "[Epoch 019] lr=0.05068 train_loss=0.3829 acc=0.8700 val_loss=0.4427 acc=0.8514\n",
      "[Epoch 020] lr=0.05071 train_loss=0.3781 acc=0.8722 val_loss=0.4122 acc=0.8580\n",
      "FINAL TEST: loss=0.3836  top1_acc=0.8753\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc ▁▃▅▅▆▆▆▇▇▇▇▇████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▆▅▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc ▁▄▄▅▆▅▆▇▇▇▆▆▇█▇▄▆▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss █▅▄▃▃▄▃▂▂▂▄▃▂▁▂▇▃▂▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.858\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.8753\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.05071\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.87216\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 0.37805\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.858\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 0.41225\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mi0002\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/oav7nb3p\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251015_023653-oav7nb3p/logs\u001b[0m\n",
      ">>> \n",
      "            python -m vgg6_cifar.scripts.train_experiment                 --data_dir ./data                 --out_dir ./runs/sweeps_ext/actrelu_bnTrue_bs32_e20_lr0.1_m0.1_adam                 --activation relu                 --batch_size 32                 --epochs 20                 --lr 0.1                 --momentum 0.1                 --optimizer adam                 --weight_decay 0.0005                 --label_smoothing 0.0                 --seed 42                 --aug_hflip                 --aug_crop                 --aug_cutout                 --aug_jitter                 --use_bn                 --amp                 --wandb --wandb_project vgg6-cifar10-assignment --wandb_group q2q5-sweeps --run_name i0003\n",
      "            \n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251015_024025-1xjz2whd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mi0003\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/1xjz2whd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.05004 train_loss=6.5840 acc=0.1294 val_loss=2.1745 acc=0.1766\n",
      "[Epoch 002] lr=0.05007 train_loss=2.2147 acc=0.1504 val_loss=2.1628 acc=0.1456\n",
      "[Epoch 003] lr=0.05011 train_loss=2.1931 acc=0.1542 val_loss=2.1580 acc=0.1592\n",
      "[Epoch 004] lr=0.05014 train_loss=2.1817 acc=0.1542 val_loss=2.1162 acc=0.1764\n",
      "[Epoch 005] lr=0.05018 train_loss=2.1859 acc=0.1544 val_loss=2.1682 acc=0.1632\n",
      "[Epoch 006] lr=0.05021 train_loss=2.1781 acc=0.1593 val_loss=2.1476 acc=0.1734\n",
      "[Epoch 007] lr=0.05025 train_loss=2.1791 acc=0.1564 val_loss=2.1762 acc=0.1388\n",
      "[Epoch 008] lr=0.05028 train_loss=2.1832 acc=0.1513 val_loss=2.4754 acc=0.1120\n",
      "[Epoch 009] lr=0.05032 train_loss=2.1816 acc=0.1515 val_loss=2.1868 acc=0.1512\n",
      "[Epoch 010] lr=0.05036 train_loss=2.1771 acc=0.1553 val_loss=2.1066 acc=0.1764\n",
      "[Epoch 011] lr=0.05039 train_loss=2.1823 acc=0.1549 val_loss=2.1444 acc=0.1538\n",
      "[Epoch 012] lr=0.05043 train_loss=2.1987 acc=0.1491 val_loss=2.1661 acc=0.1670\n",
      "[Epoch 013] lr=0.05046 train_loss=2.2241 acc=0.1399 val_loss=2.3133 acc=0.0942\n",
      "[Epoch 014] lr=0.05050 train_loss=2.3090 acc=0.0980 val_loss=2.3122 acc=0.1000\n",
      "[Epoch 015] lr=0.05053 train_loss=2.3098 acc=0.0993 val_loss=2.3125 acc=0.1008\n",
      "[Epoch 016] lr=0.05057 train_loss=2.3092 acc=0.1002 val_loss=2.3106 acc=0.1002\n",
      "[Epoch 017] lr=0.05060 train_loss=2.3091 acc=0.0986 val_loss=2.3075 acc=0.1064\n",
      "[Epoch 018] lr=0.05064 train_loss=2.3085 acc=0.1008 val_loss=2.3074 acc=0.0942\n",
      "[Epoch 019] lr=0.05068 train_loss=2.3089 acc=0.1013 val_loss=2.3144 acc=0.0942\n",
      "[Epoch 020] lr=0.05071 train_loss=2.3087 acc=0.1018 val_loss=2.3082 acc=0.1008\n",
      "FINAL TEST: loss=2.1619  top1_acc=0.1794\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc ▅▇▇▇▇██▇▇█▇▇▆▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc █▅▇█▇█▅▃▆█▆▇▁▁▂▂▂▁▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss ▂▂▂▁▂▂▂█▃▁▂▂▅▅▅▅▅▅▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.1766\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.1794\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.05071\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.10178\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 2.30872\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.1008\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 2.30822\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mi0003\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/1xjz2whd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251015_024025-1xjz2whd/logs\u001b[0m\n",
      ">>> \n",
      "            python -m vgg6_cifar.scripts.train_experiment                 --data_dir ./data                 --out_dir ./runs/sweeps_ext/actrelu_bnTrue_bs32_e20_lr0.1_m0.1_adamw                 --activation relu                 --batch_size 32                 --epochs 20                 --lr 0.1                 --momentum 0.1                 --optimizer adamw                 --weight_decay 0.0005                 --label_smoothing 0.0                 --seed 42                 --aug_hflip                 --aug_crop                 --aug_cutout                 --aug_jitter                 --use_bn                 --amp                 --wandb --wandb_project vgg6-cifar10-assignment --wandb_group q2q5-sweeps --run_name i0004\n",
      "            \n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251015_024402-m7nchd38\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mi0004\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/m7nchd38\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.05004 train_loss=6.6402 acc=0.0983 val_loss=2.3082 acc=0.1008\n",
      "[Epoch 002] lr=0.05007 train_loss=2.3084 acc=0.0998 val_loss=2.3185 acc=0.1024\n",
      "[Epoch 003] lr=0.05011 train_loss=2.3095 acc=0.0996 val_loss=2.3063 acc=0.0976\n",
      "[Epoch 004] lr=0.05014 train_loss=2.3092 acc=0.0974 val_loss=2.3086 acc=0.1002\n",
      "[Epoch 005] lr=0.05018 train_loss=2.3196 acc=0.0997 val_loss=2.3168 acc=0.1064\n",
      "[Epoch 006] lr=0.05021 train_loss=2.3093 acc=0.1017 val_loss=2.3110 acc=0.1024\n",
      "[Epoch 007] lr=0.05025 train_loss=2.3094 acc=0.0988 val_loss=2.3156 acc=0.1000\n",
      "[Epoch 008] lr=0.05028 train_loss=2.3093 acc=0.0971 val_loss=2.3087 acc=0.0942\n",
      "[Epoch 009] lr=0.05032 train_loss=2.3096 acc=0.0995 val_loss=2.3141 acc=0.1002\n",
      "[Epoch 010] lr=0.05036 train_loss=2.3092 acc=0.0964 val_loss=2.3082 acc=0.1014\n",
      "[Epoch 011] lr=0.05039 train_loss=2.3089 acc=0.1028 val_loss=2.3086 acc=0.0976\n",
      "[Epoch 012] lr=0.05043 train_loss=2.3089 acc=0.1008 val_loss=2.3069 acc=0.1008\n",
      "[Epoch 013] lr=0.05046 train_loss=2.3095 acc=0.0969 val_loss=2.3134 acc=0.0942\n",
      "[Epoch 014] lr=0.05050 train_loss=2.3092 acc=0.0977 val_loss=2.3123 acc=0.1000\n",
      "[Epoch 015] lr=0.05053 train_loss=2.3098 acc=0.0990 val_loss=2.3127 acc=0.1008\n",
      "[Epoch 016] lr=0.05057 train_loss=2.3092 acc=0.1001 val_loss=2.3106 acc=0.1002\n",
      "[Epoch 017] lr=0.05060 train_loss=2.3092 acc=0.0985 val_loss=2.3075 acc=0.1064\n",
      "[Epoch 018] lr=0.05064 train_loss=2.3085 acc=0.1010 val_loss=2.3074 acc=0.0942\n",
      "[Epoch 019] lr=0.05068 train_loss=2.3089 acc=0.1013 val_loss=2.3144 acc=0.0942\n",
      "[Epoch 020] lr=0.05071 train_loss=2.3088 acc=0.1018 val_loss=2.3082 acc=0.1008\n",
      "FINAL TEST: loss=2.3190  top1_acc=0.1000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc ▃▅▅▂▅▇▄▂▄▁█▆▂▂▄▅▃▆▆▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc ▅▆▃▄█▆▄▁▄▅▃▅▁▄▅▄█▁▁▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss ▂█▁▂▇▄▆▂▅▂▂▁▅▄▅▃▂▂▆▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.1064\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.05071\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.10182\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 2.30875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.1008\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 2.30824\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mi0004\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/m7nchd38\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251015_024402-m7nchd38/logs\u001b[0m\n",
      ">>> \n",
      "            python -m vgg6_cifar.scripts.train_experiment                 --data_dir ./data                 --out_dir ./runs/sweeps_ext/actrelu_bnTrue_bs32_e20_lr0.1_m0.1_rmsprop                 --activation relu                 --batch_size 32                 --epochs 20                 --lr 0.1                 --momentum 0.1                 --optimizer rmsprop                 --weight_decay 0.0005                 --label_smoothing 0.0                 --seed 42                 --aug_hflip                 --aug_crop                 --aug_cutout                 --aug_jitter                 --use_bn                 --amp                 --wandb --wandb_project vgg6-cifar10-assignment --wandb_group q2q5-sweeps --run_name i0005\n",
      "            \n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m setting up run cfql7wcd (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251015_024732-cfql7wcd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mi0005\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/cfql7wcd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.05004 train_loss=nan acc=0.1040 val_loss=2.3141 acc=0.0942\n",
      "[Epoch 002] lr=0.05007 train_loss=2.6661 acc=0.0996 val_loss=7.2949 acc=0.0874\n",
      "[Epoch 003] lr=0.05011 train_loss=2.4768 acc=0.1007 val_loss=2.3116 acc=0.0978\n",
      "[Epoch 004] lr=0.05014 train_loss=2.4382 acc=0.0988 val_loss=2.3147 acc=0.0942\n",
      "[Epoch 005] lr=0.05018 train_loss=2.4338 acc=0.0998 val_loss=2.3121 acc=0.1064\n",
      "[Epoch 006] lr=0.05021 train_loss=2.3114 acc=0.0996 val_loss=2.3107 acc=0.1000\n",
      "[Epoch 007] lr=0.05025 train_loss=2.3099 acc=0.1007 val_loss=2.3194 acc=0.1000\n",
      "[Epoch 008] lr=0.05028 train_loss=2.3100 acc=0.0987 val_loss=2.3106 acc=0.0942\n",
      "[Epoch 009] lr=0.05032 train_loss=2.3100 acc=0.1012 val_loss=2.3100 acc=0.0942\n",
      "[Epoch 010] lr=0.05036 train_loss=2.3106 acc=0.0966 val_loss=2.3129 acc=0.1014\n",
      "[Epoch 011] lr=0.05039 train_loss=2.3097 acc=0.1000 val_loss=2.3119 acc=0.1008\n",
      "[Epoch 012] lr=0.05043 train_loss=2.3107 acc=0.0986 val_loss=2.3048 acc=0.1014\n",
      "[Epoch 013] lr=0.05046 train_loss=2.3096 acc=0.1017 val_loss=2.3216 acc=0.1008\n",
      "[Epoch 014] lr=0.05050 train_loss=2.3098 acc=0.1000 val_loss=2.3234 acc=0.0942\n",
      "[Epoch 015] lr=0.05053 train_loss=2.3103 acc=0.1010 val_loss=2.3108 acc=0.0976\n",
      "[Epoch 016] lr=0.05057 train_loss=2.3105 acc=0.0971 val_loss=2.3122 acc=0.0942\n",
      "[Epoch 017] lr=0.05060 train_loss=2.3106 acc=0.0980 val_loss=2.3243 acc=0.0942\n",
      "[Epoch 018] lr=0.05064 train_loss=2.3103 acc=0.0997 val_loss=2.3165 acc=0.1064\n",
      "[Epoch 019] lr=0.05068 train_loss=2.3100 acc=0.0982 val_loss=2.3121 acc=0.0976\n",
      "[Epoch 020] lr=0.05071 train_loss=2.3097 acc=0.1002 val_loss=2.3142 acc=0.0976\n",
      "FINAL TEST: loss=2.3138  top1_acc=0.1000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc █▄▅▃▄▄▅▃▅▁▄▃▆▄▅▁▂▄▃▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss  █▄▄▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc ▄▁▅▄█▆▆▄▄▆▆▆▆▄▅▄▄█▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss ▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.1064\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.05071\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.10024\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 2.30968\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.0976\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 2.31419\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mi0005\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/cfql7wcd\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251015_024732-cfql7wcd/logs\u001b[0m\n",
      ">>> \n",
      "            python -m vgg6_cifar.scripts.train_experiment                 --data_dir ./data                 --out_dir ./runs/sweeps_ext/actrelu_bnTrue_bs32_e20_lr0.1_m0.1_nadam                 --activation relu                 --batch_size 32                 --epochs 20                 --lr 0.1                 --momentum 0.1                 --optimizer nadam                 --weight_decay 0.0005                 --label_smoothing 0.0                 --seed 42                 --aug_hflip                 --aug_crop                 --aug_cutout                 --aug_jitter                 --use_bn                 --amp                 --wandb --wandb_project vgg6-cifar10-assignment --wandb_group q2q5-sweeps --run_name i0006\n",
      "            \n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m setting up run rhyugul6 (0.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m setting up run rhyugul6 (0.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m setting up run rhyugul6 (0.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251015_025059-rhyugul6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mi0006\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/rhyugul6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.05004 train_loss=7.2801 acc=0.1052 val_loss=2.2768 acc=0.1510\n",
      "[Epoch 002] lr=0.05007 train_loss=2.3212 acc=0.1028 val_loss=2.3148 acc=0.1024\n",
      "[Epoch 003] lr=0.05011 train_loss=2.3086 acc=0.0999 val_loss=2.3072 acc=0.0976\n",
      "[Epoch 004] lr=0.05014 train_loss=2.3085 acc=0.0977 val_loss=2.3097 acc=0.1002\n",
      "[Epoch 005] lr=0.05018 train_loss=2.3083 acc=0.0989 val_loss=2.3140 acc=0.1064\n",
      "[Epoch 018] lr=0.05064 train_loss=2.3082 acc=0.1004 val_loss=2.3073 acc=0.0942\n",
      "[Epoch 019] lr=0.05068 train_loss=2.3085 acc=0.1013 val_loss=2.3138 acc=0.0942\n",
      "[Epoch 020] lr=0.05071 train_loss=2.3083 acc=0.1015 val_loss=2.3085 acc=0.1008\n",
      "FINAL TEST: loss=2.2662  top1_acc=0.1639\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc █▆▃▁▂▃▂▁▅▁▆▄▁▂▂▃▂▄▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc █▂▁▂▃▁▂▁▂▂▁▂▂▂▂▂▃▁▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss ▁█▇▇█▇█▇▇▇▇▆██▇▇▇▇█▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.151\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.1639\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.05071\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.10153\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 2.30831\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.1008\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 2.30846\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mi0006\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/rhyugul6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251015_025059-rhyugul6/logs\u001b[0m\n",
      ">>> \n",
      "            python -m vgg6_cifar.scripts.train_experiment                 --data_dir ./data                 --out_dir ./runs/sweeps_ext/actrelu_bnTrue_bs32_e20_lr0.1_m0.1_adagrad                 --activation relu                 --batch_size 32                 --epochs 20                 --lr 0.1                 --momentum 0.1                 --optimizer adagrad                 --weight_decay 0.0005                 --label_smoothing 0.0                 --seed 42                 --aug_hflip                 --aug_crop                 --aug_cutout                 --aug_jitter                 --use_bn                 --amp                 --wandb --wandb_project vgg6-cifar10-assignment --wandb_group q2q5-sweeps --run_name i0007\n",
      "            \n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m setting up run okinfzpq (0.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m setting up run okinfzpq (0.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251015_025429-okinfzpq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mi0007\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/okinfzpq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.05004 train_loss=6.5414 acc=0.0997 val_loss=2.3029 acc=0.0942\n",
      "[Epoch 002] lr=0.05007 train_loss=2.3032 acc=0.0968 val_loss=2.3027 acc=0.0944\n",
      "[Epoch 003] lr=0.05011 train_loss=2.3021 acc=0.1017 val_loss=2.3006 acc=0.1014\n",
      "[Epoch 004] lr=0.05014 train_loss=2.2763 acc=0.1214 val_loss=2.2195 acc=0.1554\n",
      "[Epoch 005] lr=0.05018 train_loss=2.1689 acc=0.1538 val_loss=2.0664 acc=0.1884\n",
      "[Epoch 006] lr=0.05021 train_loss=2.1384 acc=0.1589 val_loss=2.0170 acc=0.2044\n",
      "[Epoch 007] lr=0.05025 train_loss=2.0789 acc=0.1688 val_loss=1.9535 acc=0.1598\n",
      "[Epoch 008] lr=0.05028 train_loss=1.9661 acc=0.1982 val_loss=1.8675 acc=0.2146\n",
      "[Epoch 009] lr=0.05032 train_loss=1.8897 acc=0.2384 val_loss=1.7444 acc=0.2964\n",
      "[Epoch 010] lr=0.05036 train_loss=1.8401 acc=0.2571 val_loss=1.6722 acc=0.3342\n",
      "[Epoch 011] lr=0.05039 train_loss=1.8105 acc=0.2726 val_loss=1.6834 acc=0.3216\n",
      "[Epoch 012] lr=0.05043 train_loss=1.7804 acc=0.2861 val_loss=1.6159 acc=0.3836\n",
      "[Epoch 013] lr=0.05046 train_loss=1.7267 acc=0.3082 val_loss=1.5514 acc=0.3638\n",
      "[Epoch 014] lr=0.05050 train_loss=1.5896 acc=0.3661 val_loss=1.4354 acc=0.4422\n",
      "[Epoch 015] lr=0.05053 train_loss=1.4988 acc=0.4200 val_loss=1.3198 acc=0.5006\n",
      "[Epoch 016] lr=0.05057 train_loss=1.4255 acc=0.4538 val_loss=1.5638 acc=0.4878\n",
      "[Epoch 017] lr=0.05060 train_loss=1.3712 acc=0.4775 val_loss=1.1850 acc=0.5830\n",
      "[Epoch 018] lr=0.05064 train_loss=1.3234 acc=0.4976 val_loss=1.3113 acc=0.5594\n",
      "[Epoch 019] lr=0.05068 train_loss=1.2701 acc=0.5220 val_loss=1.2406 acc=0.5812\n",
      "[Epoch 020] lr=0.05071 train_loss=1.1983 acc=0.5555 val_loss=1.3697 acc=0.5588\n",
      "FINAL TEST: loss=1.0965  top1_acc=0.6122\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc ▁▁▁▁▂▂▂▃▃▃▄▄▄▅▆▆▇▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc ▁▁▁▂▂▃▂▃▄▄▄▅▅▆▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss ███▇▇▆▆▅▅▄▄▄▃▃▂▃▁▂▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.583\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.6122\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.05071\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.55547\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 1.19829\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.5588\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 1.36969\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mi0007\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/okinfzpq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251015_025429-okinfzpq/logs\u001b[0m\n",
      ">>> \n",
      "            python -m vgg6_cifar.scripts.train_experiment                 --data_dir ./data                 --out_dir ./runs/sweeps_ext/actrelu_bnTrue_bs32_e20_lr0.1_m0.9_sgd                 --activation relu                 --batch_size 32                 --epochs 20                 --lr 0.1                 --momentum 0.9                 --optimizer sgd                 --weight_decay 0.0005                 --label_smoothing 0.0                 --seed 42                 --aug_hflip                 --aug_crop                 --aug_cutout                 --aug_jitter                 --use_bn                 --amp                 --wandb --wandb_project vgg6-cifar10-assignment --wandb_group q2q5-sweeps --run_name i0008\n",
      "            \n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251015_025805-2peiw0ce\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mi0008\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/2peiw0ce\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.05004 train_loss=2.3059 acc=0.1018 val_loss=2.3070 acc=0.1008\n",
      "[Epoch 002] lr=0.05007 train_loss=2.3058 acc=0.0985 val_loss=2.3112 acc=0.1024\n",
      "[Epoch 003] lr=0.05011 train_loss=2.3065 acc=0.0988 val_loss=2.3038 acc=0.0976\n",
      "[Epoch 004] lr=0.05014 train_loss=2.3061 acc=0.0995 val_loss=2.3068 acc=0.1002\n",
      "[Epoch 005] lr=0.05018 train_loss=2.3059 acc=0.1002 val_loss=2.3086 acc=0.1028\n",
      "[Epoch 006] lr=0.05021 train_loss=2.3065 acc=0.0996 val_loss=2.3056 acc=0.1024\n",
      "[Epoch 007] lr=0.05025 train_loss=2.3058 acc=0.0987 val_loss=2.3117 acc=0.1000\n",
      "[Epoch 008] lr=0.05028 train_loss=2.0895 acc=0.1793 val_loss=1.8756 acc=0.2614\n",
      "[Epoch 009] lr=0.05032 train_loss=1.7490 acc=0.3387 val_loss=2.1137 acc=0.3312\n",
      "[Epoch 010] lr=0.05036 train_loss=1.5334 acc=0.4368 val_loss=1.4786 acc=0.4842\n",
      "[Epoch 011] lr=0.05039 train_loss=1.3705 acc=0.5139 val_loss=1.3678 acc=0.5110\n",
      "[Epoch 012] lr=0.05043 train_loss=1.2681 acc=0.5561 val_loss=1.3002 acc=0.5346\n",
      "[Epoch 013] lr=0.05046 train_loss=1.2008 acc=0.5829 val_loss=1.1695 acc=0.5812\n",
      "[Epoch 014] lr=0.05050 train_loss=1.1732 acc=0.5985 val_loss=1.4172 acc=0.5332\n",
      "[Epoch 015] lr=0.05053 train_loss=1.1393 acc=0.6107 val_loss=1.1257 acc=0.6164\n",
      "[Epoch 016] lr=0.05057 train_loss=1.1016 acc=0.6250 val_loss=1.1664 acc=0.5928\n",
      "[Epoch 017] lr=0.05060 train_loss=1.0687 acc=0.6370 val_loss=1.1462 acc=0.6254\n",
      "[Epoch 018] lr=0.05064 train_loss=1.0600 acc=0.6435 val_loss=0.9856 acc=0.6696\n",
      "[Epoch 019] lr=0.05068 train_loss=1.0296 acc=0.6555 val_loss=1.1444 acc=0.5892\n",
      "[Epoch 020] lr=0.05071 train_loss=1.0071 acc=0.6625 val_loss=1.0431 acc=0.6438\n",
      "FINAL TEST: loss=0.9366  top1_acc=0.6924\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc ▁▁▁▁▁▁▁▂▄▅▆▇▇▇▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss ███████▇▅▄▃▂▂▂▂▂▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc ▁▁▁▁▁▁▁▃▄▆▆▆▇▆▇▇▇█▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss ███████▆▇▄▃▃▂▃▂▂▂▁▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.6696\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.6924\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.05071\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.66249\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 1.00709\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.6438\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 1.04306\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mi0008\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/2peiw0ce\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251015_025805-2peiw0ce/logs\u001b[0m\n",
      ">>> \n",
      "            python -m vgg6_cifar.scripts.train_experiment                 --data_dir ./data                 --out_dir ./runs/sweeps_ext/actrelu_bnTrue_bs32_e20_lr0.1_m0.9_nesterov-sgd                 --activation relu                 --batch_size 32                 --epochs 20                 --lr 0.1                 --momentum 0.9                 --optimizer nesterov-sgd                 --weight_decay 0.0005                 --label_smoothing 0.0                 --seed 42                 --aug_hflip                 --aug_crop                 --aug_cutout                 --aug_jitter                 --use_bn                 --amp                 --wandb --wandb_project vgg6-cifar10-assignment --wandb_group q2q5-sweeps --run_name i0009\n",
      "            \n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m setting up run fnt2ffxy (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251015_030133-fnt2ffxy\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mi0009\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/fnt2ffxy\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.05004 train_loss=2.3064 acc=0.1015 val_loss=2.3070 acc=0.0942\n",
      "[Epoch 002] lr=0.05007 train_loss=2.1881 acc=0.1469 val_loss=1.9264 acc=0.2294\n",
      "[Epoch 003] lr=0.05011 train_loss=1.7516 acc=0.3385 val_loss=1.6906 acc=0.3778\n",
      "[Epoch 004] lr=0.05014 train_loss=1.3530 acc=0.5218 val_loss=1.1925 acc=0.5836\n",
      "[Epoch 005] lr=0.05018 train_loss=1.1736 acc=0.5936 val_loss=1.1771 acc=0.6140\n",
      "[Epoch 006] lr=0.05021 train_loss=1.0864 acc=0.6291 val_loss=1.0593 acc=0.6398\n",
      "[Epoch 007] lr=0.05025 train_loss=1.0176 acc=0.6561 val_loss=1.0557 acc=0.6562\n",
      "[Epoch 008] lr=0.05028 train_loss=0.9768 acc=0.6714 val_loss=1.2033 acc=0.6126\n",
      "[Epoch 009] lr=0.05032 train_loss=0.9446 acc=0.6859 val_loss=0.9600 acc=0.6792\n",
      "[Epoch 010] lr=0.05036 train_loss=0.9222 acc=0.6940 val_loss=0.8646 acc=0.6962\n",
      "[Epoch 011] lr=0.05039 train_loss=0.8959 acc=0.7024 val_loss=1.0077 acc=0.6578\n",
      "[Epoch 012] lr=0.05043 train_loss=0.8811 acc=0.7079 val_loss=1.0992 acc=0.6444\n",
      "[Epoch 013] lr=0.05046 train_loss=0.8668 acc=0.7118 val_loss=0.8560 acc=0.7162\n",
      "[Epoch 014] lr=0.05050 train_loss=0.8675 acc=0.7136 val_loss=1.1245 acc=0.6270\n",
      "[Epoch 015] lr=0.05053 train_loss=0.8623 acc=0.7175 val_loss=0.9258 acc=0.6942\n",
      "[Epoch 016] lr=0.05057 train_loss=0.8490 acc=0.7197 val_loss=1.1364 acc=0.6246\n",
      "[Epoch 017] lr=0.05060 train_loss=0.8478 acc=0.7226 val_loss=1.1554 acc=0.6382\n",
      "[Epoch 018] lr=0.05064 train_loss=0.8404 acc=0.7251 val_loss=0.8437 acc=0.7126\n",
      "[Epoch 019] lr=0.05068 train_loss=0.8392 acc=0.7245 val_loss=0.8432 acc=0.7140\n",
      "[Epoch 020] lr=0.05071 train_loss=0.8263 acc=0.7292 val_loss=1.0074 acc=0.6708\n",
      "FINAL TEST: loss=0.8304  top1_acc=0.7261\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc ▁▂▄▆▆▇▇▇████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▇▅▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc ▁▃▄▇▇▇▇▇██▇▇█▇█▇▇██▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss █▆▅▃▃▂▂▃▂▁▂▂▁▂▁▂▂▁▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.7162\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.7261\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.05071\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.72924\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 0.82626\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.6708\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 1.00745\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mi0009\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/fnt2ffxy\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251015_030133-fnt2ffxy/logs\u001b[0m\n",
      ">>> \n",
      "            python -m vgg6_cifar.scripts.train_experiment                 --data_dir ./data                 --out_dir ./runs/sweeps_ext/actrelu_bnTrue_bs32_e20_lr0.1_m0.9_adam                 --activation relu                 --batch_size 32                 --epochs 20                 --lr 0.1                 --momentum 0.9                 --optimizer adam                 --weight_decay 0.0005                 --label_smoothing 0.0                 --seed 42                 --aug_hflip                 --aug_crop                 --aug_cutout                 --aug_jitter                 --use_bn                 --amp                 --wandb --wandb_project vgg6-cifar10-assignment --wandb_group q2q5-sweeps --run_name i0010\n",
      "            \n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251015_030515-yvmojpeb\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mi0010\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/yvmojpeb\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.05004 train_loss=6.5840 acc=0.1294 val_loss=2.1745 acc=0.1766\n",
      "[Epoch 002] lr=0.05007 train_loss=2.2147 acc=0.1504 val_loss=2.1628 acc=0.1456\n",
      "[Epoch 003] lr=0.05011 train_loss=2.1931 acc=0.1542 val_loss=2.1580 acc=0.1592\n",
      "[Epoch 004] lr=0.05014 train_loss=2.1817 acc=0.1542 val_loss=2.1162 acc=0.1764\n",
      "[Epoch 005] lr=0.05018 train_loss=2.1859 acc=0.1544 val_loss=2.1682 acc=0.1632\n",
      "[Epoch 006] lr=0.05021 train_loss=2.1781 acc=0.1593 val_loss=2.1476 acc=0.1734\n",
      "[Epoch 007] lr=0.05025 train_loss=2.1791 acc=0.1564 val_loss=2.1762 acc=0.1388\n",
      "[Epoch 008] lr=0.05028 train_loss=2.1832 acc=0.1513 val_loss=2.4754 acc=0.1120\n",
      "[Epoch 009] lr=0.05032 train_loss=2.1816 acc=0.1515 val_loss=2.1868 acc=0.1512\n",
      "[Epoch 010] lr=0.05036 train_loss=2.1771 acc=0.1553 val_loss=2.1066 acc=0.1764\n",
      "[Epoch 011] lr=0.05039 train_loss=2.1823 acc=0.1549 val_loss=2.1444 acc=0.1538\n",
      "[Epoch 012] lr=0.05043 train_loss=2.1987 acc=0.1491 val_loss=2.1661 acc=0.1670\n",
      "[Epoch 013] lr=0.05046 train_loss=2.2241 acc=0.1399 val_loss=2.3133 acc=0.0942\n",
      "[Epoch 014] lr=0.05050 train_loss=2.3090 acc=0.0980 val_loss=2.3122 acc=0.1000\n",
      "[Epoch 015] lr=0.05053 train_loss=2.3098 acc=0.0993 val_loss=2.3125 acc=0.1008\n",
      "[Epoch 016] lr=0.05057 train_loss=2.3092 acc=0.1002 val_loss=2.3106 acc=0.1002\n",
      "[Epoch 017] lr=0.05060 train_loss=2.3091 acc=0.0986 val_loss=2.3075 acc=0.1064\n",
      "[Epoch 018] lr=0.05064 train_loss=2.3085 acc=0.1008 val_loss=2.3074 acc=0.0942\n",
      "[Epoch 019] lr=0.05068 train_loss=2.3089 acc=0.1013 val_loss=2.3144 acc=0.0942\n",
      "[Epoch 020] lr=0.05071 train_loss=2.3087 acc=0.1018 val_loss=2.3082 acc=0.1008\n",
      "FINAL TEST: loss=2.1619  top1_acc=0.1794\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc ▅▇▇▇▇██▇▇█▇▇▆▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc █▅▇█▇█▅▃▆█▆▇▁▁▂▂▂▁▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss ▂▂▂▁▂▂▂█▃▁▂▂▅▅▅▅▅▅▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.1766\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.1794\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.05071\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.10178\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 2.30872\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.1008\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 2.30822\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mi0010\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/yvmojpeb\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251015_030515-yvmojpeb/logs\u001b[0m\n",
      ">>> \n",
      "            python -m vgg6_cifar.scripts.train_experiment                 --data_dir ./data                 --out_dir ./runs/sweeps_ext/actrelu_bnTrue_bs32_e20_lr0.1_m0.9_adamw                 --activation relu                 --batch_size 32                 --epochs 20                 --lr 0.1                 --momentum 0.9                 --optimizer adamw                 --weight_decay 0.0005                 --label_smoothing 0.0                 --seed 42                 --aug_hflip                 --aug_crop                 --aug_cutout                 --aug_jitter                 --use_bn                 --amp                 --wandb --wandb_project vgg6-cifar10-assignment --wandb_group q2q5-sweeps --run_name i0011\n",
      "            \n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m setting up run d46dzgjq (0.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251015_030855-d46dzgjq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mi0011\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/d46dzgjq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.05004 train_loss=6.6402 acc=0.0983 val_loss=2.3082 acc=0.1008\n",
      "[Epoch 002] lr=0.05007 train_loss=2.3084 acc=0.0998 val_loss=2.3185 acc=0.1024\n",
      "[Epoch 003] lr=0.05011 train_loss=2.3095 acc=0.0996 val_loss=2.3063 acc=0.0976\n",
      "[Epoch 004] lr=0.05014 train_loss=2.3092 acc=0.0974 val_loss=2.3086 acc=0.1002\n",
      "[Epoch 005] lr=0.05018 train_loss=2.3196 acc=0.0997 val_loss=2.3168 acc=0.1064\n",
      "[Epoch 006] lr=0.05021 train_loss=2.3093 acc=0.1017 val_loss=2.3110 acc=0.1024\n",
      "[Epoch 007] lr=0.05025 train_loss=2.3094 acc=0.0988 val_loss=2.3156 acc=0.1000\n",
      "[Epoch 008] lr=0.05028 train_loss=2.3093 acc=0.0971 val_loss=2.3087 acc=0.0942\n",
      "[Epoch 009] lr=0.05032 train_loss=2.3096 acc=0.0995 val_loss=2.3141 acc=0.1002\n",
      "[Epoch 010] lr=0.05036 train_loss=2.3092 acc=0.0964 val_loss=2.3082 acc=0.1014\n",
      "[Epoch 011] lr=0.05039 train_loss=2.3089 acc=0.1028 val_loss=2.3086 acc=0.0976\n",
      "[Epoch 012] lr=0.05043 train_loss=2.3089 acc=0.1008 val_loss=2.3069 acc=0.1008\n",
      "[Epoch 013] lr=0.05046 train_loss=2.3095 acc=0.0969 val_loss=2.3134 acc=0.0942\n",
      "[Epoch 014] lr=0.05050 train_loss=2.3092 acc=0.0977 val_loss=2.3123 acc=0.1000\n",
      "[Epoch 015] lr=0.05053 train_loss=2.3098 acc=0.0990 val_loss=2.3127 acc=0.1008\n",
      "[Epoch 016] lr=0.05057 train_loss=2.3092 acc=0.1001 val_loss=2.3106 acc=0.1002\n",
      "[Epoch 017] lr=0.05060 train_loss=2.3092 acc=0.0985 val_loss=2.3075 acc=0.1064\n",
      "[Epoch 018] lr=0.05064 train_loss=2.3085 acc=0.1010 val_loss=2.3074 acc=0.0942\n",
      "[Epoch 019] lr=0.05068 train_loss=2.3089 acc=0.1013 val_loss=2.3144 acc=0.0942\n",
      "[Epoch 020] lr=0.05071 train_loss=2.3088 acc=0.1018 val_loss=2.3082 acc=0.1008\n",
      "FINAL TEST: loss=2.3190  top1_acc=0.1000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc ▃▅▅▂▅▇▄▂▄▁█▆▂▂▄▅▃▆▆▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc ▅▆▃▄█▆▄▁▄▅▃▅▁▄▅▄█▁▁▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss ▂█▁▂▇▄▆▂▅▂▂▁▅▄▅▃▂▂▆▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.1064\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.05071\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.10182\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 2.30875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.1008\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 2.30824\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mi0011\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/d46dzgjq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251015_030855-d46dzgjq/logs\u001b[0m\n",
      ">>> \n",
      "            python -m vgg6_cifar.scripts.train_experiment                 --data_dir ./data                 --out_dir ./runs/sweeps_ext/actrelu_bnTrue_bs32_e20_lr0.1_m0.9_rmsprop                 --activation relu                 --batch_size 32                 --epochs 20                 --lr 0.1                 --momentum 0.9                 --optimizer rmsprop                 --weight_decay 0.0005                 --label_smoothing 0.0                 --seed 42                 --aug_hflip                 --aug_crop                 --aug_cutout                 --aug_jitter                 --use_bn                 --amp                 --wandb --wandb_project vgg6-cifar10-assignment --wandb_group q2q5-sweeps --run_name i0012\n",
      "            \n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251015_031234-mnrn6dv8\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mi0012\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/mnrn6dv8\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.05004 train_loss=nan acc=0.1004 val_loss=52513.6325 acc=0.0976\n",
      "[Epoch 002] lr=0.05007 train_loss=nan acc=0.1004 val_loss=53038.9395 acc=0.0976\n",
      "[Epoch 003] lr=0.05011 train_loss=nan acc=0.1005 val_loss=50332.4021 acc=0.0976\n",
      "[Epoch 004] lr=0.05014 train_loss=nan acc=0.1009 val_loss=52505.8541 acc=0.0976\n",
      "[Epoch 005] lr=0.05018 train_loss=nan acc=0.1006 val_loss=46204.2278 acc=0.0976\n",
      "[Epoch 006] lr=0.05021 train_loss=nan acc=0.1004 val_loss=46070.1411 acc=0.0976\n",
      "[Epoch 007] lr=0.05025 train_loss=nan acc=0.1013 val_loss=46737.9865 acc=0.0976\n",
      "[Epoch 008] lr=0.05028 train_loss=nan acc=0.1013 val_loss=48199.8375 acc=0.0976\n",
      "[Epoch 009] lr=0.05032 train_loss=nan acc=0.1008 val_loss=52915.5197 acc=0.0976\n",
      "[Epoch 010] lr=0.05036 train_loss=nan acc=0.1005 val_loss=52479.9014 acc=0.0976\n",
      "[Epoch 011] lr=0.05039 train_loss=nan acc=0.1007 val_loss=46871.5841 acc=0.0976\n",
      "[Epoch 012] lr=0.05043 train_loss=nan acc=0.1006 val_loss=47738.3884 acc=0.0976\n",
      "[Epoch 013] lr=0.05046 train_loss=nan acc=0.1008 val_loss=47350.9434 acc=0.0976\n",
      "[Epoch 014] lr=0.05050 train_loss=nan acc=0.1005 val_loss=47260.4210 acc=0.0976\n",
      "[Epoch 015] lr=0.05053 train_loss=nan acc=0.1002 val_loss=47992.3628 acc=0.0976\n",
      "[Epoch 016] lr=0.05057 train_loss=nan acc=0.0997 val_loss=46591.6552 acc=0.0976\n",
      "[Epoch 017] lr=0.05060 train_loss=nan acc=0.1003 val_loss=49599.0377 acc=0.0976\n",
      "[Epoch 018] lr=0.05064 train_loss=nan acc=0.1005 val_loss=49874.2718 acc=0.0976\n",
      "[Epoch 019] lr=0.05068 train_loss=nan acc=0.1003 val_loss=47500.5834 acc=0.0976\n",
      "[Epoch 020] lr=0.05071 train_loss=nan acc=0.1008 val_loss=46060.3909 acc=0.0976\n",
      "FINAL TEST: loss=51086.2648  top1_acc=0.1000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        lr ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_acc ▄▄▄▆▅▄██▆▄▅▅▆▄▃▁▃▄▄▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_acc ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  val_loss ▇█▅▇▁▁▂▃█▇▂▃▂▂▃▂▅▅▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        +1 ...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.0976\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.05071\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.1008\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss nan\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.0976\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 46060.39093\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mi0012\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/mnrn6dv8\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251015_031234-mnrn6dv8/logs\u001b[0m\n",
      ">>> \n",
      "            python -m vgg6_cifar.scripts.train_experiment                 --data_dir ./data                 --out_dir ./runs/sweeps_ext/actrelu_bnTrue_bs32_e20_lr0.1_m0.9_nadam                 --activation relu                 --batch_size 32                 --epochs 20                 --lr 0.1                 --momentum 0.9                 --optimizer nadam                 --weight_decay 0.0005                 --label_smoothing 0.0                 --seed 42                 --aug_hflip                 --aug_crop                 --aug_cutout                 --aug_jitter                 --use_bn                 --amp                 --wandb --wandb_project vgg6-cifar10-assignment --wandb_group q2q5-sweeps --run_name i0013\n",
      "            \n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251015_031609-gyvqsyub\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mi0013\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/gyvqsyub\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.05004 train_loss=7.2801 acc=0.1052 val_loss=2.2768 acc=0.1510\n",
      "[Epoch 002] lr=0.05007 train_loss=2.3212 acc=0.1028 val_loss=2.3148 acc=0.1024\n",
      "[Epoch 003] lr=0.05011 train_loss=2.3086 acc=0.0999 val_loss=2.3072 acc=0.0976\n",
      "[Epoch 004] lr=0.05014 train_loss=2.3085 acc=0.0977 val_loss=2.3097 acc=0.1002\n",
      "[Epoch 005] lr=0.05018 train_loss=2.3083 acc=0.0989 val_loss=2.3140 acc=0.1064\n",
      "[Epoch 006] lr=0.05021 train_loss=2.3086 acc=0.0999 val_loss=2.3091 acc=0.0976\n",
      "[Epoch 007] lr=0.05025 train_loss=2.3085 acc=0.0989 val_loss=2.3139 acc=0.1000\n",
      "[Epoch 008] lr=0.05028 train_loss=2.3086 acc=0.0975 val_loss=2.3073 acc=0.0942\n",
      "[Epoch 009] lr=0.05032 train_loss=2.3089 acc=0.1013 val_loss=2.3116 acc=0.1002\n",
      "[Epoch 010] lr=0.05036 train_loss=2.3088 acc=0.0973 val_loss=2.3079 acc=0.1014\n",
      "[Epoch 011] lr=0.05039 train_loss=2.3084 acc=0.1033 val_loss=2.3084 acc=0.0976\n",
      "[Epoch 012] lr=0.05043 train_loss=2.3085 acc=0.1011 val_loss=2.3057 acc=0.1008\n",
      "[Epoch 013] lr=0.05046 train_loss=2.3088 acc=0.0977 val_loss=2.3123 acc=0.1024\n",
      "[Epoch 014] lr=0.05050 train_loss=2.3087 acc=0.0980 val_loss=2.3130 acc=0.1000\n",
      "[Epoch 015] lr=0.05053 train_loss=2.3093 acc=0.0982 val_loss=2.3113 acc=0.1008\n",
      "[Epoch 016] lr=0.05057 train_loss=2.3088 acc=0.0995 val_loss=2.3103 acc=0.1002\n",
      "[Epoch 017] lr=0.05060 train_loss=2.3088 acc=0.0980 val_loss=2.3079 acc=0.1064\n",
      "[Epoch 018] lr=0.05064 train_loss=2.3082 acc=0.1004 val_loss=2.3073 acc=0.0942\n",
      "[Epoch 019] lr=0.05068 train_loss=2.3085 acc=0.1013 val_loss=2.3138 acc=0.0942\n",
      "[Epoch 020] lr=0.05071 train_loss=2.3083 acc=0.1015 val_loss=2.3085 acc=0.1008\n",
      "FINAL TEST: loss=2.2662  top1_acc=0.1639\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc █▆▃▁▂▃▂▁▅▁▆▄▁▂▂▃▂▄▅▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc █▂▁▂▃▁▂▁▂▂▁▂▂▂▂▂▃▁▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss ▁█▇▇█▇█▇▇▇▇▆██▇▇▇▇█▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.151\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.1639\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.05071\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.10153\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 2.30831\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.1008\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 2.30846\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mi0013\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/gyvqsyub\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251015_031609-gyvqsyub/logs\u001b[0m\n",
      ">>> \n",
      "            python -m vgg6_cifar.scripts.train_experiment                 --data_dir ./data                 --out_dir ./runs/sweeps_ext/actrelu_bnTrue_bs32_e20_lr0.1_m0.9_adagrad                 --activation relu                 --batch_size 32                 --epochs 20                 --lr 0.1                 --momentum 0.9                 --optimizer adagrad                 --weight_decay 0.0005                 --label_smoothing 0.0                 --seed 42                 --aug_hflip                 --aug_crop                 --aug_cutout                 --aug_jitter                 --use_bn                 --amp                 --wandb --wandb_project vgg6-cifar10-assignment --wandb_group q2q5-sweeps --run_name i0014\n",
      "            \n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m setting up run mqykmqzo (0.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251015_031937-mqykmqzo\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mi0014\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/mqykmqzo\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.05004 train_loss=6.5414 acc=0.0997 val_loss=2.3029 acc=0.0942\n",
      "[Epoch 002] lr=0.05007 train_loss=2.3032 acc=0.0968 val_loss=2.3027 acc=0.0944\n",
      "[Epoch 003] lr=0.05011 train_loss=2.3021 acc=0.1017 val_loss=2.3006 acc=0.1014\n",
      "[Epoch 004] lr=0.05014 train_loss=2.2763 acc=0.1214 val_loss=2.2195 acc=0.1554\n",
      "[Epoch 005] lr=0.05018 train_loss=2.1689 acc=0.1538 val_loss=2.0664 acc=0.1884\n",
      "[Epoch 006] lr=0.05021 train_loss=2.1384 acc=0.1589 val_loss=2.0170 acc=0.2044\n",
      "[Epoch 007] lr=0.05025 train_loss=2.0789 acc=0.1688 val_loss=1.9535 acc=0.1598\n",
      "[Epoch 008] lr=0.05028 train_loss=1.9661 acc=0.1982 val_loss=1.8675 acc=0.2146\n",
      "[Epoch 009] lr=0.05032 train_loss=1.8897 acc=0.2384 val_loss=1.7444 acc=0.2964\n",
      "[Epoch 010] lr=0.05036 train_loss=1.8401 acc=0.2571 val_loss=1.6722 acc=0.3342\n",
      "[Epoch 011] lr=0.05039 train_loss=1.8105 acc=0.2726 val_loss=1.6834 acc=0.3216\n",
      "[Epoch 012] lr=0.05043 train_loss=1.7804 acc=0.2861 val_loss=1.6159 acc=0.3836\n",
      "[Epoch 013] lr=0.05046 train_loss=1.7267 acc=0.3082 val_loss=1.5514 acc=0.3638\n",
      "[Epoch 014] lr=0.05050 train_loss=1.5896 acc=0.3661 val_loss=1.4354 acc=0.4422\n",
      "[Epoch 015] lr=0.05053 train_loss=1.4988 acc=0.4200 val_loss=1.3198 acc=0.5006\n",
      "[Epoch 016] lr=0.05057 train_loss=1.4255 acc=0.4538 val_loss=1.5638 acc=0.4878\n",
      "[Epoch 017] lr=0.05060 train_loss=1.3712 acc=0.4775 val_loss=1.1850 acc=0.5830\n",
      "[Epoch 018] lr=0.05064 train_loss=1.3234 acc=0.4976 val_loss=1.3113 acc=0.5594\n",
      "[Epoch 019] lr=0.05068 train_loss=1.2701 acc=0.5220 val_loss=1.2406 acc=0.5812\n",
      "[Epoch 020] lr=0.05071 train_loss=1.1983 acc=0.5555 val_loss=1.3697 acc=0.5588\n",
      "FINAL TEST: loss=1.0965  top1_acc=0.6122\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc ▁▁▁▁▂▂▂▃▃▃▄▄▄▅▆▆▇▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc ▁▁▁▂▂▃▂▃▄▄▄▅▅▆▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss ███▇▇▆▆▅▅▄▄▄▃▃▂▃▁▂▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.583\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.6122\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.05071\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.55547\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 1.19829\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.5588\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 1.36969\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mi0014\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/mqykmqzo\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251015_031937-mqykmqzo/logs\u001b[0m\n",
      ">>> \n",
      "            python -m vgg6_cifar.scripts.train_experiment                 --data_dir ./data                 --out_dir ./runs/sweeps_ext/actrelu_bnTrue_bs32_e20_lr0.01_m0.1_sgd                 --activation relu                 --batch_size 32                 --epochs 20                 --lr 0.01                 --momentum 0.1                 --optimizer sgd                 --weight_decay 0.0005                 --label_smoothing 0.0                 --seed 42                 --aug_hflip                 --aug_crop                 --aug_cutout                 --aug_jitter                 --use_bn                 --amp                 --wandb --wandb_project vgg6-cifar10-assignment --wandb_group q2q5-sweeps --run_name i0015\n",
      "            \n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251015_032303-lsvl9fc9\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mi0015\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/lsvl9fc9\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.00500 train_loss=1.7970 acc=0.3275 val_loss=1.5237 acc=0.4338\n",
      "[Epoch 002] lr=0.00501 train_loss=1.4520 acc=0.4673 val_loss=1.5552 acc=0.4522\n",
      "[Epoch 003] lr=0.00501 train_loss=1.2894 acc=0.5321 val_loss=1.1564 acc=0.5778\n",
      "[Epoch 004] lr=0.00501 train_loss=1.1583 acc=0.5838 val_loss=1.1245 acc=0.5972\n",
      "[Epoch 005] lr=0.00502 train_loss=1.0629 acc=0.6199 val_loss=0.9871 acc=0.6356\n",
      "[Epoch 006] lr=0.00502 train_loss=0.9927 acc=0.6480 val_loss=1.0621 acc=0.6222\n",
      "[Epoch 007] lr=0.00502 train_loss=0.9360 acc=0.6645 val_loss=0.9487 acc=0.6608\n",
      "[Epoch 008] lr=0.00503 train_loss=0.8847 acc=0.6863 val_loss=0.8950 acc=0.6876\n",
      "[Epoch 009] lr=0.00503 train_loss=0.8479 acc=0.6996 val_loss=0.7990 acc=0.7126\n",
      "[Epoch 010] lr=0.00504 train_loss=0.8164 acc=0.7135 val_loss=0.7667 acc=0.7238\n",
      "[Epoch 011] lr=0.00504 train_loss=0.7804 acc=0.7241 val_loss=0.8026 acc=0.7202\n",
      "[Epoch 012] lr=0.00504 train_loss=0.7596 acc=0.7340 val_loss=0.7323 acc=0.7382\n",
      "[Epoch 013] lr=0.00505 train_loss=0.7260 acc=0.7466 val_loss=0.7122 acc=0.7460\n",
      "[Epoch 014] lr=0.00505 train_loss=0.7126 acc=0.7501 val_loss=0.6701 acc=0.7526\n",
      "[Epoch 015] lr=0.00505 train_loss=0.6870 acc=0.7559 val_loss=0.7172 acc=0.7394\n",
      "[Epoch 016] lr=0.00506 train_loss=0.6710 acc=0.7642 val_loss=0.6384 acc=0.7672\n",
      "[Epoch 017] lr=0.00506 train_loss=0.6544 acc=0.7723 val_loss=0.6606 acc=0.7666\n",
      "[Epoch 018] lr=0.00506 train_loss=0.6357 acc=0.7783 val_loss=0.6342 acc=0.7724\n",
      "[Epoch 019] lr=0.00507 train_loss=0.6195 acc=0.7834 val_loss=0.6476 acc=0.7742\n",
      "[Epoch 020] lr=0.00507 train_loss=0.6106 acc=0.7870 val_loss=0.6828 acc=0.7642\n",
      "FINAL TEST: loss=0.5986  top1_acc=0.7952\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc ▁▃▄▅▅▆▆▆▇▇▇▇▇▇██████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▆▅▄▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc ▁▁▄▄▅▅▆▆▇▇▇▇▇█▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss ██▅▅▄▄▃▃▂▂▂▂▂▁▂▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.7742\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.7952\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.00507\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.78702\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 0.61058\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.7642\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 0.6828\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mi0015\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/lsvl9fc9\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251015_032303-lsvl9fc9/logs\u001b[0m\n",
      ">>> \n",
      "            python -m vgg6_cifar.scripts.train_experiment                 --data_dir ./data                 --out_dir ./runs/sweeps_ext/actrelu_bnTrue_bs32_e20_lr0.01_m0.1_nesterov-sgd                 --activation relu                 --batch_size 32                 --epochs 20                 --lr 0.01                 --momentum 0.1                 --optimizer nesterov-sgd                 --weight_decay 0.0005                 --label_smoothing 0.0                 --seed 42                 --aug_hflip                 --aug_crop                 --aug_cutout                 --aug_jitter                 --use_bn                 --amp                 --wandb --wandb_project vgg6-cifar10-assignment --wandb_group q2q5-sweeps --run_name i0016\n",
      "            \n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251015_032631-rmhx9acj\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mi0016\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/rmhx9acj\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.00500 train_loss=1.7935 acc=0.3282 val_loss=1.4985 acc=0.4428\n",
      "[Epoch 002] lr=0.00501 train_loss=1.4503 acc=0.4670 val_loss=1.5578 acc=0.4508\n",
      "[Epoch 003] lr=0.00501 train_loss=1.2923 acc=0.5303 val_loss=1.1527 acc=0.5814\n",
      "[Epoch 004] lr=0.00501 train_loss=1.1702 acc=0.5800 val_loss=1.0932 acc=0.6106\n",
      "[Epoch 005] lr=0.00502 train_loss=1.0731 acc=0.6153 val_loss=1.0327 acc=0.6230\n",
      "[Epoch 006] lr=0.00502 train_loss=1.0038 acc=0.6432 val_loss=1.1162 acc=0.6098\n",
      "[Epoch 007] lr=0.00502 train_loss=0.9438 acc=0.6634 val_loss=0.9617 acc=0.6556\n",
      "[Epoch 008] lr=0.00503 train_loss=0.8906 acc=0.6837 val_loss=0.9013 acc=0.6808\n",
      "[Epoch 009] lr=0.00503 train_loss=0.8525 acc=0.6977 val_loss=0.8146 acc=0.7014\n",
      "[Epoch 010] lr=0.00504 train_loss=0.8220 acc=0.7100 val_loss=0.7723 acc=0.7194\n",
      "[Epoch 011] lr=0.00504 train_loss=0.7821 acc=0.7250 val_loss=0.7522 acc=0.7306\n",
      "[Epoch 012] lr=0.00504 train_loss=0.7590 acc=0.7351 val_loss=0.7672 acc=0.7290\n",
      "[Epoch 013] lr=0.00505 train_loss=0.7276 acc=0.7447 val_loss=0.7399 acc=0.7476\n",
      "[Epoch 014] lr=0.00505 train_loss=0.7091 acc=0.7523 val_loss=0.6673 acc=0.7594\n",
      "[Epoch 015] lr=0.00505 train_loss=0.6910 acc=0.7552 val_loss=0.6959 acc=0.7494\n",
      "[Epoch 016] lr=0.00506 train_loss=0.6682 acc=0.7635 val_loss=0.9718 acc=0.6590\n",
      "[Epoch 017] lr=0.00506 train_loss=0.6548 acc=0.7714 val_loss=0.6900 acc=0.7496\n",
      "[Epoch 018] lr=0.00506 train_loss=0.6336 acc=0.7791 val_loss=0.6300 acc=0.7766\n",
      "[Epoch 019] lr=0.00507 train_loss=0.6174 acc=0.7832 val_loss=0.6530 acc=0.7702\n",
      "[Epoch 020] lr=0.00507 train_loss=0.6076 acc=0.7872 val_loss=0.6296 acc=0.7768\n",
      "FINAL TEST: loss=0.5874  top1_acc=0.7965\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc ▁▃▄▅▅▆▆▆▇▇▇▇▇▇██████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▆▅▄▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc ▁▁▄▅▅▅▅▆▆▇▇▇▇█▇▆▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss ██▅▄▄▅▄▃▂▂▂▂▂▁▁▄▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.7768\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.7965\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.00507\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.7872\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 0.60763\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.7768\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 0.62964\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mi0016\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/rmhx9acj\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251015_032631-rmhx9acj/logs\u001b[0m\n",
      ">>> \n",
      "            python -m vgg6_cifar.scripts.train_experiment                 --data_dir ./data                 --out_dir ./runs/sweeps_ext/actrelu_bnTrue_bs32_e20_lr0.01_m0.1_adam                 --activation relu                 --batch_size 32                 --epochs 20                 --lr 0.01                 --momentum 0.1                 --optimizer adam                 --weight_decay 0.0005                 --label_smoothing 0.0                 --seed 42                 --aug_hflip                 --aug_crop                 --aug_cutout                 --aug_jitter                 --use_bn                 --amp                 --wandb --wandb_project vgg6-cifar10-assignment --wandb_group q2q5-sweeps --run_name i0017\n",
      "            \n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m setting up run l067x0ox (0.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251015_032957-l067x0ox\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mi0017\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/l067x0ox\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.00500 train_loss=2.1341 acc=0.1872 val_loss=1.9221 acc=0.2358\n",
      "[Epoch 002] lr=0.00501 train_loss=1.8478 acc=0.2812 val_loss=1.9644 acc=0.3090\n",
      "[Epoch 003] lr=0.00501 train_loss=1.6321 acc=0.3865 val_loss=1.5337 acc=0.4210\n",
      "[Epoch 004] lr=0.00501 train_loss=1.5121 acc=0.4467 val_loss=1.6201 acc=0.4272\n",
      "[Epoch 005] lr=0.00502 train_loss=1.3940 acc=0.5007 val_loss=1.4321 acc=0.4870\n",
      "[Epoch 006] lr=0.00502 train_loss=1.3198 acc=0.5314 val_loss=1.3965 acc=0.5008\n",
      "[Epoch 007] lr=0.00502 train_loss=1.2786 acc=0.5483 val_loss=1.2598 acc=0.5456\n",
      "[Epoch 008] lr=0.00503 train_loss=1.2606 acc=0.5551 val_loss=1.0890 acc=0.6170\n",
      "[Epoch 009] lr=0.00503 train_loss=1.2424 acc=0.5603 val_loss=1.1418 acc=0.5960\n",
      "[Epoch 010] lr=0.00504 train_loss=1.2264 acc=0.5694 val_loss=1.2293 acc=0.5776\n",
      "[Epoch 011] lr=0.00504 train_loss=1.2134 acc=0.5778 val_loss=1.0932 acc=0.6082\n",
      "[Epoch 012] lr=0.00504 train_loss=1.1991 acc=0.5798 val_loss=1.0839 acc=0.6230\n",
      "[Epoch 013] lr=0.00505 train_loss=1.1925 acc=0.5848 val_loss=1.1630 acc=0.5890\n",
      "[Epoch 014] lr=0.00505 train_loss=1.1889 acc=0.5860 val_loss=1.3423 acc=0.5376\n",
      "[Epoch 015] lr=0.00505 train_loss=1.1909 acc=0.5847 val_loss=1.0848 acc=0.6132\n",
      "[Epoch 016] lr=0.00506 train_loss=1.1712 acc=0.5954 val_loss=1.1358 acc=0.5914\n",
      "[Epoch 017] lr=0.00506 train_loss=1.1736 acc=0.5933 val_loss=1.1281 acc=0.5998\n",
      "[Epoch 018] lr=0.00506 train_loss=1.1665 acc=0.5950 val_loss=1.1733 acc=0.5974\n",
      "[Epoch 019] lr=0.00507 train_loss=1.1667 acc=0.5925 val_loss=1.1533 acc=0.5904\n",
      "[Epoch 020] lr=0.00507 train_loss=1.1675 acc=0.5943 val_loss=1.1443 acc=0.5962\n",
      "FINAL TEST: loss=1.0020  top1_acc=0.6466\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc ▁▃▄▅▆▇▇▇▇███████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▆▄▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc ▁▂▄▄▆▆▇██▇██▇▆█▇██▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss ██▅▅▄▃▂▁▁▂▁▁▂▃▁▁▁▂▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.623\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.6466\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.00507\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.59433\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 1.16754\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.5962\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 1.14427\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mi0017\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/l067x0ox\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251015_032957-l067x0ox/logs\u001b[0m\n",
      ">>> \n",
      "            python -m vgg6_cifar.scripts.train_experiment                 --data_dir ./data                 --out_dir ./runs/sweeps_ext/actrelu_bnTrue_bs32_e20_lr0.01_m0.1_adamw                 --activation relu                 --batch_size 32                 --epochs 20                 --lr 0.01                 --momentum 0.1                 --optimizer adamw                 --weight_decay 0.0005                 --label_smoothing 0.0                 --seed 42                 --aug_hflip                 --aug_crop                 --aug_cutout                 --aug_jitter                 --use_bn                 --amp                 --wandb --wandb_project vgg6-cifar10-assignment --wandb_group q2q5-sweeps --run_name i0018\n",
      "            \n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m setting up run ioxnk2tf (0.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251015_033325-ioxnk2tf\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mi0018\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/ioxnk2tf\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.00500 train_loss=2.1758 acc=0.1557 val_loss=1.9906 acc=0.1844\n",
      "[Epoch 002] lr=0.00501 train_loss=1.9203 acc=0.2297 val_loss=1.6795 acc=0.3338\n",
      "[Epoch 003] lr=0.00501 train_loss=1.6965 acc=0.3400 val_loss=1.5145 acc=0.4350\n",
      "[Epoch 004] lr=0.00501 train_loss=1.4058 acc=0.4895 val_loss=1.1521 acc=0.5922\n",
      "[Epoch 005] lr=0.00502 train_loss=1.1784 acc=0.5860 val_loss=0.9842 acc=0.6410\n",
      "[Epoch 006] lr=0.00502 train_loss=1.0711 acc=0.6301 val_loss=1.0134 acc=0.6466\n",
      "[Epoch 007] lr=0.00502 train_loss=0.9831 acc=0.6686 val_loss=0.8722 acc=0.7044\n",
      "[Epoch 008] lr=0.00503 train_loss=0.9258 acc=0.6909 val_loss=0.7469 acc=0.7442\n",
      "[Epoch 009] lr=0.00503 train_loss=0.8549 acc=0.7162 val_loss=0.7465 acc=0.7484\n",
      "[Epoch 010] lr=0.00504 train_loss=0.7961 acc=0.7374 val_loss=0.7166 acc=0.7560\n",
      "[Epoch 011] lr=0.00504 train_loss=0.7600 acc=0.7527 val_loss=0.6803 acc=0.7610\n",
      "[Epoch 012] lr=0.00504 train_loss=0.7183 acc=0.7631 val_loss=0.6753 acc=0.7676\n",
      "[Epoch 013] lr=0.00505 train_loss=0.6853 acc=0.7736 val_loss=0.6076 acc=0.7968\n",
      "[Epoch 014] lr=0.00505 train_loss=0.6656 acc=0.7820 val_loss=0.6023 acc=0.7926\n",
      "[Epoch 015] lr=0.00505 train_loss=0.6357 acc=0.7923 val_loss=0.6411 acc=0.7936\n",
      "[Epoch 016] lr=0.00506 train_loss=0.6183 acc=0.7998 val_loss=0.5837 acc=0.8012\n",
      "[Epoch 017] lr=0.00506 train_loss=0.5936 acc=0.8069 val_loss=0.5191 acc=0.8252\n",
      "[Epoch 018] lr=0.00506 train_loss=0.5855 acc=0.8097 val_loss=0.5335 acc=0.8120\n",
      "[Epoch 019] lr=0.00507 train_loss=0.5608 acc=0.8188 val_loss=0.5519 acc=0.8124\n",
      "[Epoch 020] lr=0.00507 train_loss=0.5419 acc=0.8232 val_loss=0.5427 acc=0.8204\n",
      "FINAL TEST: loss=0.5041  top1_acc=0.8347\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc ▁▂▃▅▆▆▆▇▇▇▇▇▇███████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▇▆▅▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc ▁▃▄▅▆▆▇▇▇▇▇▇████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss █▇▆▄▃▃▃▂▂▂▂▂▁▁▂▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.8252\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.8347\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.00507\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.82316\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 0.54186\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.8204\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 0.54272\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mi0018\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/ioxnk2tf\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251015_033325-ioxnk2tf/logs\u001b[0m\n",
      ">>> \n",
      "            python -m vgg6_cifar.scripts.train_experiment                 --data_dir ./data                 --out_dir ./runs/sweeps_ext/actrelu_bnTrue_bs32_e20_lr0.01_m0.1_rmsprop                 --activation relu                 --batch_size 32                 --epochs 20                 --lr 0.01                 --momentum 0.1                 --optimizer rmsprop                 --weight_decay 0.0005                 --label_smoothing 0.0                 --seed 42                 --aug_hflip                 --aug_crop                 --aug_cutout                 --aug_jitter                 --use_bn                 --amp                 --wandb --wandb_project vgg6-cifar10-assignment --wandb_group q2q5-sweeps --run_name i0019\n",
      "            \n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251015_033701-wv7zktmx\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mi0019\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/wv7zktmx\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.00500 train_loss=6.5794 acc=0.1257 val_loss=2.2654 acc=0.2328\n",
      "[Epoch 002] lr=0.00501 train_loss=1.7510 acc=0.3459 val_loss=2.3775 acc=0.2674\n",
      "[Epoch 003] lr=0.00501 train_loss=1.4994 acc=0.4513 val_loss=1.9009 acc=0.3428\n",
      "[Epoch 004] lr=0.00501 train_loss=1.3829 acc=0.5046 val_loss=1.6662 acc=0.4142\n",
      "[Epoch 005] lr=0.00502 train_loss=1.3195 acc=0.5311 val_loss=1.4301 acc=0.5204\n",
      "[Epoch 006] lr=0.00502 train_loss=1.2887 acc=0.5478 val_loss=2.0363 acc=0.3746\n",
      "[Epoch 007] lr=0.00502 train_loss=1.2485 acc=0.5603 val_loss=1.4488 acc=0.4920\n",
      "[Epoch 008] lr=0.00503 train_loss=1.2306 acc=0.5699 val_loss=2.0192 acc=0.4324\n",
      "[Epoch 009] lr=0.00503 train_loss=1.2088 acc=0.5766 val_loss=1.4544 acc=0.4786\n",
      "[Epoch 010] lr=0.00504 train_loss=1.2016 acc=0.5790 val_loss=1.3671 acc=0.5426\n",
      "[Epoch 011] lr=0.00504 train_loss=1.1852 acc=0.5871 val_loss=1.4413 acc=0.5108\n",
      "[Epoch 012] lr=0.00504 train_loss=1.1828 acc=0.5842 val_loss=1.1688 acc=0.5882\n",
      "[Epoch 013] lr=0.00505 train_loss=1.1742 acc=0.5894 val_loss=1.4733 acc=0.5158\n",
      "[Epoch 014] lr=0.00505 train_loss=1.1716 acc=0.5912 val_loss=1.2651 acc=0.5516\n",
      "[Epoch 015] lr=0.00505 train_loss=1.1698 acc=0.5923 val_loss=1.8876 acc=0.4548\n",
      "[Epoch 016] lr=0.00506 train_loss=1.1655 acc=0.5951 val_loss=1.2273 acc=0.5662\n",
      "[Epoch 017] lr=0.00506 train_loss=1.1595 acc=0.5969 val_loss=1.6956 acc=0.4424\n",
      "[Epoch 018] lr=0.00506 train_loss=1.1522 acc=0.5971 val_loss=1.3384 acc=0.5494\n",
      "[Epoch 019] lr=0.00507 train_loss=1.1479 acc=0.5998 val_loss=1.1283 acc=0.6002\n",
      "[Epoch 020] lr=0.00507 train_loss=1.1483 acc=0.6003 val_loss=1.4554 acc=0.5066\n",
      "FINAL TEST: loss=1.0283  top1_acc=0.6416\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc ▁▄▆▇▇▇▇█████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc ▁▂▃▄▆▄▆▅▆▇▆█▆▇▅▇▅▇█▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss ▇█▅▄▃▆▃▆▃▂▃▁▃▂▅▂▄▂▁▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.6002\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.6416\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.00507\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.60027\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 1.14835\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.5066\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 1.45541\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mi0019\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/wv7zktmx\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251015_033701-wv7zktmx/logs\u001b[0m\n",
      ">>> \n",
      "            python -m vgg6_cifar.scripts.train_experiment                 --data_dir ./data                 --out_dir ./runs/sweeps_ext/actrelu_bnTrue_bs32_e20_lr0.01_m0.1_nadam                 --activation relu                 --batch_size 32                 --epochs 20                 --lr 0.01                 --momentum 0.1                 --optimizer nadam                 --weight_decay 0.0005                 --label_smoothing 0.0                 --seed 42                 --aug_hflip                 --aug_crop                 --aug_cutout                 --aug_jitter                 --use_bn                 --amp                 --wandb --wandb_project vgg6-cifar10-assignment --wandb_group q2q5-sweeps --run_name i0020\n",
      "            \n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251015_034033-ujheovxq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mi0020\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/ujheovxq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.00500 train_loss=2.1360 acc=0.1860 val_loss=1.9133 acc=0.2390\n",
      "[Epoch 002] lr=0.00501 train_loss=1.7090 acc=0.3539 val_loss=1.6050 acc=0.3978\n",
      "[Epoch 003] lr=0.00501 train_loss=1.4661 acc=0.4594 val_loss=1.3205 acc=0.5040\n",
      "[Epoch 004] lr=0.00501 train_loss=1.3428 acc=0.5196 val_loss=1.3218 acc=0.5476\n",
      "[Epoch 005] lr=0.00502 train_loss=1.2723 acc=0.5499 val_loss=1.2781 acc=0.5456\n",
      "[Epoch 006] lr=0.00502 train_loss=1.2185 acc=0.5696 val_loss=1.3428 acc=0.5246\n",
      "[Epoch 007] lr=0.00502 train_loss=1.1897 acc=0.5814 val_loss=1.1627 acc=0.5848\n",
      "[Epoch 008] lr=0.00503 train_loss=1.1678 acc=0.5882 val_loss=1.1065 acc=0.6000\n",
      "[Epoch 009] lr=0.00503 train_loss=1.1560 acc=0.5976 val_loss=1.1570 acc=0.6036\n",
      "[Epoch 010] lr=0.00504 train_loss=1.1491 acc=0.5980 val_loss=1.0360 acc=0.6256\n",
      "[Epoch 011] lr=0.00504 train_loss=1.1375 acc=0.6022 val_loss=1.0400 acc=0.6314\n",
      "[Epoch 012] lr=0.00504 train_loss=1.1265 acc=0.6094 val_loss=1.0447 acc=0.6204\n",
      "[Epoch 013] lr=0.00505 train_loss=1.1154 acc=0.6131 val_loss=0.9785 acc=0.6578\n",
      "[Epoch 014] lr=0.00505 train_loss=1.1336 acc=0.6064 val_loss=1.0418 acc=0.6312\n",
      "[Epoch 015] lr=0.00505 train_loss=1.1154 acc=0.6099 val_loss=1.0823 acc=0.6088\n",
      "[Epoch 016] lr=0.00506 train_loss=1.1119 acc=0.6154 val_loss=1.0658 acc=0.6262\n",
      "[Epoch 017] lr=0.00506 train_loss=1.1180 acc=0.6156 val_loss=1.0383 acc=0.6330\n",
      "[Epoch 018] lr=0.00506 train_loss=1.1039 acc=0.6169 val_loss=1.0507 acc=0.6238\n",
      "[Epoch 019] lr=0.00507 train_loss=1.1015 acc=0.6187 val_loss=1.0530 acc=0.6262\n",
      "[Epoch 020] lr=0.00507 train_loss=1.0983 acc=0.6222 val_loss=1.1863 acc=0.5886\n",
      "FINAL TEST: loss=0.9271  top1_acc=0.6696\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc ▁▄▅▆▇▇▇▇████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▅▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc ▁▄▅▆▆▆▇▇▇▇█▇██▇▇█▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss █▆▄▄▃▄▂▂▂▁▁▁▁▁▂▂▁▂▂▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.6578\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.6696\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.00507\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.62218\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 1.09832\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.5886\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 1.18635\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mi0020\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/ujheovxq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251015_034033-ujheovxq/logs\u001b[0m\n",
      ">>> \n",
      "            python -m vgg6_cifar.scripts.train_experiment                 --data_dir ./data                 --out_dir ./runs/sweeps_ext/actrelu_bnTrue_bs32_e20_lr0.01_m0.1_adagrad                 --activation relu                 --batch_size 32                 --epochs 20                 --lr 0.01                 --momentum 0.1                 --optimizer adagrad                 --weight_decay 0.0005                 --label_smoothing 0.0                 --seed 42                 --aug_hflip                 --aug_crop                 --aug_cutout                 --aug_jitter                 --use_bn                 --amp                 --wandb --wandb_project vgg6-cifar10-assignment --wandb_group q2q5-sweeps --run_name i0021\n",
      "            \n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m setting up run jrx0pygk (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251015_034408-jrx0pygk\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mi0021\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/jrx0pygk\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.00500 train_loss=1.6551 acc=0.4018 val_loss=1.3354 acc=0.5060\n",
      "[Epoch 002] lr=0.00501 train_loss=1.1805 acc=0.5760 val_loss=1.0434 acc=0.6256\n",
      "[Epoch 003] lr=0.00501 train_loss=0.9964 acc=0.6483 val_loss=0.9890 acc=0.6310\n",
      "[Epoch 004] lr=0.00501 train_loss=0.8893 acc=0.6903 val_loss=0.8599 acc=0.6910\n",
      "[Epoch 005] lr=0.00502 train_loss=0.8174 acc=0.7163 val_loss=0.7595 acc=0.7274\n",
      "[Epoch 006] lr=0.00502 train_loss=0.7578 acc=0.7362 val_loss=0.7400 acc=0.7380\n",
      "[Epoch 007] lr=0.00502 train_loss=0.7136 acc=0.7516 val_loss=0.6924 acc=0.7628\n",
      "[Epoch 008] lr=0.00503 train_loss=0.6778 acc=0.7674 val_loss=0.6596 acc=0.7644\n",
      "[Epoch 009] lr=0.00503 train_loss=0.6501 acc=0.7774 val_loss=0.6045 acc=0.7850\n",
      "[Epoch 010] lr=0.00504 train_loss=0.6211 acc=0.7884 val_loss=0.5673 acc=0.8022\n",
      "[Epoch 011] lr=0.00504 train_loss=0.5935 acc=0.7971 val_loss=0.6341 acc=0.7788\n",
      "[Epoch 012] lr=0.00504 train_loss=0.5729 acc=0.8014 val_loss=0.5665 acc=0.8004\n",
      "[Epoch 013] lr=0.00505 train_loss=0.5487 acc=0.8128 val_loss=0.5472 acc=0.8096\n",
      "[Epoch 014] lr=0.00505 train_loss=0.5311 acc=0.8178 val_loss=0.5618 acc=0.8048\n",
      "[Epoch 015] lr=0.00505 train_loss=0.5203 acc=0.8221 val_loss=0.5772 acc=0.7942\n",
      "[Epoch 016] lr=0.00506 train_loss=0.5031 acc=0.8263 val_loss=0.5048 acc=0.8214\n",
      "[Epoch 017] lr=0.00506 train_loss=0.4857 acc=0.8338 val_loss=0.4672 acc=0.8340\n",
      "[Epoch 018] lr=0.00506 train_loss=0.4773 acc=0.8388 val_loss=0.5713 acc=0.8048\n",
      "[Epoch 019] lr=0.00507 train_loss=0.4594 acc=0.8443 val_loss=0.4886 acc=0.8312\n",
      "[Epoch 020] lr=0.00507 train_loss=0.4511 acc=0.8453 val_loss=0.4714 acc=0.8362\n",
      "FINAL TEST: loss=0.4344  top1_acc=0.8487\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc ▁▄▅▆▆▆▇▇▇▇▇▇▇███████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▅▄▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc ▁▄▄▅▆▆▆▆▇▇▇▇▇▇▇██▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss █▆▅▄▃▃▃▃▂▂▂▂▂▂▂▁▁▂▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.8362\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.8487\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.00507\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.84533\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 0.4511\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.8362\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 0.47136\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mi0021\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/jrx0pygk\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251015_034408-jrx0pygk/logs\u001b[0m\n",
      ">>> \n",
      "            python -m vgg6_cifar.scripts.train_experiment                 --data_dir ./data                 --out_dir ./runs/sweeps_ext/actrelu_bnTrue_bs32_e20_lr0.01_m0.9_sgd                 --activation relu                 --batch_size 32                 --epochs 20                 --lr 0.01                 --momentum 0.9                 --optimizer sgd                 --weight_decay 0.0005                 --label_smoothing 0.0                 --seed 42                 --aug_hflip                 --aug_crop                 --aug_cutout                 --aug_jitter                 --use_bn                 --amp                 --wandb --wandb_project vgg6-cifar10-assignment --wandb_group q2q5-sweeps --run_name i0022\n",
      "            \n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m setting up run 2l1ozpmt (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m setting up run 2l1ozpmt (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251015_034748-2l1ozpmt\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mi0022\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/2l1ozpmt\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.00500 train_loss=1.6732 acc=0.3781 val_loss=1.5190 acc=0.4562\n",
      "[Epoch 002] lr=0.00501 train_loss=1.2832 acc=0.5437 val_loss=1.0907 acc=0.6146\n",
      "[Epoch 003] lr=0.00501 train_loss=1.0588 acc=0.6307 val_loss=0.8898 acc=0.6796\n",
      "[Epoch 004] lr=0.00501 train_loss=0.9332 acc=0.6769 val_loss=0.8041 acc=0.7128\n",
      "[Epoch 005] lr=0.00502 train_loss=0.8408 acc=0.7118 val_loss=0.7365 acc=0.7446\n",
      "[Epoch 006] lr=0.00502 train_loss=0.7675 acc=0.7361 val_loss=0.6938 acc=0.7590\n",
      "[Epoch 007] lr=0.00502 train_loss=0.7048 acc=0.7574 val_loss=0.7113 acc=0.7558\n",
      "[Epoch 008] lr=0.00503 train_loss=0.6589 acc=0.7768 val_loss=0.6268 acc=0.7852\n",
      "[Epoch 009] lr=0.00503 train_loss=0.6163 acc=0.7906 val_loss=0.5571 acc=0.7984\n",
      "[Epoch 010] lr=0.00504 train_loss=0.5788 acc=0.8033 val_loss=0.5626 acc=0.8030\n",
      "[Epoch 011] lr=0.00504 train_loss=0.5491 acc=0.8124 val_loss=0.5476 acc=0.8084\n",
      "[Epoch 012] lr=0.00504 train_loss=0.5141 acc=0.8256 val_loss=0.4871 acc=0.8372\n",
      "[Epoch 013] lr=0.00505 train_loss=0.4900 acc=0.8337 val_loss=0.4761 acc=0.8390\n",
      "[Epoch 014] lr=0.00505 train_loss=0.4711 acc=0.8390 val_loss=0.4716 acc=0.8304\n",
      "[Epoch 015] lr=0.00505 train_loss=0.4544 acc=0.8451 val_loss=0.5357 acc=0.8214\n",
      "[Epoch 016] lr=0.00506 train_loss=0.4310 acc=0.8537 val_loss=0.4632 acc=0.8426\n",
      "[Epoch 017] lr=0.00506 train_loss=0.4228 acc=0.8550 val_loss=0.4249 acc=0.8508\n",
      "[Epoch 018] lr=0.00506 train_loss=0.4029 acc=0.8630 val_loss=0.4144 acc=0.8538\n",
      "[Epoch 019] lr=0.00507 train_loss=0.3866 acc=0.8672 val_loss=0.4212 acc=0.8614\n",
      "[Epoch 020] lr=0.00507 train_loss=0.3788 acc=0.8714 val_loss=0.4432 acc=0.8474\n",
      "FINAL TEST: loss=0.3818  top1_acc=0.8718\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣟\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc ▁▃▅▅▆▆▆▇▇▇▇▇▇███████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▆▅▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc ▁▄▅▅▆▆▆▇▇▇▇██▇▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss █▅▄▃▃▃▃▂▂▂▂▁▁▁▂▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.8614\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.8718\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.00507\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.87138\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 0.37878\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.8474\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 0.44318\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mi0022\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/2l1ozpmt\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251015_034748-2l1ozpmt/logs\u001b[0m\n",
      ">>> \n",
      "            python -m vgg6_cifar.scripts.train_experiment                 --data_dir ./data                 --out_dir ./runs/sweeps_ext/actrelu_bnTrue_bs32_e20_lr0.01_m0.9_nesterov-sgd                 --activation relu                 --batch_size 32                 --epochs 20                 --lr 0.01                 --momentum 0.9                 --optimizer nesterov-sgd                 --weight_decay 0.0005                 --label_smoothing 0.0                 --seed 42                 --aug_hflip                 --aug_crop                 --aug_cutout                 --aug_jitter                 --use_bn                 --amp                 --wandb --wandb_project vgg6-cifar10-assignment --wandb_group q2q5-sweeps --run_name i0023\n",
      "            \n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m setting up run 9zo8al5v (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251015_035116-9zo8al5v\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mi0023\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/9zo8al5v\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.00500 train_loss=1.6202 acc=0.3988 val_loss=1.5440 acc=0.4534\n",
      "[Epoch 002] lr=0.00501 train_loss=1.2405 acc=0.5578 val_loss=1.0759 acc=0.6110\n",
      "[Epoch 003] lr=0.00501 train_loss=1.0416 acc=0.6354 val_loss=1.0133 acc=0.6386\n",
      "[Epoch 004] lr=0.00501 train_loss=0.9161 acc=0.6845 val_loss=0.8192 acc=0.7114\n",
      "[Epoch 005] lr=0.00502 train_loss=0.8276 acc=0.7156 val_loss=0.7360 acc=0.7422\n",
      "[Epoch 006] lr=0.00502 train_loss=0.7567 acc=0.7399 val_loss=0.7749 acc=0.7352\n",
      "[Epoch 007] lr=0.00502 train_loss=0.6991 acc=0.7631 val_loss=0.6778 acc=0.7636\n",
      "[Epoch 008] lr=0.00503 train_loss=0.6530 acc=0.7786 val_loss=0.6444 acc=0.7770\n",
      "[Epoch 009] lr=0.00503 train_loss=0.6127 acc=0.7912 val_loss=0.6268 acc=0.7894\n",
      "[Epoch 010] lr=0.00504 train_loss=0.5789 acc=0.8032 val_loss=0.5674 acc=0.8028\n",
      "[Epoch 011] lr=0.00504 train_loss=0.5435 acc=0.8159 val_loss=0.5626 acc=0.8068\n",
      "[Epoch 012] lr=0.00504 train_loss=0.5151 acc=0.8251 val_loss=0.5388 acc=0.8186\n",
      "[Epoch 013] lr=0.00505 train_loss=0.4895 acc=0.8322 val_loss=0.4839 acc=0.8308\n",
      "[Epoch 014] lr=0.00505 train_loss=0.4695 acc=0.8394 val_loss=0.4839 acc=0.8356\n",
      "[Epoch 015] lr=0.00505 train_loss=0.4493 acc=0.8491 val_loss=0.5408 acc=0.8180\n",
      "[Epoch 016] lr=0.00506 train_loss=0.4363 acc=0.8514 val_loss=0.4414 acc=0.8452\n",
      "[Epoch 017] lr=0.00506 train_loss=0.4146 acc=0.8597 val_loss=0.4320 acc=0.8542\n",
      "[Epoch 018] lr=0.00506 train_loss=0.4005 acc=0.8648 val_loss=0.4287 acc=0.8522\n",
      "[Epoch 019] lr=0.00507 train_loss=0.3879 acc=0.8677 val_loss=0.4481 acc=0.8474\n",
      "[Epoch 020] lr=0.00507 train_loss=0.3785 acc=0.8715 val_loss=0.4977 acc=0.8270\n",
      "FINAL TEST: loss=0.4302  top1_acc=0.8538\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc ▁▃▅▅▆▆▆▇▇▇▇▇▇███████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▆▅▄▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc ▁▄▄▆▆▆▆▇▇▇▇▇██▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss █▅▅▃▃▃▃▂▂▂▂▂▁▁▂▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.8542\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.8538\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.00507\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.87147\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 0.37847\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.827\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 0.49773\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mi0023\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/9zo8al5v\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251015_035116-9zo8al5v/logs\u001b[0m\n",
      ">>> \n",
      "            python -m vgg6_cifar.scripts.train_experiment                 --data_dir ./data                 --out_dir ./runs/sweeps_ext/actrelu_bnTrue_bs32_e20_lr0.01_m0.9_adam                 --activation relu                 --batch_size 32                 --epochs 20                 --lr 0.01                 --momentum 0.9                 --optimizer adam                 --weight_decay 0.0005                 --label_smoothing 0.0                 --seed 42                 --aug_hflip                 --aug_crop                 --aug_cutout                 --aug_jitter                 --use_bn                 --amp                 --wandb --wandb_project vgg6-cifar10-assignment --wandb_group q2q5-sweeps --run_name i0024\n",
      "            \n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m setting up run 3lro9fny (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251015_035444-3lro9fny\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mi0024\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/3lro9fny\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.00500 train_loss=2.1341 acc=0.1872 val_loss=1.9221 acc=0.2358\n",
      "[Epoch 002] lr=0.00501 train_loss=1.8478 acc=0.2812 val_loss=1.9644 acc=0.3090\n",
      "[Epoch 003] lr=0.00501 train_loss=1.6321 acc=0.3865 val_loss=1.5337 acc=0.4210\n",
      "[Epoch 004] lr=0.00501 train_loss=1.5121 acc=0.4467 val_loss=1.6201 acc=0.4272\n",
      "[Epoch 005] lr=0.00502 train_loss=1.3940 acc=0.5007 val_loss=1.4321 acc=0.4870\n",
      "[Epoch 006] lr=0.00502 train_loss=1.3198 acc=0.5314 val_loss=1.3965 acc=0.5008\n",
      "[Epoch 007] lr=0.00502 train_loss=1.2786 acc=0.5483 val_loss=1.2598 acc=0.5456\n",
      "[Epoch 008] lr=0.00503 train_loss=1.2606 acc=0.5551 val_loss=1.0890 acc=0.6170\n",
      "[Epoch 009] lr=0.00503 train_loss=1.2424 acc=0.5603 val_loss=1.1418 acc=0.5960\n",
      "[Epoch 010] lr=0.00504 train_loss=1.2264 acc=0.5694 val_loss=1.2293 acc=0.5776\n",
      "[Epoch 011] lr=0.00504 train_loss=1.2134 acc=0.5778 val_loss=1.0932 acc=0.6082\n",
      "[Epoch 012] lr=0.00504 train_loss=1.1991 acc=0.5798 val_loss=1.0839 acc=0.6230\n",
      "[Epoch 013] lr=0.00505 train_loss=1.1925 acc=0.5848 val_loss=1.1630 acc=0.5890\n",
      "[Epoch 014] lr=0.00505 train_loss=1.1889 acc=0.5860 val_loss=1.3423 acc=0.5376\n",
      "[Epoch 015] lr=0.00505 train_loss=1.1909 acc=0.5847 val_loss=1.0848 acc=0.6132\n",
      "[Epoch 016] lr=0.00506 train_loss=1.1712 acc=0.5954 val_loss=1.1358 acc=0.5914\n",
      "[Epoch 017] lr=0.00506 train_loss=1.1736 acc=0.5933 val_loss=1.1281 acc=0.5998\n",
      "[Epoch 018] lr=0.00506 train_loss=1.1665 acc=0.5950 val_loss=1.1733 acc=0.5974\n",
      "[Epoch 019] lr=0.00507 train_loss=1.1667 acc=0.5925 val_loss=1.1533 acc=0.5904\n",
      "[Epoch 020] lr=0.00507 train_loss=1.1675 acc=0.5943 val_loss=1.1443 acc=0.5962\n",
      "FINAL TEST: loss=1.0020  top1_acc=0.6466\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc ▁▃▄▅▆▇▇▇▇███████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▆▄▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc ▁▂▄▄▆▆▇██▇██▇▆█▇██▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss ██▅▅▄▃▂▁▁▂▁▁▂▃▁▁▁▂▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.623\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.6466\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.00507\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.59433\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 1.16754\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.5962\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 1.14427\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mi0024\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/3lro9fny\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251015_035444-3lro9fny/logs\u001b[0m\n",
      ">>> \n",
      "            python -m vgg6_cifar.scripts.train_experiment                 --data_dir ./data                 --out_dir ./runs/sweeps_ext/actrelu_bnTrue_bs32_e20_lr0.01_m0.9_adamw                 --activation relu                 --batch_size 32                 --epochs 20                 --lr 0.01                 --momentum 0.9                 --optimizer adamw                 --weight_decay 0.0005                 --label_smoothing 0.0                 --seed 42                 --aug_hflip                 --aug_crop                 --aug_cutout                 --aug_jitter                 --use_bn                 --amp                 --wandb --wandb_project vgg6-cifar10-assignment --wandb_group q2q5-sweeps --run_name i0025\n",
      "            \n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m setting up run firqg8zm (0.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251015_035813-firqg8zm\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mi0025\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/firqg8zm\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.00500 train_loss=2.1758 acc=0.1557 val_loss=1.9906 acc=0.1844\n",
      "[Epoch 002] lr=0.00501 train_loss=1.9203 acc=0.2297 val_loss=1.6795 acc=0.3338\n",
      "[Epoch 003] lr=0.00501 train_loss=1.6965 acc=0.3400 val_loss=1.5145 acc=0.4350\n",
      "[Epoch 004] lr=0.00501 train_loss=1.4058 acc=0.4895 val_loss=1.1521 acc=0.5922\n",
      "[Epoch 005] lr=0.00502 train_loss=1.1784 acc=0.5860 val_loss=0.9842 acc=0.6410\n",
      "[Epoch 006] lr=0.00502 train_loss=1.0711 acc=0.6301 val_loss=1.0134 acc=0.6466\n",
      "[Epoch 007] lr=0.00502 train_loss=0.9831 acc=0.6686 val_loss=0.8722 acc=0.7044\n",
      "[Epoch 008] lr=0.00503 train_loss=0.9258 acc=0.6909 val_loss=0.7469 acc=0.7442\n",
      "[Epoch 009] lr=0.00503 train_loss=0.8549 acc=0.7162 val_loss=0.7465 acc=0.7484\n",
      "[Epoch 010] lr=0.00504 train_loss=0.7961 acc=0.7374 val_loss=0.7166 acc=0.7560\n",
      "[Epoch 011] lr=0.00504 train_loss=0.7600 acc=0.7527 val_loss=0.6803 acc=0.7610\n",
      "[Epoch 012] lr=0.00504 train_loss=0.7183 acc=0.7631 val_loss=0.6753 acc=0.7676\n",
      "[Epoch 013] lr=0.00505 train_loss=0.6853 acc=0.7736 val_loss=0.6076 acc=0.7968\n",
      "[Epoch 014] lr=0.00505 train_loss=0.6656 acc=0.7820 val_loss=0.6023 acc=0.7926\n",
      "[Epoch 015] lr=0.00505 train_loss=0.6357 acc=0.7923 val_loss=0.6411 acc=0.7936\n",
      "[Epoch 016] lr=0.00506 train_loss=0.6183 acc=0.7998 val_loss=0.5837 acc=0.8012\n",
      "[Epoch 017] lr=0.00506 train_loss=0.5936 acc=0.8069 val_loss=0.5191 acc=0.8252\n",
      "[Epoch 018] lr=0.00506 train_loss=0.5855 acc=0.8097 val_loss=0.5335 acc=0.8120\n",
      "[Epoch 019] lr=0.00507 train_loss=0.5608 acc=0.8188 val_loss=0.5519 acc=0.8124\n",
      "[Epoch 020] lr=0.00507 train_loss=0.5419 acc=0.8232 val_loss=0.5427 acc=0.8204\n",
      "FINAL TEST: loss=0.5041  top1_acc=0.8347\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣟\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc ▁▂▃▅▆▆▆▇▇▇▇▇▇███████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▇▆▅▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc ▁▃▄▅▆▆▇▇▇▇▇▇████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss █▇▆▄▃▃▃▂▂▂▂▂▁▁▂▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.8252\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.8347\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.00507\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.82316\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 0.54186\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.8204\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 0.54272\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mi0025\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/firqg8zm\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251015_035813-firqg8zm/logs\u001b[0m\n",
      ">>> \n",
      "            python -m vgg6_cifar.scripts.train_experiment                 --data_dir ./data                 --out_dir ./runs/sweeps_ext/actrelu_bnTrue_bs32_e20_lr0.01_m0.9_rmsprop                 --activation relu                 --batch_size 32                 --epochs 20                 --lr 0.01                 --momentum 0.9                 --optimizer rmsprop                 --weight_decay 0.0005                 --label_smoothing 0.0                 --seed 42                 --aug_hflip                 --aug_crop                 --aug_cutout                 --aug_jitter                 --use_bn                 --amp                 --wandb --wandb_project vgg6-cifar10-assignment --wandb_group q2q5-sweeps --run_name i0026\n",
      "            \n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m setting up run 2qz0ashz (0.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251015_040140-2qz0ashz\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mi0026\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/2qz0ashz\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.00500 train_loss=7.3977 acc=0.1190 val_loss=2.2627 acc=0.1618\n",
      "[Epoch 002] lr=0.00501 train_loss=2.3895 acc=0.1188 val_loss=2.2890 acc=0.1262\n",
      "[Epoch 003] lr=0.00501 train_loss=2.4070 acc=0.1206 val_loss=2.3009 acc=0.1188\n",
      "[Epoch 004] lr=0.00501 train_loss=2.3814 acc=0.1172 val_loss=30.3062 acc=0.0976\n",
      "[Epoch 005] lr=0.00502 train_loss=2.3988 acc=0.1216 val_loss=2.3286 acc=0.1146\n",
      "[Epoch 006] lr=0.00502 train_loss=2.3681 acc=0.1287 val_loss=2.5107 acc=0.1110\n",
      "[Epoch 007] lr=0.00502 train_loss=2.3847 acc=0.1281 val_loss=9.9815 acc=0.1352\n",
      "[Epoch 008] lr=0.00503 train_loss=2.3743 acc=0.1234 val_loss=2.3191 acc=0.0954\n",
      "[Epoch 009] lr=0.00503 train_loss=2.3774 acc=0.1209 val_loss=3.6224 acc=0.1150\n",
      "[Epoch 010] lr=0.00504 train_loss=2.3756 acc=0.1236 val_loss=4.0201 acc=0.1506\n",
      "[Epoch 011] lr=0.00504 train_loss=2.3812 acc=0.1220 val_loss=2.3224 acc=0.0934\n",
      "[Epoch 012] lr=0.00504 train_loss=2.3825 acc=0.1288 val_loss=3.6344 acc=0.1102\n",
      "[Epoch 013] lr=0.00505 train_loss=2.3905 acc=0.1181 val_loss=10.8579 acc=0.1274\n",
      "[Epoch 014] lr=0.00505 train_loss=2.3840 acc=0.1196 val_loss=2.3026 acc=0.1182\n",
      "[Epoch 015] lr=0.00505 train_loss=2.3787 acc=0.1200 val_loss=11.5842 acc=0.1464\n",
      "[Epoch 016] lr=0.00506 train_loss=2.3693 acc=0.1306 val_loss=51.3432 acc=0.1002\n",
      "[Epoch 017] lr=0.00506 train_loss=2.3842 acc=0.1270 val_loss=5.5116 acc=0.0976\n",
      "[Epoch 018] lr=0.00506 train_loss=2.3857 acc=0.1228 val_loss=2.3298 acc=0.0904\n",
      "[Epoch 019] lr=0.00507 train_loss=2.3867 acc=0.1210 val_loss=3.7064 acc=0.1462\n",
      "[Epoch 020] lr=0.00507 train_loss=2.3893 acc=0.1279 val_loss=2.4171 acc=0.0960\n",
      "FINAL TEST: loss=2.2654  top1_acc=0.1691\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣟\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc ▂▂▃▁▃▇▇▄▃▄▄▇▁▂▃█▆▄▃▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc █▅▄▂▃▃▅▁▃▇▁▃▅▄▆▂▂▁▆▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss ▁▁▁▅▁▁▂▁▁▁▁▁▂▁▂█▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.1618\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.1691\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.00507\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.12791\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 2.38926\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.096\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 2.41714\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mi0026\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/2qz0ashz\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251015_040140-2qz0ashz/logs\u001b[0m\n",
      ">>> \n",
      "            python -m vgg6_cifar.scripts.train_experiment                 --data_dir ./data                 --out_dir ./runs/sweeps_ext/actrelu_bnTrue_bs32_e20_lr0.01_m0.9_nadam                 --activation relu                 --batch_size 32                 --epochs 20                 --lr 0.01                 --momentum 0.9                 --optimizer nadam                 --weight_decay 0.0005                 --label_smoothing 0.0                 --seed 42                 --aug_hflip                 --aug_crop                 --aug_cutout                 --aug_jitter                 --use_bn                 --amp                 --wandb --wandb_project vgg6-cifar10-assignment --wandb_group q2q5-sweeps --run_name i0027\n",
      "            \n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m setting up run 9ffc5v02 (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251015_040508-9ffc5v02\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mi0027\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/9ffc5v02\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.00500 train_loss=2.1360 acc=0.1860 val_loss=1.9133 acc=0.2390\n",
      "[Epoch 002] lr=0.00501 train_loss=1.7090 acc=0.3539 val_loss=1.6050 acc=0.3978\n",
      "[Epoch 003] lr=0.00501 train_loss=1.4661 acc=0.4594 val_loss=1.3205 acc=0.5040\n",
      "[Epoch 004] lr=0.00501 train_loss=1.3428 acc=0.5196 val_loss=1.3218 acc=0.5476\n",
      "[Epoch 005] lr=0.00502 train_loss=1.2723 acc=0.5499 val_loss=1.2781 acc=0.5456\n",
      "[Epoch 006] lr=0.00502 train_loss=1.2185 acc=0.5696 val_loss=1.3428 acc=0.5246\n",
      "[Epoch 007] lr=0.00502 train_loss=1.1897 acc=0.5814 val_loss=1.1627 acc=0.5848\n",
      "[Epoch 008] lr=0.00503 train_loss=1.1678 acc=0.5882 val_loss=1.1065 acc=0.6000\n",
      "[Epoch 009] lr=0.00503 train_loss=1.1560 acc=0.5976 val_loss=1.1570 acc=0.6036\n",
      "[Epoch 010] lr=0.00504 train_loss=1.1491 acc=0.5980 val_loss=1.0360 acc=0.6256\n",
      "[Epoch 011] lr=0.00504 train_loss=1.1375 acc=0.6022 val_loss=1.0400 acc=0.6314\n",
      "[Epoch 012] lr=0.00504 train_loss=1.1265 acc=0.6094 val_loss=1.0447 acc=0.6204\n",
      "[Epoch 013] lr=0.00505 train_loss=1.1154 acc=0.6131 val_loss=0.9785 acc=0.6578\n",
      "[Epoch 014] lr=0.00505 train_loss=1.1336 acc=0.6064 val_loss=1.0418 acc=0.6312\n",
      "[Epoch 015] lr=0.00505 train_loss=1.1154 acc=0.6099 val_loss=1.0823 acc=0.6088\n",
      "[Epoch 016] lr=0.00506 train_loss=1.1119 acc=0.6154 val_loss=1.0658 acc=0.6262\n",
      "[Epoch 017] lr=0.00506 train_loss=1.1180 acc=0.6156 val_loss=1.0383 acc=0.6330\n",
      "[Epoch 018] lr=0.00506 train_loss=1.1039 acc=0.6169 val_loss=1.0507 acc=0.6238\n",
      "[Epoch 019] lr=0.00507 train_loss=1.1015 acc=0.6187 val_loss=1.0530 acc=0.6262\n",
      "[Epoch 020] lr=0.00507 train_loss=1.0983 acc=0.6222 val_loss=1.1863 acc=0.5886\n",
      "FINAL TEST: loss=0.9271  top1_acc=0.6696\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc ▁▄▅▆▇▇▇▇████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▅▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc ▁▄▅▆▆▆▇▇▇▇█▇██▇▇█▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss █▆▄▄▃▄▂▂▂▁▁▁▁▁▂▂▁▂▂▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.6578\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.6696\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.00507\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.62218\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 1.09832\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.5886\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 1.18635\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mi0027\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/9ffc5v02\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251015_040508-9ffc5v02/logs\u001b[0m\n",
      ">>> \n",
      "            python -m vgg6_cifar.scripts.train_experiment                 --data_dir ./data                 --out_dir ./runs/sweeps_ext/actrelu_bnTrue_bs32_e20_lr0.01_m0.9_adagrad                 --activation relu                 --batch_size 32                 --epochs 20                 --lr 0.01                 --momentum 0.9                 --optimizer adagrad                 --weight_decay 0.0005                 --label_smoothing 0.0                 --seed 42                 --aug_hflip                 --aug_crop                 --aug_cutout                 --aug_jitter                 --use_bn                 --amp                 --wandb --wandb_project vgg6-cifar10-assignment --wandb_group q2q5-sweeps --run_name i0028\n",
      "            \n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m setting up run y4lj4p49 (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251015_040840-y4lj4p49\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mi0028\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/y4lj4p49\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.00500 train_loss=1.6551 acc=0.4018 val_loss=1.3354 acc=0.5060\n",
      "[Epoch 002] lr=0.00501 train_loss=1.1805 acc=0.5760 val_loss=1.0434 acc=0.6256\n",
      "[Epoch 003] lr=0.00501 train_loss=0.9964 acc=0.6483 val_loss=0.9890 acc=0.6310\n",
      "[Epoch 004] lr=0.00501 train_loss=0.8893 acc=0.6903 val_loss=0.8599 acc=0.6910\n",
      "[Epoch 005] lr=0.00502 train_loss=0.8174 acc=0.7163 val_loss=0.7595 acc=0.7274\n",
      "[Epoch 006] lr=0.00502 train_loss=0.7578 acc=0.7362 val_loss=0.7400 acc=0.7380\n",
      "[Epoch 007] lr=0.00502 train_loss=0.7136 acc=0.7516 val_loss=0.6924 acc=0.7628\n",
      "[Epoch 008] lr=0.00503 train_loss=0.6778 acc=0.7674 val_loss=0.6596 acc=0.7644\n",
      "[Epoch 009] lr=0.00503 train_loss=0.6501 acc=0.7774 val_loss=0.6045 acc=0.7850\n",
      "[Epoch 010] lr=0.00504 train_loss=0.6211 acc=0.7884 val_loss=0.5673 acc=0.8022\n",
      "[Epoch 011] lr=0.00504 train_loss=0.5935 acc=0.7971 val_loss=0.6341 acc=0.7788\n",
      "[Epoch 012] lr=0.00504 train_loss=0.5729 acc=0.8014 val_loss=0.5665 acc=0.8004\n",
      "[Epoch 013] lr=0.00505 train_loss=0.5487 acc=0.8128 val_loss=0.5472 acc=0.8096\n",
      "[Epoch 014] lr=0.00505 train_loss=0.5311 acc=0.8178 val_loss=0.5618 acc=0.8048\n",
      "[Epoch 015] lr=0.00505 train_loss=0.5203 acc=0.8221 val_loss=0.5772 acc=0.7942\n",
      "[Epoch 016] lr=0.00506 train_loss=0.5031 acc=0.8263 val_loss=0.5048 acc=0.8214\n",
      "[Epoch 017] lr=0.00506 train_loss=0.4857 acc=0.8338 val_loss=0.4672 acc=0.8340\n",
      "[Epoch 018] lr=0.00506 train_loss=0.4773 acc=0.8388 val_loss=0.5713 acc=0.8048\n",
      "[Epoch 019] lr=0.00507 train_loss=0.4594 acc=0.8443 val_loss=0.4886 acc=0.8312\n",
      "[Epoch 020] lr=0.00507 train_loss=0.4511 acc=0.8453 val_loss=0.4714 acc=0.8362\n",
      "FINAL TEST: loss=0.4344  top1_acc=0.8487\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc ▁▄▅▆▆▆▇▇▇▇▇▇▇███████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▅▄▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc ▁▄▄▅▆▆▆▆▇▇▇▇▇▇▇██▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss █▆▅▄▃▃▃▃▂▂▂▂▂▂▂▁▁▂▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.8362\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.8487\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.00507\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.84533\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 0.4511\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.8362\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 0.47136\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mi0028\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/y4lj4p49\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251015_040840-y4lj4p49/logs\u001b[0m\n",
      ">>> \n",
      "            python -m vgg6_cifar.scripts.train_experiment                 --data_dir ./data                 --out_dir ./runs/sweeps_ext/actrelu_bnTrue_bs32_e20_lr0.001_m0.1_sgd                 --activation relu                 --batch_size 32                 --epochs 20                 --lr 0.001                 --momentum 0.1                 --optimizer sgd                 --weight_decay 0.0005                 --label_smoothing 0.0                 --seed 42                 --aug_hflip                 --aug_crop                 --aug_cutout                 --aug_jitter                 --use_bn                 --amp                 --wandb --wandb_project vgg6-cifar10-assignment --wandb_group q2q5-sweeps --run_name i0029\n",
      "            \n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m setting up run ol0xo286 (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251015_041208-ol0xo286\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mi0029\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/ol0xo286\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.00050 train_loss=2.2406 acc=0.1693 val_loss=2.1136 acc=0.2466\n",
      "[Epoch 002] lr=0.00050 train_loss=2.0376 acc=0.2495 val_loss=1.9263 acc=0.3138\n",
      "[Epoch 003] lr=0.00050 train_loss=1.8866 acc=0.3003 val_loss=1.7725 acc=0.3622\n",
      "[Epoch 004] lr=0.00050 train_loss=1.7682 acc=0.3417 val_loss=1.6826 acc=0.3954\n",
      "[Epoch 005] lr=0.00050 train_loss=1.6835 acc=0.3723 val_loss=1.5899 acc=0.4208\n",
      "[Epoch 006] lr=0.00050 train_loss=1.6215 acc=0.3965 val_loss=1.5374 acc=0.4380\n",
      "[Epoch 007] lr=0.00050 train_loss=1.5712 acc=0.4150 val_loss=1.5031 acc=0.4458\n",
      "[Epoch 008] lr=0.00050 train_loss=1.5370 acc=0.4303 val_loss=1.4552 acc=0.4658\n",
      "[Epoch 009] lr=0.00050 train_loss=1.5008 acc=0.4460 val_loss=1.4172 acc=0.4832\n",
      "[Epoch 010] lr=0.00050 train_loss=1.4620 acc=0.4642 val_loss=1.3845 acc=0.4956\n",
      "[Epoch 011] lr=0.00050 train_loss=1.4328 acc=0.4726 val_loss=1.3555 acc=0.4964\n",
      "[Epoch 012] lr=0.00050 train_loss=1.4102 acc=0.4821 val_loss=1.3176 acc=0.5218\n",
      "[Epoch 013] lr=0.00050 train_loss=1.3762 acc=0.4980 val_loss=1.3301 acc=0.5130\n",
      "[Epoch 014] lr=0.00050 train_loss=1.3552 acc=0.5029 val_loss=1.3053 acc=0.5244\n",
      "[Epoch 015] lr=0.00051 train_loss=1.3290 acc=0.5125 val_loss=1.2840 acc=0.5346\n",
      "[Epoch 016] lr=0.00051 train_loss=1.3016 acc=0.5263 val_loss=1.2439 acc=0.5502\n",
      "[Epoch 017] lr=0.00051 train_loss=1.2779 acc=0.5326 val_loss=1.2622 acc=0.5474\n",
      "[Epoch 018] lr=0.00051 train_loss=1.2548 acc=0.5438 val_loss=1.1906 acc=0.5716\n",
      "[Epoch 019] lr=0.00051 train_loss=1.2352 acc=0.5545 val_loss=1.1659 acc=0.5888\n",
      "[Epoch 020] lr=0.00051 train_loss=1.2185 acc=0.5580 val_loss=1.1331 acc=0.5930\n",
      "FINAL TEST: loss=1.0477  top1_acc=0.6223\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc ▁▂▃▄▅▅▅▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▇▆▅▄▄▃▃▃▃▂▂▂▂▂▂▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc ▁▂▃▄▅▅▅▅▆▆▆▇▆▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss █▇▆▅▄▄▄▃▃▃▃▂▂▂▂▂▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.593\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.6223\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.00051\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.55802\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 1.21848\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.593\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 1.13307\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mi0029\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/ol0xo286\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251015_041208-ol0xo286/logs\u001b[0m\n",
      ">>> \n",
      "            python -m vgg6_cifar.scripts.train_experiment                 --data_dir ./data                 --out_dir ./runs/sweeps_ext/actrelu_bnTrue_bs32_e20_lr0.001_m0.1_nesterov-sgd                 --activation relu                 --batch_size 32                 --epochs 20                 --lr 0.001                 --momentum 0.1                 --optimizer nesterov-sgd                 --weight_decay 0.0005                 --label_smoothing 0.0                 --seed 42                 --aug_hflip                 --aug_crop                 --aug_cutout                 --aug_jitter                 --use_bn                 --amp                 --wandb --wandb_project vgg6-cifar10-assignment --wandb_group q2q5-sweeps --run_name i0030\n",
      "            \n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m setting up run 8gutsczc (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251015_041540-8gutsczc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mi0030\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/8gutsczc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.00050 train_loss=2.2406 acc=0.1691 val_loss=2.1136 acc=0.2462\n",
      "[Epoch 002] lr=0.00050 train_loss=2.0374 acc=0.2503 val_loss=1.9262 acc=0.3142\n",
      "[Epoch 003] lr=0.00050 train_loss=1.8864 acc=0.3003 val_loss=1.7730 acc=0.3624\n",
      "[Epoch 004] lr=0.00050 train_loss=1.7681 acc=0.3410 val_loss=1.6797 acc=0.3936\n",
      "[Epoch 005] lr=0.00050 train_loss=1.6835 acc=0.3719 val_loss=1.5893 acc=0.4224\n",
      "[Epoch 006] lr=0.00050 train_loss=1.6218 acc=0.3967 val_loss=1.5422 acc=0.4378\n",
      "[Epoch 007] lr=0.00050 train_loss=1.5716 acc=0.4147 val_loss=1.5058 acc=0.4456\n",
      "[Epoch 008] lr=0.00050 train_loss=1.5376 acc=0.4296 val_loss=1.4611 acc=0.4678\n",
      "[Epoch 009] lr=0.00050 train_loss=1.5016 acc=0.4456 val_loss=1.4431 acc=0.4708\n",
      "[Epoch 010] lr=0.00050 train_loss=1.4629 acc=0.4629 val_loss=1.3838 acc=0.4934\n",
      "[Epoch 011] lr=0.00050 train_loss=1.4338 acc=0.4722 val_loss=1.3609 acc=0.4936\n",
      "[Epoch 012] lr=0.00050 train_loss=1.4115 acc=0.4817 val_loss=1.3236 acc=0.5182\n",
      "[Epoch 013] lr=0.00050 train_loss=1.3777 acc=0.4969 val_loss=1.3401 acc=0.5132\n",
      "[Epoch 014] lr=0.00050 train_loss=1.3572 acc=0.5021 val_loss=1.3141 acc=0.5192\n",
      "[Epoch 015] lr=0.00051 train_loss=1.3306 acc=0.5110 val_loss=1.2994 acc=0.5304\n",
      "[Epoch 016] lr=0.00051 train_loss=1.3036 acc=0.5260 val_loss=1.2648 acc=0.5432\n",
      "[Epoch 017] lr=0.00051 train_loss=1.2807 acc=0.5324 val_loss=1.2651 acc=0.5516\n",
      "[Epoch 018] lr=0.00051 train_loss=1.2567 acc=0.5432 val_loss=1.1995 acc=0.5724\n",
      "[Epoch 019] lr=0.00051 train_loss=1.2375 acc=0.5526 val_loss=1.1752 acc=0.5824\n",
      "[Epoch 020] lr=0.00051 train_loss=1.2208 acc=0.5571 val_loss=1.1417 acc=0.5910\n",
      "FINAL TEST: loss=1.0536  top1_acc=0.6196\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc ▁▂▃▄▅▅▅▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▇▆▅▄▄▃▃▃▃▂▂▂▂▂▂▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc ▁▂▃▄▅▅▅▅▆▆▆▇▆▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss █▇▆▅▄▄▄▃▃▃▃▂▂▂▂▂▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.591\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.6196\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.00051\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.55711\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 1.2208\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.591\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 1.14172\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mi0030\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/8gutsczc\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251015_041540-8gutsczc/logs\u001b[0m\n",
      ">>> \n",
      "            python -m vgg6_cifar.scripts.train_experiment                 --data_dir ./data                 --out_dir ./runs/sweeps_ext/actrelu_bnTrue_bs32_e20_lr0.001_m0.1_adam                 --activation relu                 --batch_size 32                 --epochs 20                 --lr 0.001                 --momentum 0.1                 --optimizer adam                 --weight_decay 0.0005                 --label_smoothing 0.0                 --seed 42                 --aug_hflip                 --aug_crop                 --aug_cutout                 --aug_jitter                 --use_bn                 --amp                 --wandb --wandb_project vgg6-cifar10-assignment --wandb_group q2q5-sweeps --run_name i0031\n",
      "            \n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m setting up run 87hdnmiy (0.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m setting up run 87hdnmiy (0.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251015_041909-87hdnmiy\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mi0031\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/87hdnmiy\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.00050 train_loss=1.4498 acc=0.4675 val_loss=1.1773 acc=0.5702\n",
      "[Epoch 002] lr=0.00050 train_loss=1.0256 acc=0.6358 val_loss=0.8816 acc=0.6800\n",
      "[Epoch 003] lr=0.00050 train_loss=0.8856 acc=0.6920 val_loss=0.8353 acc=0.7062\n",
      "[Epoch 004] lr=0.00050 train_loss=0.7918 acc=0.7280 val_loss=0.7885 acc=0.7256\n",
      "[Epoch 005] lr=0.00050 train_loss=0.7265 acc=0.7536 val_loss=0.6670 acc=0.7624\n",
      "[Epoch 006] lr=0.00050 train_loss=0.6798 acc=0.7671 val_loss=0.6550 acc=0.7752\n",
      "[Epoch 007] lr=0.00050 train_loss=0.6344 acc=0.7848 val_loss=0.5677 acc=0.8022\n",
      "[Epoch 008] lr=0.00050 train_loss=0.5958 acc=0.7995 val_loss=0.5537 acc=0.8112\n",
      "[Epoch 009] lr=0.00050 train_loss=0.5673 acc=0.8101 val_loss=0.5819 acc=0.7990\n",
      "[Epoch 010] lr=0.00050 train_loss=0.5385 acc=0.8184 val_loss=0.5985 acc=0.7930\n",
      "[Epoch 011] lr=0.00050 train_loss=0.5154 acc=0.8277 val_loss=0.5055 acc=0.8324\n",
      "[Epoch 012] lr=0.00050 train_loss=0.4955 acc=0.8337 val_loss=0.5134 acc=0.8270\n",
      "[Epoch 013] lr=0.00050 train_loss=0.4743 acc=0.8409 val_loss=0.4702 acc=0.8386\n",
      "[Epoch 014] lr=0.00050 train_loss=0.4641 acc=0.8446 val_loss=0.4664 acc=0.8424\n",
      "[Epoch 015] lr=0.00051 train_loss=0.4493 acc=0.8501 val_loss=0.4826 acc=0.8386\n",
      "[Epoch 016] lr=0.00051 train_loss=0.4430 acc=0.8524 val_loss=0.4693 acc=0.8460\n",
      "[Epoch 017] lr=0.00051 train_loss=0.4268 acc=0.8577 val_loss=0.4355 acc=0.8528\n",
      "[Epoch 018] lr=0.00051 train_loss=0.4183 acc=0.8601 val_loss=0.4329 acc=0.8498\n",
      "[Epoch 019] lr=0.00051 train_loss=0.4040 acc=0.8646 val_loss=0.4718 acc=0.8384\n",
      "[Epoch 020] lr=0.00051 train_loss=0.4026 acc=0.8650 val_loss=0.4735 acc=0.8436\n",
      "FINAL TEST: loss=0.4171  top1_acc=0.8596\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc ▁▄▅▆▆▆▇▇▇▇▇▇████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▅▄▄▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc ▁▄▄▅▆▆▇▇▇▇▇▇████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss █▅▅▄▃▃▂▂▂▃▂▂▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.8528\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.8596\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.00051\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.86504\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 0.40258\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.8436\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 0.47346\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mi0031\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/87hdnmiy\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251015_041909-87hdnmiy/logs\u001b[0m\n",
      ">>> \n",
      "            python -m vgg6_cifar.scripts.train_experiment                 --data_dir ./data                 --out_dir ./runs/sweeps_ext/actrelu_bnTrue_bs32_e20_lr0.001_m0.1_adamw                 --activation relu                 --batch_size 32                 --epochs 20                 --lr 0.001                 --momentum 0.1                 --optimizer adamw                 --weight_decay 0.0005                 --label_smoothing 0.0                 --seed 42                 --aug_hflip                 --aug_crop                 --aug_cutout                 --aug_jitter                 --use_bn                 --amp                 --wandb --wandb_project vgg6-cifar10-assignment --wandb_group q2q5-sweeps --run_name i0032\n",
      "            \n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251015_042239-a9g5ylkz\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mi0032\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/a9g5ylkz\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.00050 train_loss=1.4430 acc=0.4716 val_loss=1.1864 acc=0.5678\n",
      "[Epoch 002] lr=0.00050 train_loss=1.0143 acc=0.6407 val_loss=0.8704 acc=0.6904\n",
      "[Epoch 003] lr=0.00050 train_loss=0.8696 acc=0.6973 val_loss=0.7665 acc=0.7332\n",
      "[Epoch 004] lr=0.00050 train_loss=0.7800 acc=0.7300 val_loss=0.7308 acc=0.7452\n",
      "[Epoch 005] lr=0.00050 train_loss=0.7115 acc=0.7574 val_loss=0.6654 acc=0.7628\n",
      "[Epoch 006] lr=0.00050 train_loss=0.6596 acc=0.7754 val_loss=0.5881 acc=0.7982\n",
      "[Epoch 007] lr=0.00050 train_loss=0.6157 acc=0.7906 val_loss=0.5717 acc=0.7986\n",
      "[Epoch 008] lr=0.00050 train_loss=0.5743 acc=0.8046 val_loss=0.5217 acc=0.8216\n",
      "[Epoch 009] lr=0.00050 train_loss=0.5438 acc=0.8164 val_loss=0.5255 acc=0.8130\n",
      "[Epoch 010] lr=0.00050 train_loss=0.5166 acc=0.8260 val_loss=0.5103 acc=0.8222\n",
      "[Epoch 011] lr=0.00050 train_loss=0.4859 acc=0.8359 val_loss=0.4966 acc=0.8244\n",
      "[Epoch 012] lr=0.00050 train_loss=0.4624 acc=0.8435 val_loss=0.4708 acc=0.8370\n",
      "[Epoch 013] lr=0.00050 train_loss=0.4379 acc=0.8518 val_loss=0.4551 acc=0.8422\n",
      "[Epoch 014] lr=0.00050 train_loss=0.4241 acc=0.8568 val_loss=0.4588 acc=0.8402\n",
      "[Epoch 015] lr=0.00051 train_loss=0.4059 acc=0.8611 val_loss=0.4584 acc=0.8474\n",
      "[Epoch 016] lr=0.00051 train_loss=0.3920 acc=0.8670 val_loss=0.4178 acc=0.8562\n",
      "[Epoch 017] lr=0.00051 train_loss=0.3709 acc=0.8755 val_loss=0.4060 acc=0.8600\n",
      "[Epoch 018] lr=0.00051 train_loss=0.3549 acc=0.8796 val_loss=0.4100 acc=0.8586\n",
      "[Epoch 019] lr=0.00051 train_loss=0.3401 acc=0.8839 val_loss=0.4040 acc=0.8632\n",
      "[Epoch 020] lr=0.00051 train_loss=0.3274 acc=0.8911 val_loss=0.3764 acc=0.8706\n",
      "FINAL TEST: loss=0.3666  top1_acc=0.8801\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc ▁▄▅▅▆▆▆▇▇▇▇▇▇▇▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▅▄▄▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc ▁▄▅▅▆▆▆▇▇▇▇▇▇▇▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss █▅▄▄▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.8706\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.8801\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.00051\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.89109\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 0.32737\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.8706\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 0.37637\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mi0032\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/a9g5ylkz\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251015_042239-a9g5ylkz/logs\u001b[0m\n",
      ">>> \n",
      "            python -m vgg6_cifar.scripts.train_experiment                 --data_dir ./data                 --out_dir ./runs/sweeps_ext/actrelu_bnTrue_bs32_e20_lr0.001_m0.1_rmsprop                 --activation relu                 --batch_size 32                 --epochs 20                 --lr 0.001                 --momentum 0.1                 --optimizer rmsprop                 --weight_decay 0.0005                 --label_smoothing 0.0                 --seed 42                 --aug_hflip                 --aug_crop                 --aug_cutout                 --aug_jitter                 --use_bn                 --amp                 --wandb --wandb_project vgg6-cifar10-assignment --wandb_group q2q5-sweeps --run_name i0033\n",
      "            \n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m setting up run t8e263td (0.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251015_042621-t8e263td\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mi0033\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/t8e263td\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.00050 train_loss=1.6541 acc=0.4062 val_loss=1.3380 acc=0.5056\n",
      "[Epoch 002] lr=0.00050 train_loss=1.1448 acc=0.5956 val_loss=1.3058 acc=0.5588\n",
      "[Epoch 003] lr=0.00050 train_loss=0.9609 acc=0.6673 val_loss=0.9926 acc=0.6462\n",
      "[Epoch 004] lr=0.00050 train_loss=0.8482 acc=0.7098 val_loss=0.8717 acc=0.6966\n",
      "[Epoch 005] lr=0.00050 train_loss=0.7775 acc=0.7370 val_loss=0.8157 acc=0.7178\n",
      "[Epoch 006] lr=0.00050 train_loss=0.7160 acc=0.7563 val_loss=0.8091 acc=0.7226\n",
      "[Epoch 007] lr=0.00050 train_loss=0.6643 acc=0.7755 val_loss=0.6990 acc=0.7602\n",
      "[Epoch 008] lr=0.00050 train_loss=0.6219 acc=0.7916 val_loss=0.5625 acc=0.8086\n",
      "[Epoch 009] lr=0.00050 train_loss=0.5960 acc=0.8010 val_loss=0.6892 acc=0.7762\n",
      "[Epoch 010] lr=0.00050 train_loss=0.5615 acc=0.8122 val_loss=0.5856 acc=0.8004\n",
      "[Epoch 011] lr=0.00050 train_loss=0.5402 acc=0.8196 val_loss=0.7461 acc=0.7576\n",
      "[Epoch 012] lr=0.00050 train_loss=0.5171 acc=0.8267 val_loss=0.5699 acc=0.8076\n",
      "[Epoch 013] lr=0.00050 train_loss=0.4963 acc=0.8334 val_loss=0.6325 acc=0.7902\n",
      "[Epoch 014] lr=0.00050 train_loss=0.4840 acc=0.8374 val_loss=0.9146 acc=0.7162\n",
      "[Epoch 015] lr=0.00051 train_loss=0.4721 acc=0.8416 val_loss=0.5221 acc=0.8252\n",
      "[Epoch 016] lr=0.00051 train_loss=0.4591 acc=0.8460 val_loss=0.7111 acc=0.7628\n",
      "[Epoch 017] lr=0.00051 train_loss=0.4521 acc=0.8490 val_loss=0.5478 acc=0.8112\n",
      "[Epoch 018] lr=0.00051 train_loss=0.4367 acc=0.8549 val_loss=0.5328 acc=0.8286\n",
      "[Epoch 019] lr=0.00051 train_loss=0.4317 acc=0.8559 val_loss=0.5130 acc=0.8296\n",
      "[Epoch 020] lr=0.00051 train_loss=0.4187 acc=0.8622 val_loss=0.6553 acc=0.7868\n",
      "FINAL TEST: loss=0.5293  top1_acc=0.8323\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc ▁▄▅▆▆▆▇▇▇▇▇▇████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▅▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc ▁▂▄▅▆▆▇█▇▇▆█▇▆█▇███▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss ██▅▄▄▄▃▁▂▂▃▁▂▄▁▃▁▁▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.8296\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.8323\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.00051\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.8622\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 0.41872\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.7868\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 0.65525\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mi0033\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/t8e263td\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251015_042621-t8e263td/logs\u001b[0m\n",
      ">>> \n",
      "            python -m vgg6_cifar.scripts.train_experiment                 --data_dir ./data                 --out_dir ./runs/sweeps_ext/actrelu_bnTrue_bs32_e20_lr0.001_m0.1_nadam                 --activation relu                 --batch_size 32                 --epochs 20                 --lr 0.001                 --momentum 0.1                 --optimizer nadam                 --weight_decay 0.0005                 --label_smoothing 0.0                 --seed 42                 --aug_hflip                 --aug_crop                 --aug_cutout                 --aug_jitter                 --use_bn                 --amp                 --wandb --wandb_project vgg6-cifar10-assignment --wandb_group q2q5-sweeps --run_name i0034\n",
      "            \n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m setting up run 2f38imnu (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251015_042951-2f38imnu\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mi0034\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/2f38imnu\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.00050 train_loss=1.3755 acc=0.4980 val_loss=1.0855 acc=0.6052\n",
      "[Epoch 002] lr=0.00050 train_loss=0.9681 acc=0.6613 val_loss=0.9334 acc=0.6748\n",
      "[Epoch 003] lr=0.00050 train_loss=0.8339 acc=0.7112 val_loss=0.8055 acc=0.7074\n",
      "[Epoch 004] lr=0.00050 train_loss=0.7522 acc=0.7444 val_loss=0.7642 acc=0.7324\n",
      "[Epoch 005] lr=0.00050 train_loss=0.6919 acc=0.7637 val_loss=0.6890 acc=0.7662\n",
      "[Epoch 006] lr=0.00050 train_loss=0.6458 acc=0.7812 val_loss=0.6938 acc=0.7632\n",
      "[Epoch 007] lr=0.00050 train_loss=0.6041 acc=0.7960 val_loss=0.5742 acc=0.8040\n",
      "[Epoch 008] lr=0.00050 train_loss=0.5750 acc=0.8056 val_loss=0.5636 acc=0.8050\n",
      "[Epoch 009] lr=0.00050 train_loss=0.5510 acc=0.8136 val_loss=0.6306 acc=0.7902\n",
      "[Epoch 010] lr=0.00050 train_loss=0.5240 acc=0.8239 val_loss=0.5888 acc=0.7966\n",
      "[Epoch 011] lr=0.00050 train_loss=0.5022 acc=0.8323 val_loss=0.5172 acc=0.8226\n",
      "[Epoch 012] lr=0.00050 train_loss=0.4831 acc=0.8366 val_loss=0.4846 acc=0.8358\n",
      "[Epoch 013] lr=0.00050 train_loss=0.4664 acc=0.8426 val_loss=0.5020 acc=0.8264\n",
      "[Epoch 014] lr=0.00050 train_loss=0.4551 acc=0.8484 val_loss=0.5094 acc=0.8284\n",
      "[Epoch 015] lr=0.00051 train_loss=0.4429 acc=0.8502 val_loss=0.4859 acc=0.8318\n",
      "[Epoch 016] lr=0.00051 train_loss=0.4336 acc=0.8543 val_loss=0.4767 acc=0.8350\n",
      "[Epoch 017] lr=0.00051 train_loss=0.4191 acc=0.8588 val_loss=0.4309 acc=0.8528\n",
      "[Epoch 018] lr=0.00051 train_loss=0.4075 acc=0.8633 val_loss=0.4048 acc=0.8642\n",
      "[Epoch 019] lr=0.00051 train_loss=0.4012 acc=0.8658 val_loss=0.5093 acc=0.8286\n",
      "[Epoch 020] lr=0.00051 train_loss=0.3938 acc=0.8680 val_loss=0.4774 acc=0.8430\n",
      "FINAL TEST: loss=0.3746  top1_acc=0.8769\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc ▁▄▅▆▆▆▇▇▇▇▇▇████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▅▄▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc ▁▃▄▄▅▅▆▆▆▆▇▇▇▇▇▇██▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss █▆▅▅▄▄▃▃▃▃▂▂▂▂▂▂▁▁▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.8642\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.8769\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.00051\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.86798\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 0.39379\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.843\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 0.47742\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mi0034\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/2f38imnu\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251015_042951-2f38imnu/logs\u001b[0m\n",
      ">>> \n",
      "            python -m vgg6_cifar.scripts.train_experiment                 --data_dir ./data                 --out_dir ./runs/sweeps_ext/actrelu_bnTrue_bs32_e20_lr0.001_m0.1_adagrad                 --activation relu                 --batch_size 32                 --epochs 20                 --lr 0.001                 --momentum 0.1                 --optimizer adagrad                 --weight_decay 0.0005                 --label_smoothing 0.0                 --seed 42                 --aug_hflip                 --aug_crop                 --aug_cutout                 --aug_jitter                 --use_bn                 --amp                 --wandb --wandb_project vgg6-cifar10-assignment --wandb_group q2q5-sweeps --run_name i0035\n",
      "            \n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m setting up run dfwxycas (0.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251015_043333-dfwxycas\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mi0035\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/dfwxycas\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.00050 train_loss=1.6676 acc=0.3832 val_loss=1.4626 acc=0.4634\n",
      "[Epoch 002] lr=0.00050 train_loss=1.4512 acc=0.4678 val_loss=1.3493 acc=0.5116\n",
      "[Epoch 003] lr=0.00050 train_loss=1.3650 acc=0.5018 val_loss=1.2801 acc=0.5382\n",
      "[Epoch 004] lr=0.00050 train_loss=1.3048 acc=0.5248 val_loss=1.2352 acc=0.5592\n",
      "[Epoch 005] lr=0.00050 train_loss=1.2608 acc=0.5465 val_loss=1.1933 acc=0.5718\n",
      "[Epoch 006] lr=0.00050 train_loss=1.2253 acc=0.5574 val_loss=1.1725 acc=0.5898\n",
      "[Epoch 007] lr=0.00050 train_loss=1.1958 acc=0.5708 val_loss=1.1294 acc=0.5970\n",
      "[Epoch 008] lr=0.00050 train_loss=1.1750 acc=0.5788 val_loss=1.1179 acc=0.6100\n",
      "[Epoch 009] lr=0.00050 train_loss=1.1579 acc=0.5848 val_loss=1.0970 acc=0.6064\n",
      "[Epoch 010] lr=0.00050 train_loss=1.1370 acc=0.5921 val_loss=1.0736 acc=0.6156\n",
      "[Epoch 011] lr=0.00050 train_loss=1.1196 acc=0.6003 val_loss=1.0678 acc=0.6168\n",
      "[Epoch 012] lr=0.00050 train_loss=1.1084 acc=0.6021 val_loss=1.0369 acc=0.6268\n",
      "[Epoch 013] lr=0.00050 train_loss=1.0897 acc=0.6106 val_loss=1.0284 acc=0.6346\n",
      "[Epoch 014] lr=0.00050 train_loss=1.0804 acc=0.6167 val_loss=1.0343 acc=0.6290\n",
      "[Epoch 015] lr=0.00051 train_loss=1.0729 acc=0.6176 val_loss=1.0198 acc=0.6374\n",
      "[Epoch 016] lr=0.00051 train_loss=1.0586 acc=0.6244 val_loss=0.9965 acc=0.6506\n",
      "[Epoch 017] lr=0.00051 train_loss=1.0494 acc=0.6283 val_loss=0.9918 acc=0.6554\n",
      "[Epoch 018] lr=0.00051 train_loss=1.0392 acc=0.6310 val_loss=0.9821 acc=0.6556\n",
      "[Epoch 019] lr=0.00051 train_loss=1.0344 acc=0.6321 val_loss=0.9817 acc=0.6602\n",
      "[Epoch 020] lr=0.00051 train_loss=1.0245 acc=0.6347 val_loss=0.9754 acc=0.6584\n",
      "FINAL TEST: loss=0.8978  top1_acc=0.6776\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣟\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc ▁▃▄▅▆▆▆▆▇▇▇▇▇▇██████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▆▅▄▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc ▁▃▄▄▅▅▆▆▆▆▆▇▇▇▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss █▆▅▅▄▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.6602\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.6776\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.00051\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.63471\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 1.02446\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.6584\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 0.97541\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mi0035\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/dfwxycas\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251015_043333-dfwxycas/logs\u001b[0m\n",
      ">>> \n",
      "            python -m vgg6_cifar.scripts.train_experiment                 --data_dir ./data                 --out_dir ./runs/sweeps_ext/actrelu_bnTrue_bs32_e20_lr0.001_m0.9_sgd                 --activation relu                 --batch_size 32                 --epochs 20                 --lr 0.001                 --momentum 0.9                 --optimizer sgd                 --weight_decay 0.0005                 --label_smoothing 0.0                 --seed 42                 --aug_hflip                 --aug_crop                 --aug_cutout                 --aug_jitter                 --use_bn                 --amp                 --wandb --wandb_project vgg6-cifar10-assignment --wandb_group q2q5-sweeps --run_name i0036\n",
      "            \n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m setting up run us7ll863 (0.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m setting up run us7ll863 (0.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251015_043706-us7ll863\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mi0036\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/us7ll863\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.00050 train_loss=1.8242 acc=0.3186 val_loss=1.5480 acc=0.4400\n",
      "[Epoch 002] lr=0.00050 train_loss=1.4670 acc=0.4625 val_loss=1.3590 acc=0.5036\n",
      "[Epoch 003] lr=0.00050 train_loss=1.3113 acc=0.5215 val_loss=1.1920 acc=0.5728\n",
      "[Epoch 004] lr=0.00050 train_loss=1.1855 acc=0.5728 val_loss=1.0788 acc=0.6220\n",
      "[Epoch 005] lr=0.00050 train_loss=1.0939 acc=0.6040 val_loss=1.0370 acc=0.6238\n",
      "[Epoch 006] lr=0.00050 train_loss=1.0176 acc=0.6392 val_loss=0.9671 acc=0.6650\n",
      "[Epoch 007] lr=0.00050 train_loss=0.9583 acc=0.6569 val_loss=0.8799 acc=0.6810\n",
      "[Epoch 008] lr=0.00050 train_loss=0.9107 acc=0.6763 val_loss=0.8638 acc=0.6922\n",
      "[Epoch 009] lr=0.00050 train_loss=0.8690 acc=0.6907 val_loss=0.7936 acc=0.7140\n",
      "[Epoch 010] lr=0.00050 train_loss=0.8386 acc=0.7038 val_loss=0.7670 acc=0.7178\n",
      "[Epoch 011] lr=0.00050 train_loss=0.7994 acc=0.7162 val_loss=0.7555 acc=0.7278\n",
      "[Epoch 012] lr=0.00050 train_loss=0.7763 acc=0.7266 val_loss=0.7202 acc=0.7382\n",
      "[Epoch 013] lr=0.00050 train_loss=0.7472 acc=0.7366 val_loss=0.6804 acc=0.7594\n",
      "[Epoch 014] lr=0.00050 train_loss=0.7264 acc=0.7444 val_loss=0.6726 acc=0.7612\n",
      "[Epoch 015] lr=0.00051 train_loss=0.7064 acc=0.7525 val_loss=0.7000 acc=0.7480\n",
      "[Epoch 016] lr=0.00051 train_loss=0.6857 acc=0.7576 val_loss=0.6554 acc=0.7648\n",
      "[Epoch 017] lr=0.00051 train_loss=0.6698 acc=0.7664 val_loss=0.6145 acc=0.7816\n",
      "[Epoch 018] lr=0.00051 train_loss=0.6481 acc=0.7723 val_loss=0.6255 acc=0.7788\n",
      "[Epoch 019] lr=0.00051 train_loss=0.6332 acc=0.7795 val_loss=0.6225 acc=0.7814\n",
      "[Epoch 020] lr=0.00051 train_loss=0.6259 acc=0.7800 val_loss=0.5990 acc=0.7864\n",
      "FINAL TEST: loss=0.5535  top1_acc=0.8082\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc ▁▃▄▅▅▆▆▆▇▇▇▇▇▇██████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▆▅▄▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc ▁▂▄▅▅▆▆▆▇▇▇▇▇▇▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss █▇▅▅▄▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.7864\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.8082\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.00051\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.78\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 0.62593\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.7864\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 0.59905\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mi0036\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/us7ll863\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251015_043706-us7ll863/logs\u001b[0m\n",
      ">>> \n",
      "            python -m vgg6_cifar.scripts.train_experiment                 --data_dir ./data                 --out_dir ./runs/sweeps_ext/actrelu_bnTrue_bs32_e20_lr0.001_m0.9_nesterov-sgd                 --activation relu                 --batch_size 32                 --epochs 20                 --lr 0.001                 --momentum 0.9                 --optimizer nesterov-sgd                 --weight_decay 0.0005                 --label_smoothing 0.0                 --seed 42                 --aug_hflip                 --aug_crop                 --aug_cutout                 --aug_jitter                 --use_bn                 --amp                 --wandb --wandb_project vgg6-cifar10-assignment --wandb_group q2q5-sweeps --run_name i0037\n",
      "            \n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m setting up run y70mzvfv (0.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251015_044038-y70mzvfv\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mi0037\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/y70mzvfv\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.00050 train_loss=1.8212 acc=0.3193 val_loss=1.5373 acc=0.4370\n",
      "[Epoch 002] lr=0.00050 train_loss=1.4672 acc=0.4612 val_loss=1.3901 acc=0.4950\n",
      "[Epoch 003] lr=0.00050 train_loss=1.3111 acc=0.5244 val_loss=1.2120 acc=0.5714\n",
      "[Epoch 004] lr=0.00050 train_loss=1.1806 acc=0.5751 val_loss=1.0934 acc=0.6170\n",
      "[Epoch 005] lr=0.00050 train_loss=1.0846 acc=0.6099 val_loss=1.0201 acc=0.6286\n",
      "[Epoch 006] lr=0.00050 train_loss=1.0118 acc=0.6392 val_loss=0.9502 acc=0.6706\n",
      "[Epoch 007] lr=0.00050 train_loss=0.9526 acc=0.6583 val_loss=0.8801 acc=0.6868\n",
      "[Epoch 008] lr=0.00050 train_loss=0.9051 acc=0.6778 val_loss=0.8658 acc=0.6902\n",
      "[Epoch 009] lr=0.00050 train_loss=0.8668 acc=0.6930 val_loss=0.7875 acc=0.7180\n",
      "[Epoch 010] lr=0.00050 train_loss=0.8337 acc=0.7060 val_loss=0.7851 acc=0.7188\n",
      "[Epoch 011] lr=0.00050 train_loss=0.7960 acc=0.7176 val_loss=0.7415 acc=0.7342\n",
      "[Epoch 012] lr=0.00050 train_loss=0.7733 acc=0.7282 val_loss=0.7143 acc=0.7412\n",
      "[Epoch 013] lr=0.00050 train_loss=0.7443 acc=0.7373 val_loss=0.6866 acc=0.7558\n",
      "[Epoch 014] lr=0.00050 train_loss=0.7240 acc=0.7462 val_loss=0.6977 acc=0.7486\n",
      "[Epoch 015] lr=0.00051 train_loss=0.7020 acc=0.7527 val_loss=0.6861 acc=0.7492\n",
      "[Epoch 016] lr=0.00051 train_loss=0.6826 acc=0.7574 val_loss=0.6693 acc=0.7662\n",
      "[Epoch 017] lr=0.00051 train_loss=0.6673 acc=0.7672 val_loss=0.6037 acc=0.7876\n",
      "[Epoch 018] lr=0.00051 train_loss=0.6470 acc=0.7731 val_loss=0.6177 acc=0.7824\n",
      "[Epoch 019] lr=0.00051 train_loss=0.6335 acc=0.7774 val_loss=0.6310 acc=0.7766\n",
      "[Epoch 020] lr=0.00051 train_loss=0.6225 acc=0.7823 val_loss=0.6047 acc=0.7916\n",
      "FINAL TEST: loss=0.5656  top1_acc=0.8024\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc ▁▃▄▅▅▆▆▆▇▇▇▇▇▇██████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▆▅▄▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc ▁▂▄▅▅▆▆▆▇▇▇▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss █▇▆▅▄▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.7916\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.8024\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.00051\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.78229\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 0.62253\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.7916\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 0.60475\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mi0037\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/y70mzvfv\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251015_044038-y70mzvfv/logs\u001b[0m\n",
      ">>> \n",
      "            python -m vgg6_cifar.scripts.train_experiment                 --data_dir ./data                 --out_dir ./runs/sweeps_ext/actrelu_bnTrue_bs32_e20_lr0.001_m0.9_adam                 --activation relu                 --batch_size 32                 --epochs 20                 --lr 0.001                 --momentum 0.9                 --optimizer adam                 --weight_decay 0.0005                 --label_smoothing 0.0                 --seed 42                 --aug_hflip                 --aug_crop                 --aug_cutout                 --aug_jitter                 --use_bn                 --amp                 --wandb --wandb_project vgg6-cifar10-assignment --wandb_group q2q5-sweeps --run_name i0038\n",
      "            \n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m setting up run 9vaeqomw (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251015_044405-9vaeqomw\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mi0038\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/9vaeqomw\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.00050 train_loss=1.4498 acc=0.4675 val_loss=1.1773 acc=0.5702\n",
      "[Epoch 002] lr=0.00050 train_loss=1.0256 acc=0.6358 val_loss=0.8816 acc=0.6800\n",
      "[Epoch 003] lr=0.00050 train_loss=0.8856 acc=0.6920 val_loss=0.8353 acc=0.7062\n",
      "[Epoch 004] lr=0.00050 train_loss=0.7918 acc=0.7280 val_loss=0.7885 acc=0.7256\n",
      "[Epoch 005] lr=0.00050 train_loss=0.7265 acc=0.7536 val_loss=0.6670 acc=0.7624\n",
      "[Epoch 006] lr=0.00050 train_loss=0.6798 acc=0.7671 val_loss=0.6550 acc=0.7752\n",
      "[Epoch 007] lr=0.00050 train_loss=0.6344 acc=0.7848 val_loss=0.5677 acc=0.8022\n",
      "[Epoch 008] lr=0.00050 train_loss=0.5958 acc=0.7995 val_loss=0.5537 acc=0.8112\n",
      "[Epoch 009] lr=0.00050 train_loss=0.5673 acc=0.8101 val_loss=0.5819 acc=0.7990\n",
      "[Epoch 010] lr=0.00050 train_loss=0.5385 acc=0.8184 val_loss=0.5985 acc=0.7930\n",
      "[Epoch 011] lr=0.00050 train_loss=0.5154 acc=0.8277 val_loss=0.5055 acc=0.8324\n",
      "[Epoch 012] lr=0.00050 train_loss=0.4955 acc=0.8337 val_loss=0.5134 acc=0.8270\n",
      "[Epoch 013] lr=0.00050 train_loss=0.4743 acc=0.8409 val_loss=0.4702 acc=0.8386\n",
      "[Epoch 014] lr=0.00050 train_loss=0.4641 acc=0.8446 val_loss=0.4664 acc=0.8424\n",
      "[Epoch 015] lr=0.00051 train_loss=0.4493 acc=0.8501 val_loss=0.4826 acc=0.8386\n",
      "[Epoch 016] lr=0.00051 train_loss=0.4430 acc=0.8524 val_loss=0.4693 acc=0.8460\n",
      "[Epoch 017] lr=0.00051 train_loss=0.4268 acc=0.8577 val_loss=0.4355 acc=0.8528\n",
      "[Epoch 018] lr=0.00051 train_loss=0.4183 acc=0.8601 val_loss=0.4329 acc=0.8498\n",
      "[Epoch 019] lr=0.00051 train_loss=0.4040 acc=0.8646 val_loss=0.4718 acc=0.8384\n",
      "[Epoch 020] lr=0.00051 train_loss=0.4026 acc=0.8650 val_loss=0.4735 acc=0.8436\n",
      "FINAL TEST: loss=0.4171  top1_acc=0.8596\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc ▁▄▅▆▆▆▇▇▇▇▇▇████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▅▄▄▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc ▁▄▄▅▆▆▇▇▇▇▇▇████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss █▅▅▄▃▃▂▂▂▃▂▂▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.8528\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.8596\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.00051\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.86504\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 0.40258\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.8436\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 0.47346\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mi0038\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/9vaeqomw\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251015_044405-9vaeqomw/logs\u001b[0m\n",
      ">>> \n",
      "            python -m vgg6_cifar.scripts.train_experiment                 --data_dir ./data                 --out_dir ./runs/sweeps_ext/actrelu_bnTrue_bs32_e20_lr0.001_m0.9_adamw                 --activation relu                 --batch_size 32                 --epochs 20                 --lr 0.001                 --momentum 0.9                 --optimizer adamw                 --weight_decay 0.0005                 --label_smoothing 0.0                 --seed 42                 --aug_hflip                 --aug_crop                 --aug_cutout                 --aug_jitter                 --use_bn                 --amp                 --wandb --wandb_project vgg6-cifar10-assignment --wandb_group q2q5-sweeps --run_name i0039\n",
      "            \n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m setting up run 49fm7wnf (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251015_044735-49fm7wnf\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mi0039\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/49fm7wnf\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.00050 train_loss=1.4430 acc=0.4716 val_loss=1.1864 acc=0.5678\n",
      "[Epoch 002] lr=0.00050 train_loss=1.0143 acc=0.6407 val_loss=0.8704 acc=0.6904\n",
      "[Epoch 003] lr=0.00050 train_loss=0.8696 acc=0.6973 val_loss=0.7665 acc=0.7332\n",
      "[Epoch 004] lr=0.00050 train_loss=0.7800 acc=0.7300 val_loss=0.7308 acc=0.7452\n",
      "[Epoch 005] lr=0.00050 train_loss=0.7115 acc=0.7574 val_loss=0.6654 acc=0.7628\n",
      "[Epoch 006] lr=0.00050 train_loss=0.6596 acc=0.7754 val_loss=0.5881 acc=0.7982\n",
      "[Epoch 007] lr=0.00050 train_loss=0.6157 acc=0.7906 val_loss=0.5717 acc=0.7986\n",
      "[Epoch 008] lr=0.00050 train_loss=0.5743 acc=0.8046 val_loss=0.5217 acc=0.8216\n",
      "[Epoch 009] lr=0.00050 train_loss=0.5438 acc=0.8164 val_loss=0.5255 acc=0.8130\n",
      "[Epoch 010] lr=0.00050 train_loss=0.5166 acc=0.8260 val_loss=0.5103 acc=0.8222\n",
      "[Epoch 011] lr=0.00050 train_loss=0.4859 acc=0.8359 val_loss=0.4966 acc=0.8244\n",
      "[Epoch 012] lr=0.00050 train_loss=0.4624 acc=0.8435 val_loss=0.4708 acc=0.8370\n",
      "[Epoch 013] lr=0.00050 train_loss=0.4379 acc=0.8518 val_loss=0.4551 acc=0.8422\n",
      "[Epoch 014] lr=0.00050 train_loss=0.4241 acc=0.8568 val_loss=0.4588 acc=0.8402\n",
      "[Epoch 015] lr=0.00051 train_loss=0.4059 acc=0.8611 val_loss=0.4584 acc=0.8474\n",
      "[Epoch 016] lr=0.00051 train_loss=0.3920 acc=0.8670 val_loss=0.4178 acc=0.8562\n",
      "[Epoch 017] lr=0.00051 train_loss=0.3709 acc=0.8755 val_loss=0.4060 acc=0.8600\n",
      "[Epoch 018] lr=0.00051 train_loss=0.3549 acc=0.8796 val_loss=0.4100 acc=0.8586\n",
      "[Epoch 019] lr=0.00051 train_loss=0.3401 acc=0.8839 val_loss=0.4040 acc=0.8632\n",
      "[Epoch 020] lr=0.00051 train_loss=0.3274 acc=0.8911 val_loss=0.3764 acc=0.8706\n",
      "FINAL TEST: loss=0.3666  top1_acc=0.8801\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc ▁▄▅▅▆▆▆▇▇▇▇▇▇▇▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▅▄▄▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc ▁▄▅▅▆▆▆▇▇▇▇▇▇▇▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss █▅▄▄▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.8706\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.8801\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.00051\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.89109\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 0.32737\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.8706\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 0.37637\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mi0039\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/49fm7wnf\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251015_044735-49fm7wnf/logs\u001b[0m\n",
      ">>> \n",
      "            python -m vgg6_cifar.scripts.train_experiment                 --data_dir ./data                 --out_dir ./runs/sweeps_ext/actrelu_bnTrue_bs32_e20_lr0.001_m0.9_rmsprop                 --activation relu                 --batch_size 32                 --epochs 20                 --lr 0.001                 --momentum 0.9                 --optimizer rmsprop                 --weight_decay 0.0005                 --label_smoothing 0.0                 --seed 42                 --aug_hflip                 --aug_crop                 --aug_cutout                 --aug_jitter                 --use_bn                 --amp                 --wandb --wandb_project vgg6-cifar10-assignment --wandb_group q2q5-sweeps --run_name i0040\n",
      "            \n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m setting up run wgdridpg (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251015_045105-wgdridpg\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mi0040\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/wgdridpg\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.00050 train_loss=2.0574 acc=0.2592 val_loss=1.7398 acc=0.3520\n",
      "[Epoch 002] lr=0.00050 train_loss=1.5975 acc=0.4145 val_loss=1.5413 acc=0.4522\n",
      "[Epoch 003] lr=0.00050 train_loss=1.4196 acc=0.4878 val_loss=1.8878 acc=0.3922\n",
      "[Epoch 004] lr=0.00050 train_loss=1.3251 acc=0.5267 val_loss=1.2171 acc=0.5610\n",
      "[Epoch 005] lr=0.00050 train_loss=1.2627 acc=0.5528 val_loss=1.9433 acc=0.4244\n",
      "[Epoch 006] lr=0.00050 train_loss=1.2256 acc=0.5692 val_loss=1.3338 acc=0.5382\n",
      "[Epoch 007] lr=0.00050 train_loss=1.1798 acc=0.5856 val_loss=1.2965 acc=0.5734\n",
      "[Epoch 008] lr=0.00050 train_loss=1.1629 acc=0.5912 val_loss=1.0865 acc=0.6148\n",
      "[Epoch 009] lr=0.00050 train_loss=1.1606 acc=0.5961 val_loss=1.0803 acc=0.6174\n",
      "[Epoch 010] lr=0.00050 train_loss=1.1320 acc=0.6090 val_loss=1.1381 acc=0.6096\n",
      "[Epoch 011] lr=0.00050 train_loss=1.1331 acc=0.6090 val_loss=1.5412 acc=0.5264\n",
      "[Epoch 012] lr=0.00050 train_loss=1.1111 acc=0.6154 val_loss=1.0740 acc=0.6224\n",
      "[Epoch 013] lr=0.00050 train_loss=1.1025 acc=0.6181 val_loss=1.1811 acc=0.5972\n",
      "[Epoch 014] lr=0.00050 train_loss=1.1053 acc=0.6195 val_loss=1.2583 acc=0.5976\n",
      "[Epoch 015] lr=0.00051 train_loss=1.0998 acc=0.6262 val_loss=1.0149 acc=0.6398\n",
      "[Epoch 016] lr=0.00051 train_loss=1.0979 acc=0.6240 val_loss=1.6107 acc=0.4960\n",
      "[Epoch 017] lr=0.00051 train_loss=1.0822 acc=0.6291 val_loss=1.1828 acc=0.6042\n",
      "[Epoch 018] lr=0.00051 train_loss=1.0776 acc=0.6314 val_loss=1.1109 acc=0.6218\n",
      "[Epoch 019] lr=0.00051 train_loss=1.0717 acc=0.6339 val_loss=1.0423 acc=0.6362\n",
      "[Epoch 020] lr=0.00051 train_loss=1.0728 acc=0.6331 val_loss=1.1804 acc=0.6098\n",
      "FINAL TEST: loss=0.9664  top1_acc=0.6716\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc ▁▄▅▆▆▇▇▇▇███████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▅▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc ▁▃▂▆▃▆▆▇▇▇▅█▇▇█▅▇██▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss ▆▅█▃█▃▃▂▁▂▅▁▂▃▁▅▂▂▁▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.6398\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.6716\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.00051\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.63313\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 1.07277\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.6098\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 1.18036\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mi0040\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/wgdridpg\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251015_045105-wgdridpg/logs\u001b[0m\n",
      ">>> \n",
      "            python -m vgg6_cifar.scripts.train_experiment                 --data_dir ./data                 --out_dir ./runs/sweeps_ext/actrelu_bnTrue_bs32_e20_lr0.001_m0.9_nadam                 --activation relu                 --batch_size 32                 --epochs 20                 --lr 0.001                 --momentum 0.9                 --optimizer nadam                 --weight_decay 0.0005                 --label_smoothing 0.0                 --seed 42                 --aug_hflip                 --aug_crop                 --aug_cutout                 --aug_jitter                 --use_bn                 --amp                 --wandb --wandb_project vgg6-cifar10-assignment --wandb_group q2q5-sweeps --run_name i0041\n",
      "            \n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251015_045439-azc811di\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mi0041\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/azc811di\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.00050 train_loss=1.3755 acc=0.4980 val_loss=1.0855 acc=0.6052\n",
      "[Epoch 002] lr=0.00050 train_loss=0.9681 acc=0.6613 val_loss=0.9334 acc=0.6748\n",
      "[Epoch 003] lr=0.00050 train_loss=0.8339 acc=0.7112 val_loss=0.8055 acc=0.7074\n",
      "[Epoch 004] lr=0.00050 train_loss=0.7522 acc=0.7444 val_loss=0.7642 acc=0.7324\n",
      "[Epoch 005] lr=0.00050 train_loss=0.6919 acc=0.7637 val_loss=0.6890 acc=0.7662\n",
      "[Epoch 006] lr=0.00050 train_loss=0.6458 acc=0.7812 val_loss=0.6938 acc=0.7632\n",
      "[Epoch 007] lr=0.00050 train_loss=0.6041 acc=0.7960 val_loss=0.5742 acc=0.8040\n",
      "[Epoch 008] lr=0.00050 train_loss=0.5750 acc=0.8056 val_loss=0.5636 acc=0.8050\n",
      "[Epoch 009] lr=0.00050 train_loss=0.5510 acc=0.8136 val_loss=0.6306 acc=0.7902\n",
      "[Epoch 010] lr=0.00050 train_loss=0.5240 acc=0.8239 val_loss=0.5888 acc=0.7966\n",
      "[Epoch 011] lr=0.00050 train_loss=0.5022 acc=0.8323 val_loss=0.5172 acc=0.8226\n",
      "[Epoch 012] lr=0.00050 train_loss=0.4831 acc=0.8366 val_loss=0.4846 acc=0.8358\n",
      "[Epoch 013] lr=0.00050 train_loss=0.4664 acc=0.8426 val_loss=0.5020 acc=0.8264\n",
      "[Epoch 014] lr=0.00050 train_loss=0.4551 acc=0.8484 val_loss=0.5094 acc=0.8284\n",
      "[Epoch 015] lr=0.00051 train_loss=0.4429 acc=0.8502 val_loss=0.4859 acc=0.8318\n",
      "[Epoch 016] lr=0.00051 train_loss=0.4336 acc=0.8543 val_loss=0.4767 acc=0.8350\n",
      "[Epoch 017] lr=0.00051 train_loss=0.4191 acc=0.8588 val_loss=0.4309 acc=0.8528\n",
      "[Epoch 018] lr=0.00051 train_loss=0.4075 acc=0.8633 val_loss=0.4048 acc=0.8642\n",
      "[Epoch 019] lr=0.00051 train_loss=0.4012 acc=0.8658 val_loss=0.5093 acc=0.8286\n",
      "[Epoch 020] lr=0.00051 train_loss=0.3938 acc=0.8680 val_loss=0.4774 acc=0.8430\n",
      "FINAL TEST: loss=0.3746  top1_acc=0.8769\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣟\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc ▁▄▅▆▆▆▇▇▇▇▇▇████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▅▄▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc ▁▃▄▄▅▅▆▆▆▆▇▇▇▇▇▇██▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss █▆▅▅▄▄▃▃▃▃▂▂▂▂▂▂▁▁▂▂\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.8642\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.8769\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.00051\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.86798\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 0.39379\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.843\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 0.47742\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mi0041\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/azc811di\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251015_045439-azc811di/logs\u001b[0m\n",
      ">>> \n",
      "            python -m vgg6_cifar.scripts.train_experiment                 --data_dir ./data                 --out_dir ./runs/sweeps_ext/actrelu_bnTrue_bs32_e20_lr0.001_m0.9_adagrad                 --activation relu                 --batch_size 32                 --epochs 20                 --lr 0.001                 --momentum 0.9                 --optimizer adagrad                 --weight_decay 0.0005                 --label_smoothing 0.0                 --seed 42                 --aug_hflip                 --aug_crop                 --aug_cutout                 --aug_jitter                 --use_bn                 --amp                 --wandb --wandb_project vgg6-cifar10-assignment --wandb_group q2q5-sweeps --run_name i0042\n",
      "            \n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251015_045810-s804vk4x\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mi0042\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/s804vk4x\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.00050 train_loss=1.6676 acc=0.3832 val_loss=1.4626 acc=0.4634\n",
      "[Epoch 002] lr=0.00050 train_loss=1.4512 acc=0.4678 val_loss=1.3493 acc=0.5116\n",
      "[Epoch 003] lr=0.00050 train_loss=1.3650 acc=0.5018 val_loss=1.2801 acc=0.5382\n",
      "[Epoch 004] lr=0.00050 train_loss=1.3048 acc=0.5248 val_loss=1.2352 acc=0.5592\n",
      "[Epoch 005] lr=0.00050 train_loss=1.2608 acc=0.5465 val_loss=1.1933 acc=0.5718\n",
      "[Epoch 006] lr=0.00050 train_loss=1.2253 acc=0.5574 val_loss=1.1725 acc=0.5898\n",
      "[Epoch 007] lr=0.00050 train_loss=1.1958 acc=0.5708 val_loss=1.1294 acc=0.5970\n",
      "[Epoch 008] lr=0.00050 train_loss=1.1750 acc=0.5788 val_loss=1.1179 acc=0.6100\n",
      "[Epoch 009] lr=0.00050 train_loss=1.1579 acc=0.5848 val_loss=1.0970 acc=0.6064\n",
      "[Epoch 010] lr=0.00050 train_loss=1.1370 acc=0.5921 val_loss=1.0736 acc=0.6156\n",
      "[Epoch 011] lr=0.00050 train_loss=1.1196 acc=0.6003 val_loss=1.0678 acc=0.6168\n",
      "[Epoch 012] lr=0.00050 train_loss=1.1084 acc=0.6021 val_loss=1.0369 acc=0.6268\n",
      "[Epoch 013] lr=0.00050 train_loss=1.0897 acc=0.6106 val_loss=1.0284 acc=0.6346\n",
      "[Epoch 014] lr=0.00050 train_loss=1.0804 acc=0.6167 val_loss=1.0343 acc=0.6290\n",
      "[Epoch 015] lr=0.00051 train_loss=1.0729 acc=0.6176 val_loss=1.0198 acc=0.6374\n",
      "[Epoch 016] lr=0.00051 train_loss=1.0586 acc=0.6244 val_loss=0.9965 acc=0.6506\n",
      "[Epoch 017] lr=0.00051 train_loss=1.0494 acc=0.6283 val_loss=0.9918 acc=0.6554\n",
      "[Epoch 018] lr=0.00051 train_loss=1.0392 acc=0.6310 val_loss=0.9821 acc=0.6556\n",
      "[Epoch 019] lr=0.00051 train_loss=1.0344 acc=0.6321 val_loss=0.9817 acc=0.6602\n",
      "[Epoch 020] lr=0.00051 train_loss=1.0245 acc=0.6347 val_loss=0.9754 acc=0.6584\n",
      "FINAL TEST: loss=0.8978  top1_acc=0.6776\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc ▁▃▄▅▆▆▆▆▇▇▇▇▇▇██████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▆▅▄▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc ▁▃▄▄▅▅▆▆▆▆▆▇▇▇▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss █▆▅▅▄▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.6602\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.6776\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.00051\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.63471\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 1.02446\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.6584\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 0.97541\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mi0042\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/s804vk4x\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251015_045810-s804vk4x/logs\u001b[0m\n",
      ">>> \n",
      "            python -m vgg6_cifar.scripts.train_experiment                 --data_dir ./data                 --out_dir ./runs/sweeps_ext/actrelu_bnTrue_bs32_e60_lr0.1_m0.1_sgd                 --activation relu                 --batch_size 32                 --epochs 60                 --lr 0.1                 --momentum 0.1                 --optimizer sgd                 --weight_decay 0.0005                 --label_smoothing 0.0                 --seed 42                 --aug_hflip                 --aug_crop                 --aug_cutout                 --aug_jitter                 --use_bn                 --amp                 --wandb --wandb_project vgg6-cifar10-assignment --wandb_group q2q5-sweeps --run_name i0043\n",
      "            \n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m setting up run w7ugouqb (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m setting up run w7ugouqb (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251015_050137-w7ugouqb\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mi0043\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/w7ugouqb\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.02001 train_loss=1.6322 acc=0.3900 val_loss=1.4241 acc=0.4696\n",
      "[Epoch 002] lr=0.02003 train_loss=1.2499 acc=0.5471 val_loss=1.1871 acc=0.5768\n",
      "[Epoch 003] lr=0.02004 train_loss=1.0510 acc=0.6245 val_loss=1.0404 acc=0.6140\n",
      "[Epoch 004] lr=0.02006 train_loss=0.9348 acc=0.6708 val_loss=0.9609 acc=0.6454\n",
      "[Epoch 005] lr=0.02007 train_loss=0.8554 acc=0.6993 val_loss=0.7567 acc=0.7326\n",
      "[Epoch 006] lr=0.02009 train_loss=0.7926 acc=0.7238 val_loss=0.8655 acc=0.6922\n",
      "[Epoch 007] lr=0.02010 train_loss=0.7406 acc=0.7425 val_loss=0.7801 acc=0.7220\n",
      "[Epoch 008] lr=0.02011 train_loss=0.7019 acc=0.7568 val_loss=0.7787 acc=0.7310\n",
      "[Epoch 009] lr=0.02013 train_loss=0.6631 acc=0.7714 val_loss=0.6254 acc=0.7762\n",
      "[Epoch 010] lr=0.02014 train_loss=0.6363 acc=0.7816 val_loss=0.5925 acc=0.7914\n",
      "[Epoch 011] lr=0.02016 train_loss=0.6037 acc=0.7938 val_loss=0.8103 acc=0.7338\n",
      "[Epoch 012] lr=0.02017 train_loss=0.5791 acc=0.8008 val_loss=0.5557 acc=0.8026\n",
      "[Epoch 013] lr=0.02018 train_loss=0.5525 acc=0.8105 val_loss=0.6861 acc=0.7678\n",
      "[Epoch 014] lr=0.02020 train_loss=0.5350 acc=0.8147 val_loss=0.5476 acc=0.8088\n",
      "[Epoch 015] lr=0.02021 train_loss=0.5176 acc=0.8218 val_loss=0.5570 acc=0.8108\n",
      "[Epoch 016] lr=0.02023 train_loss=0.4978 acc=0.8274 val_loss=0.5513 acc=0.8098\n",
      "[Epoch 017] lr=0.02024 train_loss=0.4806 acc=0.8355 val_loss=0.5350 acc=0.8152\n",
      "[Epoch 018] lr=0.02026 train_loss=0.4632 acc=0.8400 val_loss=0.5599 acc=0.8040\n",
      "[Epoch 019] lr=0.02027 train_loss=0.4481 acc=0.8476 val_loss=0.5352 acc=0.8110\n",
      "[Epoch 020] lr=0.02028 train_loss=0.4290 acc=0.8529 val_loss=0.4950 acc=0.8284\n",
      "[Epoch 021] lr=0.02030 train_loss=0.4201 acc=0.8562 val_loss=0.7350 acc=0.7478\n",
      "[Epoch 022] lr=0.02031 train_loss=0.4116 acc=0.8577 val_loss=0.4294 acc=0.8516\n",
      "[Epoch 023] lr=0.02033 train_loss=0.3960 acc=0.8651 val_loss=0.5681 acc=0.8070\n",
      "[Epoch 024] lr=0.02034 train_loss=0.3857 acc=0.8672 val_loss=0.4343 acc=0.8510\n",
      "[Epoch 025] lr=0.02036 train_loss=0.3791 acc=0.8686 val_loss=0.4983 acc=0.8284\n",
      "[Epoch 026] lr=0.02037 train_loss=0.3640 acc=0.8747 val_loss=0.4307 acc=0.8460\n",
      "[Epoch 027] lr=0.02038 train_loss=0.3538 acc=0.8770 val_loss=0.4292 acc=0.8498\n",
      "[Epoch 028] lr=0.02040 train_loss=0.3450 acc=0.8784 val_loss=0.4810 acc=0.8356\n",
      "[Epoch 029] lr=0.02041 train_loss=0.3346 acc=0.8846 val_loss=0.3810 acc=0.8656\n",
      "[Epoch 030] lr=0.02043 train_loss=0.3296 acc=0.8868 val_loss=0.6293 acc=0.8054\n",
      "[Epoch 031] lr=0.02044 train_loss=0.3158 acc=0.8892 val_loss=0.3848 acc=0.8648\n",
      "[Epoch 032] lr=0.02045 train_loss=0.3111 acc=0.8935 val_loss=0.3978 acc=0.8624\n",
      "[Epoch 033] lr=0.02047 train_loss=0.3026 acc=0.8950 val_loss=0.4126 acc=0.8646\n",
      "[Epoch 034] lr=0.02048 train_loss=0.2992 acc=0.8992 val_loss=0.6488 acc=0.8006\n",
      "[Epoch 035] lr=0.02050 train_loss=0.2949 acc=0.8982 val_loss=0.3945 acc=0.8650\n",
      "[Epoch 036] lr=0.02051 train_loss=0.2825 acc=0.9011 val_loss=0.3656 acc=0.8732\n",
      "[Epoch 037] lr=0.02053 train_loss=0.2758 acc=0.9051 val_loss=0.3518 acc=0.8786\n",
      "[Epoch 038] lr=0.02054 train_loss=0.2735 acc=0.9060 val_loss=0.4049 acc=0.8572\n",
      "[Epoch 039] lr=0.02055 train_loss=0.2662 acc=0.9079 val_loss=0.3818 acc=0.8704\n",
      "[Epoch 040] lr=0.02057 train_loss=0.2560 acc=0.9129 val_loss=0.4451 acc=0.8478\n",
      "[Epoch 041] lr=0.02058 train_loss=0.2539 acc=0.9136 val_loss=0.3791 acc=0.8718\n",
      "[Epoch 042] lr=0.02060 train_loss=0.2495 acc=0.9132 val_loss=0.3560 acc=0.8802\n",
      "[Epoch 043] lr=0.02061 train_loss=0.2430 acc=0.9166 val_loss=0.4016 acc=0.8662\n",
      "[Epoch 044] lr=0.02063 train_loss=0.2422 acc=0.9142 val_loss=0.3453 acc=0.8834\n",
      "[Epoch 045] lr=0.02064 train_loss=0.2348 acc=0.9189 val_loss=0.3884 acc=0.8678\n",
      "[Epoch 046] lr=0.02065 train_loss=0.2290 acc=0.9217 val_loss=0.3964 acc=0.8652\n",
      "[Epoch 047] lr=0.02067 train_loss=0.2269 acc=0.9210 val_loss=0.3404 acc=0.8878\n",
      "[Epoch 048] lr=0.02068 train_loss=0.2243 acc=0.9231 val_loss=0.3803 acc=0.8736\n",
      "[Epoch 049] lr=0.02070 train_loss=0.2164 acc=0.9256 val_loss=0.3906 acc=0.8724\n",
      "[Epoch 050] lr=0.02071 train_loss=0.2103 acc=0.9273 val_loss=0.3372 acc=0.8836\n",
      "[Epoch 051] lr=0.02072 train_loss=0.2130 acc=0.9252 val_loss=0.5566 acc=0.8310\n",
      "[Epoch 052] lr=0.02074 train_loss=0.2090 acc=0.9275 val_loss=0.3952 acc=0.8700\n",
      "[Epoch 053] lr=0.02075 train_loss=0.2086 acc=0.9275 val_loss=0.4204 acc=0.8582\n",
      "[Epoch 054] lr=0.02077 train_loss=0.2066 acc=0.9291 val_loss=0.3586 acc=0.8804\n",
      "[Epoch 055] lr=0.02078 train_loss=0.2057 acc=0.9284 val_loss=0.3287 acc=0.8896\n",
      "[Epoch 056] lr=0.02080 train_loss=0.1996 acc=0.9298 val_loss=0.3271 acc=0.8910\n",
      "[Epoch 057] lr=0.02081 train_loss=0.1929 acc=0.9340 val_loss=0.3714 acc=0.8742\n",
      "[Epoch 058] lr=0.02082 train_loss=0.1879 acc=0.9344 val_loss=0.3436 acc=0.8874\n",
      "[Epoch 059] lr=0.02084 train_loss=0.1884 acc=0.9341 val_loss=0.3280 acc=0.8866\n",
      "[Epoch 060] lr=0.02085 train_loss=0.1909 acc=0.9339 val_loss=0.3571 acc=0.8820\n",
      "FINAL TEST: loss=0.3092  top1_acc=0.9014\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc ▁▃▄▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇█████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc ▁▃▃▄▅▅▅▆▆▅▇▇▇▇▇▇▇▇▇▇▇█▇██▆██▇███████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss █▆▅▄▄▄▃▃▄▂▂▂▂▂▂▂▂▂▂▂▃▁▂▃▁▁▂▁▁▁▁▁▁▁▂▂▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.891\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 60\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.9014\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.02085\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.93389\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 0.19088\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.882\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 0.35713\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mi0043\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/w7ugouqb\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251015_050137-w7ugouqb/logs\u001b[0m\n",
      ">>> \n",
      "            python -m vgg6_cifar.scripts.train_experiment                 --data_dir ./data                 --out_dir ./runs/sweeps_ext/actrelu_bnTrue_bs32_e60_lr0.1_m0.1_nesterov-sgd                 --activation relu                 --batch_size 32                 --epochs 60                 --lr 0.1                 --momentum 0.1                 --optimizer nesterov-sgd                 --weight_decay 0.0005                 --label_smoothing 0.0                 --seed 42                 --aug_hflip                 --aug_crop                 --aug_cutout                 --aug_jitter                 --use_bn                 --amp                 --wandb --wandb_project vgg6-cifar10-assignment --wandb_group q2q5-sweeps --run_name i0044\n",
      "            \n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m setting up run uzzflqb4 (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251015_051142-uzzflqb4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mi0044\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/uzzflqb4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.02001 train_loss=1.6364 acc=0.3909 val_loss=1.4847 acc=0.4550\n",
      "[Epoch 002] lr=0.02003 train_loss=1.2596 acc=0.5430 val_loss=1.3983 acc=0.5016\n",
      "[Epoch 003] lr=0.02004 train_loss=1.0580 acc=0.6235 val_loss=1.0889 acc=0.5924\n",
      "[Epoch 004] lr=0.02006 train_loss=0.9416 acc=0.6694 val_loss=0.9381 acc=0.6564\n",
      "[Epoch 005] lr=0.02007 train_loss=0.8608 acc=0.6996 val_loss=1.0486 acc=0.6326\n",
      "[Epoch 006] lr=0.02009 train_loss=0.7993 acc=0.7207 val_loss=1.0629 acc=0.6398\n",
      "[Epoch 007] lr=0.02010 train_loss=0.7415 acc=0.7412 val_loss=0.7121 acc=0.7542\n",
      "[Epoch 008] lr=0.02011 train_loss=0.6999 acc=0.7575 val_loss=0.8533 acc=0.7154\n",
      "[Epoch 009] lr=0.02013 train_loss=0.6636 acc=0.7687 val_loss=0.7253 acc=0.7426\n",
      "[Epoch 010] lr=0.02014 train_loss=0.6374 acc=0.7784 val_loss=0.6481 acc=0.7676\n",
      "[Epoch 011] lr=0.02016 train_loss=0.6011 acc=0.7927 val_loss=1.6523 acc=0.5686\n",
      "[Epoch 012] lr=0.02017 train_loss=0.5796 acc=0.8008 val_loss=0.7265 acc=0.7590\n",
      "[Epoch 013] lr=0.02018 train_loss=0.5512 acc=0.8116 val_loss=0.6574 acc=0.7804\n",
      "[Epoch 014] lr=0.02020 train_loss=0.5346 acc=0.8159 val_loss=0.5650 acc=0.7952\n",
      "[Epoch 015] lr=0.02021 train_loss=0.5183 acc=0.8222 val_loss=0.6868 acc=0.7710\n",
      "[Epoch 016] lr=0.02023 train_loss=0.4958 acc=0.8273 val_loss=1.0303 acc=0.6708\n",
      "[Epoch 017] lr=0.02024 train_loss=0.4794 acc=0.8377 val_loss=0.6808 acc=0.7636\n",
      "[Epoch 018] lr=0.02026 train_loss=0.4613 acc=0.8414 val_loss=0.5212 acc=0.8206\n",
      "[Epoch 019] lr=0.02027 train_loss=0.4466 acc=0.8475 val_loss=0.5204 acc=0.8234\n",
      "[Epoch 020] lr=0.02028 train_loss=0.4326 acc=0.8532 val_loss=0.4732 acc=0.8350\n",
      "[Epoch 021] lr=0.02030 train_loss=0.4211 acc=0.8559 val_loss=0.7160 acc=0.7642\n",
      "[Epoch 022] lr=0.02031 train_loss=0.4066 acc=0.8610 val_loss=0.4340 acc=0.8418\n",
      "[Epoch 023] lr=0.02033 train_loss=0.3981 acc=0.8640 val_loss=0.6627 acc=0.7804\n",
      "[Epoch 024] lr=0.02034 train_loss=0.3838 acc=0.8684 val_loss=0.4331 acc=0.8528\n",
      "[Epoch 025] lr=0.02036 train_loss=0.3756 acc=0.8712 val_loss=0.5070 acc=0.8270\n",
      "[Epoch 026] lr=0.02037 train_loss=0.3605 acc=0.8770 val_loss=0.4147 acc=0.8548\n",
      "[Epoch 027] lr=0.02038 train_loss=0.3498 acc=0.8803 val_loss=0.4115 acc=0.8538\n",
      "[Epoch 028] lr=0.02040 train_loss=0.3453 acc=0.8802 val_loss=1.0366 acc=0.6954\n",
      "[Epoch 029] lr=0.02041 train_loss=0.3334 acc=0.8850 val_loss=0.4888 acc=0.8392\n",
      "[Epoch 030] lr=0.02043 train_loss=0.3287 acc=0.8870 val_loss=0.6036 acc=0.8068\n",
      "[Epoch 031] lr=0.02044 train_loss=0.3177 acc=0.8904 val_loss=0.3848 acc=0.8664\n",
      "[Epoch 032] lr=0.02045 train_loss=0.3092 acc=0.8914 val_loss=0.4849 acc=0.8344\n",
      "[Epoch 033] lr=0.02047 train_loss=0.3048 acc=0.8953 val_loss=0.4697 acc=0.8444\n",
      "[Epoch 034] lr=0.02048 train_loss=0.2958 acc=0.8980 val_loss=0.3772 acc=0.8712\n",
      "[Epoch 035] lr=0.02050 train_loss=0.2913 acc=0.9006 val_loss=0.3629 acc=0.8764\n",
      "[Epoch 036] lr=0.02051 train_loss=0.2829 acc=0.9022 val_loss=0.4077 acc=0.8604\n",
      "[Epoch 037] lr=0.02053 train_loss=0.2715 acc=0.9051 val_loss=0.4514 acc=0.8536\n",
      "[Epoch 038] lr=0.02054 train_loss=0.2734 acc=0.9070 val_loss=0.6660 acc=0.7882\n",
      "[Epoch 039] lr=0.02055 train_loss=0.2646 acc=0.9082 val_loss=0.4141 acc=0.8628\n",
      "[Epoch 040] lr=0.02057 train_loss=0.2587 acc=0.9116 val_loss=0.4111 acc=0.8552\n",
      "[Epoch 041] lr=0.02058 train_loss=0.2508 acc=0.9135 val_loss=0.3830 acc=0.8704\n",
      "[Epoch 042] lr=0.02060 train_loss=0.2527 acc=0.9136 val_loss=0.4093 acc=0.8604\n",
      "[Epoch 043] lr=0.02061 train_loss=0.2436 acc=0.9155 val_loss=0.4082 acc=0.8616\n",
      "[Epoch 044] lr=0.02063 train_loss=0.2376 acc=0.9177 val_loss=0.4102 acc=0.8624\n",
      "[Epoch 045] lr=0.02064 train_loss=0.2322 acc=0.9196 val_loss=0.4000 acc=0.8652\n",
      "[Epoch 046] lr=0.02065 train_loss=0.2282 acc=0.9215 val_loss=0.4053 acc=0.8648\n",
      "[Epoch 047] lr=0.02067 train_loss=0.2302 acc=0.9216 val_loss=0.3719 acc=0.8766\n",
      "[Epoch 048] lr=0.02068 train_loss=0.2201 acc=0.9243 val_loss=0.3621 acc=0.8756\n",
      "[Epoch 049] lr=0.02070 train_loss=0.2206 acc=0.9241 val_loss=0.3920 acc=0.8712\n",
      "[Epoch 050] lr=0.02071 train_loss=0.2135 acc=0.9260 val_loss=0.3485 acc=0.8824\n",
      "[Epoch 051] lr=0.02072 train_loss=0.2110 acc=0.9269 val_loss=0.3736 acc=0.8714\n",
      "[Epoch 052] lr=0.02074 train_loss=0.2071 acc=0.9286 val_loss=0.4112 acc=0.8686\n",
      "[Epoch 053] lr=0.02075 train_loss=0.2050 acc=0.9299 val_loss=0.3654 acc=0.8770\n",
      "[Epoch 054] lr=0.02077 train_loss=0.2017 acc=0.9309 val_loss=0.3494 acc=0.8800\n",
      "[Epoch 055] lr=0.02078 train_loss=0.1989 acc=0.9317 val_loss=0.3243 acc=0.8882\n",
      "[Epoch 056] lr=0.02080 train_loss=0.1924 acc=0.9337 val_loss=0.3285 acc=0.8876\n",
      "[Epoch 057] lr=0.02081 train_loss=0.1961 acc=0.9338 val_loss=0.3573 acc=0.8786\n",
      "[Epoch 058] lr=0.02082 train_loss=0.1854 acc=0.9373 val_loss=0.4285 acc=0.8672\n",
      "[Epoch 059] lr=0.02084 train_loss=0.1880 acc=0.9338 val_loss=0.3643 acc=0.8756\n",
      "[Epoch 060] lr=0.02085 train_loss=0.1868 acc=0.9353 val_loss=0.3699 acc=0.8750\n",
      "FINAL TEST: loss=0.3253  top1_acc=0.8976\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc ▁▃▄▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▅▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc ▁▃▄▃▄▅▅▆▂▆▆▆▇▆▇▇▇▇▅▇█▇▇▇▆█▇█████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss ▇▄▅▅▃▃▃█▃▃▃▅▃▂▂▃▂▃▂▂▅▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.8882\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 60\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.8976\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.02085\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.93531\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 0.18682\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.875\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 0.3699\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mi0044\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/uzzflqb4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251015_051142-uzzflqb4/logs\u001b[0m\n",
      ">>> \n",
      "            python -m vgg6_cifar.scripts.train_experiment                 --data_dir ./data                 --out_dir ./runs/sweeps_ext/actrelu_bnTrue_bs32_e60_lr0.1_m0.1_adam                 --activation relu                 --batch_size 32                 --epochs 60                 --lr 0.1                 --momentum 0.1                 --optimizer adam                 --weight_decay 0.0005                 --label_smoothing 0.0                 --seed 42                 --aug_hflip                 --aug_crop                 --aug_cutout                 --aug_jitter                 --use_bn                 --amp                 --wandb --wandb_project vgg6-cifar10-assignment --wandb_group q2q5-sweeps --run_name i0045\n",
      "            \n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m setting up run 8f7ufk9d (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251015_052151-8f7ufk9d\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mi0045\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/8f7ufk9d\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.02001 train_loss=2.8835 acc=0.1392 val_loss=2.1312 acc=0.1762\n",
      "[Epoch 002] lr=0.02003 train_loss=2.0860 acc=0.1838 val_loss=1.9827 acc=0.1888\n",
      "[Epoch 003] lr=0.02004 train_loss=2.0476 acc=0.1853 val_loss=2.1754 acc=0.1720\n",
      "[Epoch 004] lr=0.02006 train_loss=2.0313 acc=0.1876 val_loss=2.0268 acc=0.1962\n",
      "[Epoch 005] lr=0.02007 train_loss=2.0273 acc=0.1831 val_loss=1.9661 acc=0.2050\n",
      "[Epoch 006] lr=0.02009 train_loss=2.0193 acc=0.1879 val_loss=1.9524 acc=0.2040\n",
      "[Epoch 007] lr=0.02010 train_loss=2.0155 acc=0.1900 val_loss=2.1655 acc=0.1670\n",
      "[Epoch 008] lr=0.02011 train_loss=2.0149 acc=0.1895 val_loss=1.9699 acc=0.1932\n",
      "[Epoch 009] lr=0.02013 train_loss=2.0192 acc=0.1931 val_loss=2.0797 acc=0.1870\n",
      "[Epoch 010] lr=0.02014 train_loss=2.0108 acc=0.1929 val_loss=2.0624 acc=0.1994\n",
      "[Epoch 011] lr=0.02016 train_loss=2.0092 acc=0.1936 val_loss=2.0093 acc=0.2214\n",
      "[Epoch 012] lr=0.02017 train_loss=2.0079 acc=0.1931 val_loss=2.0113 acc=0.2106\n",
      "[Epoch 013] lr=0.02018 train_loss=2.0041 acc=0.1927 val_loss=1.9330 acc=0.2032\n",
      "[Epoch 014] lr=0.02020 train_loss=2.0167 acc=0.1935 val_loss=2.0548 acc=0.1636\n",
      "[Epoch 015] lr=0.02021 train_loss=2.0179 acc=0.1898 val_loss=1.9844 acc=0.2002\n",
      "[Epoch 016] lr=0.02023 train_loss=2.0107 acc=0.1898 val_loss=1.9429 acc=0.1882\n",
      "[Epoch 017] lr=0.02024 train_loss=2.0145 acc=0.1898 val_loss=1.9752 acc=0.1956\n",
      "[Epoch 018] lr=0.02026 train_loss=2.0087 acc=0.1922 val_loss=1.9275 acc=0.2082\n",
      "[Epoch 019] lr=0.02027 train_loss=2.0116 acc=0.1933 val_loss=2.0812 acc=0.2062\n",
      "[Epoch 020] lr=0.02028 train_loss=2.0147 acc=0.1932 val_loss=2.0592 acc=0.2074\n",
      "[Epoch 021] lr=0.02030 train_loss=2.0185 acc=0.1895 val_loss=1.9470 acc=0.2252\n",
      "[Epoch 022] lr=0.02031 train_loss=2.0111 acc=0.1946 val_loss=2.0148 acc=0.2120\n",
      "[Epoch 023] lr=0.02033 train_loss=2.0144 acc=0.1939 val_loss=1.9549 acc=0.2088\n",
      "[Epoch 024] lr=0.02034 train_loss=2.0159 acc=0.1916 val_loss=1.9538 acc=0.2000\n",
      "[Epoch 025] lr=0.02036 train_loss=2.0144 acc=0.1876 val_loss=2.0153 acc=0.2020\n",
      "[Epoch 026] lr=0.02037 train_loss=2.0105 acc=0.1923 val_loss=2.0176 acc=0.1842\n",
      "[Epoch 027] lr=0.02038 train_loss=2.0121 acc=0.1928 val_loss=1.9437 acc=0.2050\n",
      "[Epoch 028] lr=0.02040 train_loss=2.0119 acc=0.1974 val_loss=1.9572 acc=0.1958\n",
      "[Epoch 029] lr=0.02041 train_loss=2.0106 acc=0.1932 val_loss=2.1894 acc=0.1554\n",
      "[Epoch 030] lr=0.02043 train_loss=2.0074 acc=0.1918 val_loss=2.1827 acc=0.1468\n",
      "[Epoch 031] lr=0.02044 train_loss=2.0157 acc=0.1893 val_loss=2.1067 acc=0.1708\n",
      "[Epoch 032] lr=0.02045 train_loss=2.0141 acc=0.1901 val_loss=1.9454 acc=0.2028\n",
      "[Epoch 033] lr=0.02047 train_loss=2.0099 acc=0.1903 val_loss=2.1159 acc=0.1740\n",
      "[Epoch 034] lr=0.02048 train_loss=2.0176 acc=0.1872 val_loss=1.9471 acc=0.2092\n",
      "[Epoch 035] lr=0.02050 train_loss=2.0101 acc=0.1920 val_loss=2.1350 acc=0.1842\n",
      "[Epoch 036] lr=0.02051 train_loss=2.0158 acc=0.1895 val_loss=1.9538 acc=0.2222\n",
      "[Epoch 037] lr=0.02053 train_loss=2.0171 acc=0.1879 val_loss=2.0772 acc=0.1654\n",
      "[Epoch 038] lr=0.02054 train_loss=2.0124 acc=0.1910 val_loss=1.9652 acc=0.1924\n",
      "[Epoch 039] lr=0.02055 train_loss=2.0169 acc=0.1922 val_loss=2.0113 acc=0.2090\n",
      "[Epoch 040] lr=0.02057 train_loss=2.0112 acc=0.1940 val_loss=2.0788 acc=0.1790\n",
      "[Epoch 041] lr=0.02058 train_loss=2.0119 acc=0.1916 val_loss=2.0030 acc=0.1906\n",
      "[Epoch 042] lr=0.02060 train_loss=2.0086 acc=0.1932 val_loss=1.9596 acc=0.2114\n",
      "[Epoch 043] lr=0.02061 train_loss=2.0079 acc=0.1953 val_loss=1.9545 acc=0.2134\n",
      "[Epoch 044] lr=0.02063 train_loss=2.0122 acc=0.1890 val_loss=1.9596 acc=0.2044\n",
      "[Epoch 045] lr=0.02064 train_loss=2.0088 acc=0.1923 val_loss=1.9704 acc=0.1920\n",
      "[Epoch 046] lr=0.02065 train_loss=2.0102 acc=0.1926 val_loss=2.0623 acc=0.1898\n",
      "[Epoch 047] lr=0.02067 train_loss=2.0069 acc=0.1935 val_loss=1.9581 acc=0.2022\n",
      "[Epoch 048] lr=0.02068 train_loss=2.0136 acc=0.1950 val_loss=1.9678 acc=0.1946\n",
      "[Epoch 049] lr=0.02070 train_loss=2.0154 acc=0.1886 val_loss=2.0047 acc=0.1748\n",
      "[Epoch 050] lr=0.02071 train_loss=2.0107 acc=0.1884 val_loss=2.0701 acc=0.1660\n",
      "[Epoch 051] lr=0.02072 train_loss=2.0157 acc=0.1916 val_loss=1.9477 acc=0.2316\n",
      "[Epoch 052] lr=0.02074 train_loss=2.0080 acc=0.1952 val_loss=1.9395 acc=0.2174\n",
      "[Epoch 053] lr=0.02075 train_loss=2.0086 acc=0.1896 val_loss=2.1488 acc=0.1740\n",
      "[Epoch 054] lr=0.02077 train_loss=2.0116 acc=0.1911 val_loss=1.9792 acc=0.2114\n",
      "[Epoch 055] lr=0.02078 train_loss=2.0134 acc=0.1931 val_loss=1.9861 acc=0.2050\n",
      "[Epoch 056] lr=0.02080 train_loss=2.0139 acc=0.1902 val_loss=2.0026 acc=0.2066\n",
      "[Epoch 057] lr=0.02081 train_loss=2.0144 acc=0.1906 val_loss=1.9642 acc=0.2092\n",
      "[Epoch 058] lr=0.02082 train_loss=2.0113 acc=0.1914 val_loss=1.9580 acc=0.2218\n",
      "[Epoch 059] lr=0.02084 train_loss=2.0117 acc=0.1921 val_loss=1.9884 acc=0.1902\n",
      "[Epoch 060] lr=0.02085 train_loss=2.0096 acc=0.1928 val_loss=1.9981 acc=0.2018\n",
      "FINAL TEST: loss=1.9311  top1_acc=0.2354\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc ▁▆▇▆▇▇▇█▇█▇▇█▇▇█▇▇▇█▇▇▇▇▇▇▇▇▇█▇█▇█▇▇▇▇▇▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc ▄▃▅▆▅▄▄▅▇▅▄▆▆▇▆▅▅▄▅▁▂▅▃▄▇▆▃▆▆▄▅▃▂█▇▆▆▆▇▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss ▆█▂▂▅▄▃▁▂▁▅▃▂▂▃▁▂██▆▂▇▂▅▂▅▃▂▂▂▂▂▃▂▁▂▃▃▂▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.2316\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 60\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.2354\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.02085\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.19276\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 2.00957\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.2018\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 1.99811\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mi0045\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/8f7ufk9d\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251015_052151-8f7ufk9d/logs\u001b[0m\n",
      ">>> \n",
      "            python -m vgg6_cifar.scripts.train_experiment                 --data_dir ./data                 --out_dir ./runs/sweeps_ext/actrelu_bnTrue_bs32_e60_lr0.1_m0.1_adamw                 --activation relu                 --batch_size 32                 --epochs 60                 --lr 0.1                 --momentum 0.1                 --optimizer adamw                 --weight_decay 0.0005                 --label_smoothing 0.0                 --seed 42                 --aug_hflip                 --aug_crop                 --aug_cutout                 --aug_jitter                 --use_bn                 --amp                 --wandb --wandb_project vgg6-cifar10-assignment --wandb_group q2q5-sweeps --run_name i0046\n",
      "            \n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m setting up run xh7nel90 (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251015_053158-xh7nel90\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mi0046\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/xh7nel90\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.02001 train_loss=2.9324 acc=0.0990 val_loss=2.3064 acc=0.0942\n",
      "[Epoch 002] lr=0.02003 train_loss=2.3051 acc=0.0994 val_loss=2.3086 acc=0.1024\n",
      "[Epoch 003] lr=0.02004 train_loss=2.3056 acc=0.0993 val_loss=2.3035 acc=0.0976\n",
      "[Epoch 004] lr=0.02006 train_loss=2.3052 acc=0.1011 val_loss=2.3065 acc=0.0942\n",
      "[Epoch 005] lr=0.02007 train_loss=2.3051 acc=0.1000 val_loss=2.3062 acc=0.1064\n",
      "[Epoch 006] lr=0.02009 train_loss=2.3056 acc=0.0996 val_loss=2.3043 acc=0.1024\n",
      "[Epoch 007] lr=0.02010 train_loss=2.3052 acc=0.0970 val_loss=2.3076 acc=0.1000\n",
      "[Epoch 008] lr=0.02011 train_loss=2.3052 acc=0.1002 val_loss=2.3054 acc=0.0976\n",
      "[Epoch 009] lr=0.02013 train_loss=2.3053 acc=0.1002 val_loss=2.3097 acc=0.1002\n",
      "[Epoch 010] lr=0.02014 train_loss=2.3318 acc=0.0976 val_loss=2.3041 acc=0.1014\n",
      "[Epoch 011] lr=0.02016 train_loss=2.3054 acc=0.1001 val_loss=2.3068 acc=0.0976\n",
      "[Epoch 012] lr=0.02017 train_loss=2.3052 acc=0.0987 val_loss=2.3054 acc=0.0942\n",
      "[Epoch 013] lr=0.02018 train_loss=2.3055 acc=0.0972 val_loss=2.3089 acc=0.0942\n",
      "[Epoch 014] lr=0.02020 train_loss=2.3054 acc=0.1013 val_loss=2.3075 acc=0.0942\n",
      "[Epoch 015] lr=0.02021 train_loss=2.3056 acc=0.1002 val_loss=2.3044 acc=0.1008\n",
      "[Epoch 016] lr=0.02023 train_loss=2.3054 acc=0.0988 val_loss=2.3050 acc=0.1002\n",
      "[Epoch 017] lr=0.02024 train_loss=2.3054 acc=0.1004 val_loss=2.3043 acc=0.1064\n",
      "[Epoch 018] lr=0.02026 train_loss=2.3050 acc=0.0998 val_loss=2.3055 acc=0.0942\n",
      "[Epoch 019] lr=0.02027 train_loss=2.3052 acc=0.1004 val_loss=2.3047 acc=0.1002\n",
      "[Epoch 020] lr=0.02028 train_loss=2.3052 acc=0.1010 val_loss=2.3049 acc=0.1008\n",
      "[Epoch 021] lr=0.02030 train_loss=2.3053 acc=0.1018 val_loss=2.3073 acc=0.1014\n",
      "[Epoch 022] lr=0.02031 train_loss=2.3055 acc=0.1006 val_loss=2.3029 acc=0.1024\n",
      "[Epoch 023] lr=0.02033 train_loss=2.3053 acc=0.0988 val_loss=2.3045 acc=0.1008\n",
      "[Epoch 024] lr=0.02034 train_loss=2.3054 acc=0.1002 val_loss=2.3052 acc=0.0976\n",
      "[Epoch 025] lr=0.02036 train_loss=2.3054 acc=0.0977 val_loss=2.3056 acc=0.1014\n",
      "[Epoch 026] lr=0.02037 train_loss=2.3056 acc=0.0998 val_loss=2.3062 acc=0.0976\n",
      "[Epoch 027] lr=0.02038 train_loss=2.3055 acc=0.1010 val_loss=2.3093 acc=0.0942\n",
      "[Epoch 028] lr=0.02040 train_loss=2.3052 acc=0.1011 val_loss=2.3046 acc=0.0976\n",
      "[Epoch 029] lr=0.02041 train_loss=2.3053 acc=0.0999 val_loss=2.3038 acc=0.0942\n",
      "[Epoch 030] lr=0.02043 train_loss=2.3053 acc=0.0998 val_loss=2.3049 acc=0.1028\n",
      "[Epoch 031] lr=0.02044 train_loss=2.3054 acc=0.0982 val_loss=2.3040 acc=0.0942\n",
      "[Epoch 032] lr=0.02045 train_loss=2.3054 acc=0.0992 val_loss=2.3030 acc=0.1024\n",
      "[Epoch 033] lr=0.02047 train_loss=2.3054 acc=0.0973 val_loss=2.3033 acc=0.1002\n",
      "[Epoch 034] lr=0.02048 train_loss=2.3055 acc=0.0983 val_loss=2.3042 acc=0.1008\n",
      "[Epoch 035] lr=0.02050 train_loss=2.3052 acc=0.1023 val_loss=2.3039 acc=0.1002\n",
      "[Epoch 036] lr=0.02051 train_loss=2.3055 acc=0.1011 val_loss=2.3057 acc=0.0942\n",
      "[Epoch 037] lr=0.02053 train_loss=2.3055 acc=0.0993 val_loss=2.3063 acc=0.1008\n",
      "[Epoch 038] lr=0.02054 train_loss=2.3052 acc=0.1011 val_loss=2.3039 acc=0.1024\n",
      "[Epoch 039] lr=0.02055 train_loss=2.3057 acc=0.0992 val_loss=2.3036 acc=0.1008\n",
      "[Epoch 040] lr=0.02057 train_loss=2.3053 acc=0.0989 val_loss=2.3060 acc=0.0942\n",
      "[Epoch 041] lr=0.02058 train_loss=2.3054 acc=0.0989 val_loss=2.3055 acc=0.1008\n",
      "[Epoch 042] lr=0.02060 train_loss=2.3053 acc=0.1000 val_loss=2.3027 acc=0.1014\n",
      "[Epoch 043] lr=0.02061 train_loss=2.3052 acc=0.1008 val_loss=2.3040 acc=0.1024\n",
      "[Epoch 044] lr=0.02063 train_loss=2.3054 acc=0.0999 val_loss=2.3055 acc=0.1008\n",
      "[Epoch 045] lr=0.02064 train_loss=2.3054 acc=0.1007 val_loss=2.3043 acc=0.1028\n",
      "[Epoch 046] lr=0.02065 train_loss=2.3051 acc=0.1002 val_loss=2.3062 acc=0.1014\n",
      "[Epoch 047] lr=0.02067 train_loss=2.3052 acc=0.0983 val_loss=2.3045 acc=0.0942\n",
      "[Epoch 048] lr=0.02068 train_loss=2.3055 acc=0.1000 val_loss=2.3073 acc=0.1000\n",
      "[Epoch 049] lr=0.02070 train_loss=2.3057 acc=0.1009 val_loss=2.3042 acc=0.1000\n",
      "[Epoch 050] lr=0.02071 train_loss=2.3053 acc=0.1012 val_loss=2.3063 acc=0.0942\n",
      "[Epoch 051] lr=0.02072 train_loss=2.3053 acc=0.0995 val_loss=2.3049 acc=0.1008\n",
      "[Epoch 052] lr=0.02074 train_loss=2.3049 acc=0.1030 val_loss=2.3101 acc=0.1024\n",
      "[Epoch 053] lr=0.02075 train_loss=2.3056 acc=0.0994 val_loss=2.3062 acc=0.0942\n",
      "[Epoch 054] lr=0.02077 train_loss=2.3055 acc=0.0995 val_loss=2.3049 acc=0.1008\n",
      "[Epoch 055] lr=0.02078 train_loss=2.3052 acc=0.0997 val_loss=2.3054 acc=0.1008\n",
      "[Epoch 056] lr=0.02080 train_loss=2.3056 acc=0.0995 val_loss=2.3029 acc=0.1028\n",
      "[Epoch 057] lr=0.02081 train_loss=2.3056 acc=0.0982 val_loss=2.3042 acc=0.0942\n",
      "[Epoch 058] lr=0.02082 train_loss=2.3052 acc=0.1002 val_loss=2.3056 acc=0.0942\n",
      "[Epoch 059] lr=0.02084 train_loss=2.3054 acc=0.0999 val_loss=2.3053 acc=0.0976\n",
      "[Epoch 060] lr=0.02085 train_loss=2.3052 acc=0.1010 val_loss=2.3074 acc=0.0942\n",
      "FINAL TEST: loss=2.3077  top1_acc=0.1000\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc ▃▃▃▆▄▄▄▁▄▂▆▄▅▄▅▆▅▂▄▁▅▄▂▂▇▃▆▃▃▃▄▂▄▅▆█▃▄▃▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc ▁▆▃█▆▃▄▅▃▁▅▄█▁▅▆▅▃▁▃▆▄▅▄▁▅▁▅▅▆▁▄▄▁▅▅▅▆▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss ▅▂▅▄▅█▆▃▃▃▃▃▆▁▃▅█▃▂▃▁▂▃▂▄▂▂▄▄▁▃▅▆▃▃▄▁▃▄▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.1064\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 60\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.02085\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.10104\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 2.30521\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.0942\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 2.30739\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mi0046\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/xh7nel90\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251015_053158-xh7nel90/logs\u001b[0m\n",
      ">>> \n",
      "            python -m vgg6_cifar.scripts.train_experiment                 --data_dir ./data                 --out_dir ./runs/sweeps_ext/actrelu_bnTrue_bs32_e60_lr0.1_m0.1_rmsprop                 --activation relu                 --batch_size 32                 --epochs 60                 --lr 0.1                 --momentum 0.1                 --optimizer rmsprop                 --weight_decay 0.0005                 --label_smoothing 0.0                 --seed 42                 --aug_hflip                 --aug_crop                 --aug_cutout                 --aug_jitter                 --use_bn                 --amp                 --wandb --wandb_project vgg6-cifar10-assignment --wandb_group q2q5-sweeps --run_name i0047\n",
      "            \n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251015_054202-hj1bi0n7\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mi0047\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/hj1bi0n7\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.02001 train_loss=80.8540 acc=0.1060 val_loss=2.3143 acc=0.0942\n",
      "[Epoch 002] lr=0.02003 train_loss=2.2966 acc=0.1234 val_loss=2.1628 acc=0.1910\n",
      "[Epoch 003] lr=0.02004 train_loss=2.0989 acc=0.1795 val_loss=1.9770 acc=0.1882\n",
      "[Epoch 004] lr=0.02006 train_loss=2.0202 acc=0.1880 val_loss=1.9653 acc=0.2014\n",
      "[Epoch 005] lr=0.02007 train_loss=2.0070 acc=0.1944 val_loss=2.1691 acc=0.1942\n",
      "[Epoch 006] lr=0.02009 train_loss=2.0019 acc=0.1940 val_loss=2.7808 acc=0.1216\n",
      "[Epoch 007] lr=0.02010 train_loss=1.9921 acc=0.1980 val_loss=1.9794 acc=0.2018\n",
      "[Epoch 008] lr=0.02011 train_loss=1.9946 acc=0.1923 val_loss=2.1493 acc=0.1714\n",
      "[Epoch 009] lr=0.02013 train_loss=1.9894 acc=0.1981 val_loss=2.6008 acc=0.1334\n",
      "[Epoch 010] lr=0.02014 train_loss=1.9954 acc=0.1918 val_loss=1.9753 acc=0.2134\n",
      "[Epoch 011] lr=0.02016 train_loss=1.9877 acc=0.1972 val_loss=2.0001 acc=0.2174\n",
      "[Epoch 012] lr=0.02017 train_loss=1.9949 acc=0.1923 val_loss=2.1115 acc=0.2010\n",
      "[Epoch 013] lr=0.02018 train_loss=1.9910 acc=0.1961 val_loss=2.0319 acc=0.2066\n",
      "[Epoch 014] lr=0.02020 train_loss=1.9921 acc=0.1940 val_loss=1.9640 acc=0.1810\n",
      "[Epoch 015] lr=0.02021 train_loss=1.9949 acc=0.1946 val_loss=2.4561 acc=0.1350\n",
      "[Epoch 016] lr=0.02023 train_loss=1.9924 acc=0.1942 val_loss=2.0907 acc=0.1408\n",
      "[Epoch 017] lr=0.02024 train_loss=1.9934 acc=0.1892 val_loss=1.9977 acc=0.1824\n",
      "[Epoch 018] lr=0.02026 train_loss=1.9905 acc=0.1935 val_loss=2.0554 acc=0.1730\n",
      "[Epoch 019] lr=0.02027 train_loss=1.9907 acc=0.1952 val_loss=2.3040 acc=0.1762\n",
      "[Epoch 020] lr=0.02028 train_loss=1.9939 acc=0.1990 val_loss=1.9959 acc=0.2052\n",
      "[Epoch 021] lr=0.02030 train_loss=1.9915 acc=0.1925 val_loss=2.9441 acc=0.1594\n",
      "[Epoch 022] lr=0.02031 train_loss=1.9879 acc=0.1955 val_loss=1.9288 acc=0.2186\n",
      "[Epoch 023] lr=0.02033 train_loss=1.9937 acc=0.1946 val_loss=2.1058 acc=0.1648\n",
      "[Epoch 024] lr=0.02034 train_loss=1.9916 acc=0.1997 val_loss=1.9514 acc=0.2260\n",
      "[Epoch 025] lr=0.02036 train_loss=1.9886 acc=0.1920 val_loss=2.5947 acc=0.1300\n",
      "[Epoch 026] lr=0.02037 train_loss=1.9916 acc=0.1991 val_loss=2.1018 acc=0.1750\n",
      "[Epoch 027] lr=0.02038 train_loss=1.9906 acc=0.1977 val_loss=2.6227 acc=0.1318\n",
      "[Epoch 028] lr=0.02040 train_loss=1.9908 acc=0.1975 val_loss=1.9744 acc=0.2136\n",
      "[Epoch 029] lr=0.02041 train_loss=1.9880 acc=0.1951 val_loss=2.7017 acc=0.1202\n",
      "[Epoch 030] lr=0.02043 train_loss=1.9900 acc=0.1938 val_loss=2.2719 acc=0.1604\n",
      "[Epoch 031] lr=0.02044 train_loss=1.9939 acc=0.1925 val_loss=2.3877 acc=0.1316\n",
      "[Epoch 032] lr=0.02045 train_loss=1.9919 acc=0.1974 val_loss=1.9325 acc=0.2388\n",
      "[Epoch 033] lr=0.02047 train_loss=1.9919 acc=0.1963 val_loss=2.2984 acc=0.1714\n",
      "[Epoch 034] lr=0.02048 train_loss=1.9990 acc=0.1902 val_loss=1.9513 acc=0.2004\n",
      "[Epoch 035] lr=0.02050 train_loss=1.9969 acc=0.1932 val_loss=2.1643 acc=0.1662\n",
      "[Epoch 036] lr=0.02051 train_loss=1.9957 acc=0.1920 val_loss=1.9299 acc=0.2246\n",
      "[Epoch 037] lr=0.02053 train_loss=1.9908 acc=0.1915 val_loss=2.0087 acc=0.1768\n",
      "[Epoch 038] lr=0.02054 train_loss=1.9928 acc=0.1927 val_loss=1.9486 acc=0.2118\n",
      "[Epoch 039] lr=0.02055 train_loss=1.9903 acc=0.1954 val_loss=2.0567 acc=0.1602\n",
      "[Epoch 040] lr=0.02057 train_loss=1.9935 acc=0.1935 val_loss=2.2420 acc=0.1630\n",
      "[Epoch 041] lr=0.02058 train_loss=1.9896 acc=0.1970 val_loss=2.2685 acc=0.1506\n",
      "[Epoch 042] lr=0.02060 train_loss=1.9928 acc=0.1956 val_loss=1.9194 acc=0.2320\n",
      "[Epoch 043] lr=0.02061 train_loss=1.9993 acc=0.1932 val_loss=3.9777 acc=0.1046\n",
      "[Epoch 044] lr=0.02063 train_loss=1.9967 acc=0.1948 val_loss=1.9942 acc=0.1618\n",
      "[Epoch 045] lr=0.02064 train_loss=1.9997 acc=0.1922 val_loss=2.4737 acc=0.1376\n",
      "[Epoch 046] lr=0.02065 train_loss=2.0000 acc=0.1936 val_loss=2.1871 acc=0.1620\n",
      "[Epoch 047] lr=0.02067 train_loss=1.9942 acc=0.1950 val_loss=2.2556 acc=0.1436\n",
      "[Epoch 048] lr=0.02068 train_loss=1.9960 acc=0.1955 val_loss=1.9667 acc=0.2258\n",
      "[Epoch 049] lr=0.02070 train_loss=2.0007 acc=0.1955 val_loss=2.0908 acc=0.1630\n",
      "[Epoch 050] lr=0.02071 train_loss=1.9940 acc=0.1914 val_loss=1.9861 acc=0.1816\n",
      "[Epoch 051] lr=0.02072 train_loss=1.9956 acc=0.1934 val_loss=2.0355 acc=0.1872\n",
      "[Epoch 052] lr=0.02074 train_loss=1.9958 acc=0.1952 val_loss=2.1279 acc=0.1744\n",
      "[Epoch 053] lr=0.02075 train_loss=1.9964 acc=0.1942 val_loss=1.9528 acc=0.2172\n",
      "[Epoch 054] lr=0.02077 train_loss=1.9928 acc=0.1923 val_loss=1.9992 acc=0.2058\n",
      "[Epoch 055] lr=0.02078 train_loss=1.9998 acc=0.1905 val_loss=1.9803 acc=0.2138\n",
      "[Epoch 056] lr=0.02080 train_loss=1.9927 acc=0.1954 val_loss=1.9713 acc=0.1904\n",
      "[Epoch 057] lr=0.02081 train_loss=1.9956 acc=0.1950 val_loss=2.0525 acc=0.2000\n",
      "[Epoch 058] lr=0.02082 train_loss=1.9973 acc=0.1905 val_loss=2.6720 acc=0.1092\n",
      "[Epoch 059] lr=0.02084 train_loss=2.0047 acc=0.1940 val_loss=2.2199 acc=0.1662\n",
      "[Epoch 060] lr=0.02085 train_loss=1.9976 acc=0.1934 val_loss=1.9455 acc=0.1864\n",
      "FINAL TEST: loss=1.8920  top1_acc=0.2301\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣟\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc ▁▂▆▇██████▇███▇██▇█████▇▇████▇███▇█▇██▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc ▁▆▆▆▆▆▅▃▇▆▃▅▅▆▄▄▇▃▅▇▄█▅▆▄▅▄▄▄▂▃▄▃▇▄▅▆▇▂▅\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss ▂▂▁▁▂▁▂▃▁▂▃▂▁▁▂▄▁▁▃▂▁▄▂▃▁▂▁▁▂█▃▂▂▁▂▂▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.2388\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 60\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.2301\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.02085\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.1934\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 1.9976\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.1864\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 1.94554\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mi0047\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/hj1bi0n7\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251015_054202-hj1bi0n7/logs\u001b[0m\n",
      ">>> \n",
      "            python -m vgg6_cifar.scripts.train_experiment                 --data_dir ./data                 --out_dir ./runs/sweeps_ext/actrelu_bnTrue_bs32_e60_lr0.1_m0.1_nadam                 --activation relu                 --batch_size 32                 --epochs 60                 --lr 0.1                 --momentum 0.1                 --optimizer nadam                 --weight_decay 0.0005                 --label_smoothing 0.0                 --seed 42                 --aug_hflip                 --aug_crop                 --aug_cutout                 --aug_jitter                 --use_bn                 --amp                 --wandb --wandb_project vgg6-cifar10-assignment --wandb_group q2q5-sweeps --run_name i0048\n",
      "            \n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m setting up run r95ldye9 (0.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251015_055210-r95ldye9\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mi0048\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/r95ldye9\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.02001 train_loss=3.0362 acc=0.1449 val_loss=2.0129 acc=0.2032\n",
      "[Epoch 002] lr=0.02003 train_loss=2.0061 acc=0.1961 val_loss=1.9717 acc=0.2084\n",
      "[Epoch 003] lr=0.02004 train_loss=1.9704 acc=0.2038 val_loss=1.9256 acc=0.2080\n",
      "[Epoch 004] lr=0.02006 train_loss=1.9500 acc=0.2196 val_loss=1.8981 acc=0.2386\n",
      "[Epoch 005] lr=0.02007 train_loss=1.9280 acc=0.2363 val_loss=1.8677 acc=0.2614\n",
      "[Epoch 006] lr=0.02009 train_loss=1.9067 acc=0.2523 val_loss=1.9344 acc=0.2300\n",
      "[Epoch 007] lr=0.02010 train_loss=1.8951 acc=0.2584 val_loss=2.2541 acc=0.2044\n",
      "[Epoch 008] lr=0.02011 train_loss=1.8920 acc=0.2588 val_loss=2.5292 acc=0.1748\n",
      "[Epoch 009] lr=0.02013 train_loss=1.8898 acc=0.2600 val_loss=2.8243 acc=0.2166\n",
      "[Epoch 010] lr=0.02014 train_loss=1.8947 acc=0.2513 val_loss=1.9318 acc=0.2282\n",
      "[Epoch 011] lr=0.02016 train_loss=1.8826 acc=0.2557 val_loss=1.8395 acc=0.2752\n",
      "[Epoch 012] lr=0.02017 train_loss=1.8853 acc=0.2527 val_loss=1.9724 acc=0.2566\n",
      "[Epoch 013] lr=0.02018 train_loss=1.8780 acc=0.2608 val_loss=1.9128 acc=0.2648\n",
      "[Epoch 014] lr=0.02020 train_loss=1.8885 acc=0.2622 val_loss=1.8056 acc=0.2842\n",
      "[Epoch 015] lr=0.02021 train_loss=1.8851 acc=0.2596 val_loss=1.9704 acc=0.2688\n",
      "[Epoch 016] lr=0.02023 train_loss=1.8789 acc=0.2602 val_loss=1.9460 acc=0.2438\n",
      "[Epoch 017] lr=0.02024 train_loss=1.8782 acc=0.2625 val_loss=2.5575 acc=0.1930\n",
      "[Epoch 018] lr=0.02026 train_loss=1.8798 acc=0.2645 val_loss=1.8036 acc=0.2626\n",
      "[Epoch 019] lr=0.02027 train_loss=1.8736 acc=0.2648 val_loss=1.8864 acc=0.2416\n",
      "[Epoch 020] lr=0.02028 train_loss=1.8746 acc=0.2618 val_loss=1.8071 acc=0.2946\n",
      "[Epoch 021] lr=0.02030 train_loss=1.8775 acc=0.2624 val_loss=1.7828 acc=0.3030\n",
      "[Epoch 022] lr=0.02031 train_loss=1.8765 acc=0.2621 val_loss=1.8507 acc=0.2814\n",
      "[Epoch 023] lr=0.02033 train_loss=1.8770 acc=0.2652 val_loss=1.9190 acc=0.2520\n",
      "[Epoch 024] lr=0.02034 train_loss=1.8811 acc=0.2643 val_loss=1.8303 acc=0.2950\n",
      "[Epoch 025] lr=0.02036 train_loss=1.8746 acc=0.2664 val_loss=1.8970 acc=0.2554\n",
      "[Epoch 026] lr=0.02037 train_loss=1.8782 acc=0.2615 val_loss=1.8214 acc=0.2980\n",
      "[Epoch 027] lr=0.02038 train_loss=1.8776 acc=0.2658 val_loss=2.1515 acc=0.2242\n",
      "[Epoch 028] lr=0.02040 train_loss=1.8744 acc=0.2632 val_loss=1.8528 acc=0.2938\n",
      "[Epoch 029] lr=0.02041 train_loss=1.8748 acc=0.2659 val_loss=1.8966 acc=0.2716\n",
      "[Epoch 030] lr=0.02043 train_loss=1.8718 acc=0.2688 val_loss=1.8711 acc=0.2538\n",
      "[Epoch 031] lr=0.02044 train_loss=1.8800 acc=0.2604 val_loss=1.8027 acc=0.2478\n",
      "[Epoch 032] lr=0.02045 train_loss=1.8819 acc=0.2568 val_loss=1.8132 acc=0.3066\n",
      "[Epoch 033] lr=0.02047 train_loss=1.8721 acc=0.2680 val_loss=1.9806 acc=0.1928\n",
      "[Epoch 034] lr=0.02048 train_loss=1.8788 acc=0.2665 val_loss=2.4965 acc=0.2488\n",
      "[Epoch 035] lr=0.02050 train_loss=1.8783 acc=0.2628 val_loss=1.9202 acc=0.2620\n",
      "[Epoch 036] lr=0.02051 train_loss=1.8800 acc=0.2584 val_loss=2.3015 acc=0.1986\n",
      "[Epoch 037] lr=0.02053 train_loss=1.8774 acc=0.2647 val_loss=1.8311 acc=0.2922\n",
      "[Epoch 038] lr=0.02054 train_loss=1.8779 acc=0.2671 val_loss=1.8615 acc=0.2552\n",
      "[Epoch 039] lr=0.02055 train_loss=1.8755 acc=0.2647 val_loss=1.9061 acc=0.2636\n",
      "[Epoch 040] lr=0.02057 train_loss=1.8756 acc=0.2629 val_loss=1.8058 acc=0.2816\n",
      "[Epoch 041] lr=0.02058 train_loss=1.8732 acc=0.2676 val_loss=2.1031 acc=0.2538\n",
      "[Epoch 042] lr=0.02060 train_loss=1.8770 acc=0.2614 val_loss=1.7942 acc=0.2898\n",
      "[Epoch 043] lr=0.02061 train_loss=1.8723 acc=0.2669 val_loss=1.7888 acc=0.2822\n",
      "[Epoch 044] lr=0.02063 train_loss=1.8769 acc=0.2622 val_loss=1.7734 acc=0.3140\n",
      "[Epoch 045] lr=0.02064 train_loss=1.8789 acc=0.2662 val_loss=2.9586 acc=0.1970\n",
      "[Epoch 046] lr=0.02065 train_loss=1.8700 acc=0.2695 val_loss=1.9052 acc=0.2650\n",
      "[Epoch 047] lr=0.02067 train_loss=1.8711 acc=0.2656 val_loss=1.7963 acc=0.3062\n",
      "[Epoch 048] lr=0.02068 train_loss=1.8702 acc=0.2659 val_loss=2.1672 acc=0.2432\n",
      "[Epoch 049] lr=0.02070 train_loss=1.8775 acc=0.2619 val_loss=2.0880 acc=0.2228\n",
      "[Epoch 050] lr=0.02071 train_loss=1.8728 acc=0.2664 val_loss=1.8072 acc=0.2890\n",
      "[Epoch 051] lr=0.02072 train_loss=1.8749 acc=0.2620 val_loss=2.0441 acc=0.2402\n",
      "[Epoch 052] lr=0.02074 train_loss=1.8726 acc=0.2657 val_loss=1.8995 acc=0.2834\n",
      "[Epoch 053] lr=0.02075 train_loss=1.8761 acc=0.2647 val_loss=1.9664 acc=0.2614\n",
      "[Epoch 054] lr=0.02077 train_loss=1.8717 acc=0.2649 val_loss=2.6598 acc=0.1974\n",
      "[Epoch 055] lr=0.02078 train_loss=1.8769 acc=0.2647 val_loss=1.7586 acc=0.3176\n",
      "[Epoch 056] lr=0.02080 train_loss=1.8651 acc=0.2683 val_loss=1.7562 acc=0.3202\n",
      "[Epoch 057] lr=0.02081 train_loss=1.8673 acc=0.2644 val_loss=1.8167 acc=0.3124\n",
      "[Epoch 058] lr=0.02082 train_loss=1.8738 acc=0.2651 val_loss=2.1933 acc=0.2510\n",
      "[Epoch 059] lr=0.02084 train_loss=1.8694 acc=0.2685 val_loss=2.0091 acc=0.2396\n",
      "[Epoch 060] lr=0.02085 train_loss=1.8694 acc=0.2658 val_loss=1.7926 acc=0.2948\n",
      "FINAL TEST: loss=1.7396  top1_acc=0.3022\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc ▁▄▆▇▇█▇▇▇█▇█████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc ▂▂▄▃▂▆▅▆▅▄▅▇▇▆▄▇▃▇▅▄▁▅▁▇▅▄▆▆█▁▇▄▃▆▆▁██▄▇\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss ▃▂▂▂▂█▂▂▁▂▆▁▂▁▁▂▁▁▄▂▂▁▁▆▅▂▁▃▁▁▂▁▄▃▁▁▁▁▄▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.3202\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 60\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.3022\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.02085\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.2658\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 1.86943\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.2948\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 1.79262\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mi0048\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/r95ldye9\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251015_055210-r95ldye9/logs\u001b[0m\n",
      ">>> \n",
      "            python -m vgg6_cifar.scripts.train_experiment                 --data_dir ./data                 --out_dir ./runs/sweeps_ext/actrelu_bnTrue_bs32_e60_lr0.1_m0.1_adagrad                 --activation relu                 --batch_size 32                 --epochs 60                 --lr 0.1                 --momentum 0.1                 --optimizer adagrad                 --weight_decay 0.0005                 --label_smoothing 0.0                 --seed 42                 --aug_hflip                 --aug_crop                 --aug_cutout                 --aug_jitter                 --use_bn                 --amp                 --wandb --wandb_project vgg6-cifar10-assignment --wandb_group q2q5-sweeps --run_name i0049\n",
      "            \n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251015_060220-0ccp5diu\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mi0049\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/0ccp5diu\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.02001 train_loss=2.7128 acc=0.1844 val_loss=1.8417 acc=0.2844\n",
      "[Epoch 002] lr=0.02003 train_loss=1.8433 acc=0.2726 val_loss=2.2477 acc=0.2308\n",
      "[Epoch 003] lr=0.02004 train_loss=1.6259 acc=0.3684 val_loss=1.5795 acc=0.4040\n",
      "[Epoch 004] lr=0.02006 train_loss=1.4441 acc=0.4495 val_loss=1.2524 acc=0.5608\n",
      "[Epoch 005] lr=0.02007 train_loss=1.3056 acc=0.5115 val_loss=1.3763 acc=0.5028\n",
      "[Epoch 006] lr=0.02009 train_loss=1.1563 acc=0.5777 val_loss=1.2192 acc=0.5774\n",
      "[Epoch 007] lr=0.02010 train_loss=1.0471 acc=0.6276 val_loss=0.9592 acc=0.6652\n",
      "[Epoch 008] lr=0.02011 train_loss=0.9720 acc=0.6562 val_loss=0.8502 acc=0.7000\n",
      "[Epoch 009] lr=0.02013 train_loss=0.8940 acc=0.6884 val_loss=0.8051 acc=0.7070\n",
      "[Epoch 010] lr=0.02014 train_loss=0.8231 acc=0.7179 val_loss=0.7264 acc=0.7534\n",
      "[Epoch 011] lr=0.02016 train_loss=0.7688 acc=0.7418 val_loss=0.7614 acc=0.7382\n",
      "[Epoch 012] lr=0.02017 train_loss=0.7238 acc=0.7564 val_loss=0.6318 acc=0.7770\n",
      "[Epoch 013] lr=0.02018 train_loss=0.6718 acc=0.7784 val_loss=0.6317 acc=0.7760\n",
      "[Epoch 014] lr=0.02020 train_loss=0.6426 acc=0.7878 val_loss=0.7397 acc=0.7420\n",
      "[Epoch 015] lr=0.02021 train_loss=0.6100 acc=0.7992 val_loss=0.6050 acc=0.7946\n",
      "[Epoch 016] lr=0.02023 train_loss=0.5766 acc=0.8104 val_loss=0.6016 acc=0.7932\n",
      "[Epoch 017] lr=0.02024 train_loss=0.5532 acc=0.8214 val_loss=0.5503 acc=0.8124\n",
      "[Epoch 018] lr=0.02026 train_loss=0.5295 acc=0.8272 val_loss=0.4752 acc=0.8352\n",
      "[Epoch 019] lr=0.02027 train_loss=0.4981 acc=0.8371 val_loss=0.5244 acc=0.8228\n",
      "[Epoch 020] lr=0.02028 train_loss=0.4804 acc=0.8460 val_loss=0.8543 acc=0.7382\n",
      "[Epoch 021] lr=0.02030 train_loss=0.4610 acc=0.8494 val_loss=0.4703 acc=0.8380\n",
      "[Epoch 022] lr=0.02031 train_loss=0.4452 acc=0.8564 val_loss=0.4625 acc=0.8442\n",
      "[Epoch 023] lr=0.02033 train_loss=0.4313 acc=0.8591 val_loss=0.5192 acc=0.8276\n",
      "[Epoch 024] lr=0.02034 train_loss=0.4171 acc=0.8645 val_loss=0.4585 acc=0.8444\n",
      "[Epoch 025] lr=0.02036 train_loss=0.4018 acc=0.8696 val_loss=0.4469 acc=0.8460\n",
      "[Epoch 026] lr=0.02037 train_loss=0.3879 acc=0.8738 val_loss=0.4749 acc=0.8382\n",
      "[Epoch 027] lr=0.02038 train_loss=0.3745 acc=0.8780 val_loss=0.4247 acc=0.8552\n",
      "[Epoch 028] lr=0.02040 train_loss=0.3681 acc=0.8802 val_loss=0.4470 acc=0.8502\n",
      "[Epoch 029] lr=0.02041 train_loss=0.3560 acc=0.8858 val_loss=0.4696 acc=0.8430\n",
      "[Epoch 030] lr=0.02043 train_loss=0.3402 acc=0.8904 val_loss=0.4123 acc=0.8584\n",
      "[Epoch 031] lr=0.02044 train_loss=0.3355 acc=0.8895 val_loss=1.0541 acc=0.6920\n",
      "[Epoch 032] lr=0.02045 train_loss=0.3306 acc=0.8925 val_loss=0.4404 acc=0.8598\n",
      "[Epoch 033] lr=0.02047 train_loss=0.3149 acc=0.8994 val_loss=0.3910 acc=0.8670\n",
      "[Epoch 034] lr=0.02048 train_loss=0.3057 acc=0.9012 val_loss=0.3864 acc=0.8686\n",
      "[Epoch 035] lr=0.02050 train_loss=0.3044 acc=0.9021 val_loss=0.4049 acc=0.8700\n",
      "[Epoch 036] lr=0.02051 train_loss=0.2929 acc=0.9048 val_loss=0.6748 acc=0.8154\n",
      "[Epoch 037] lr=0.02053 train_loss=0.2879 acc=0.9065 val_loss=0.4114 acc=0.8650\n",
      "[Epoch 038] lr=0.02054 train_loss=0.2808 acc=0.9093 val_loss=0.3803 acc=0.8768\n",
      "[Epoch 039] lr=0.02055 train_loss=0.2711 acc=0.9130 val_loss=0.3824 acc=0.8710\n",
      "[Epoch 040] lr=0.02057 train_loss=0.2640 acc=0.9154 val_loss=0.3712 acc=0.8784\n",
      "[Epoch 041] lr=0.02058 train_loss=0.2541 acc=0.9172 val_loss=0.3799 acc=0.8774\n",
      "[Epoch 042] lr=0.02060 train_loss=0.2468 acc=0.9187 val_loss=0.4323 acc=0.8694\n",
      "[Epoch 043] lr=0.02061 train_loss=0.2456 acc=0.9209 val_loss=0.3898 acc=0.8756\n",
      "[Epoch 044] lr=0.02063 train_loss=0.2438 acc=0.9216 val_loss=0.3724 acc=0.8808\n",
      "[Epoch 045] lr=0.02064 train_loss=0.2342 acc=0.9235 val_loss=0.3784 acc=0.8776\n",
      "[Epoch 046] lr=0.02065 train_loss=0.2262 acc=0.9260 val_loss=0.3744 acc=0.8768\n",
      "[Epoch 047] lr=0.02067 train_loss=0.2233 acc=0.9274 val_loss=0.4438 acc=0.8586\n",
      "[Epoch 048] lr=0.02068 train_loss=0.2182 acc=0.9292 val_loss=0.3611 acc=0.8804\n",
      "[Epoch 049] lr=0.02070 train_loss=0.2125 acc=0.9316 val_loss=0.3669 acc=0.8818\n",
      "[Epoch 050] lr=0.02071 train_loss=0.2039 acc=0.9336 val_loss=0.3471 acc=0.8860\n",
      "[Epoch 051] lr=0.02072 train_loss=0.2069 acc=0.9329 val_loss=0.3835 acc=0.8810\n",
      "[Epoch 052] lr=0.02074 train_loss=0.2003 acc=0.9350 val_loss=0.3511 acc=0.8866\n",
      "[Epoch 053] lr=0.02075 train_loss=0.1940 acc=0.9366 val_loss=0.4502 acc=0.8630\n",
      "[Epoch 054] lr=0.02077 train_loss=0.1976 acc=0.9359 val_loss=0.3460 acc=0.8900\n",
      "[Epoch 055] lr=0.02078 train_loss=0.1952 acc=0.9363 val_loss=0.3644 acc=0.8818\n",
      "[Epoch 056] lr=0.02080 train_loss=0.1840 acc=0.9410 val_loss=0.3652 acc=0.8840\n",
      "[Epoch 057] lr=0.02081 train_loss=0.1826 acc=0.9422 val_loss=0.4110 acc=0.8714\n",
      "[Epoch 058] lr=0.02082 train_loss=0.1758 acc=0.9439 val_loss=0.3678 acc=0.8858\n",
      "[Epoch 059] lr=0.02084 train_loss=0.1726 acc=0.9446 val_loss=0.3687 acc=0.8882\n",
      "[Epoch 060] lr=0.02085 train_loss=0.1726 acc=0.9441 val_loss=0.3529 acc=0.8888\n",
      "FINAL TEST: loss=0.3591  top1_acc=0.8968\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc ▁▂▃▄▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇█████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▆▅▅▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc ▂▁▃▄▆▆▇▆▇▇▇▇▆▇█▇███▆███▇████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss █▇▅▆▅▃▃▃▂▃▂▂▃▂▂▂▁▂▁▁▁▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.89\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 60\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.8968\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.02085\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.94413\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 0.17261\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.8888\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 0.35293\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mi0049\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/0ccp5diu\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251015_060220-0ccp5diu/logs\u001b[0m\n",
      ">>> \n",
      "            python -m vgg6_cifar.scripts.train_experiment                 --data_dir ./data                 --out_dir ./runs/sweeps_ext/actrelu_bnTrue_bs32_e60_lr0.1_m0.9_sgd                 --activation relu                 --batch_size 32                 --epochs 60                 --lr 0.1                 --momentum 0.9                 --optimizer sgd                 --weight_decay 0.0005                 --label_smoothing 0.0                 --seed 42                 --aug_hflip                 --aug_crop                 --aug_cutout                 --aug_jitter                 --use_bn                 --amp                 --wandb --wandb_project vgg6-cifar10-assignment --wandb_group q2q5-sweeps --run_name i0050\n",
      "            \n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m setting up run ge3le8im (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251015_061229-ge3le8im\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mi0050\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/ge3le8im\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.02001 train_loss=2.0052 acc=0.2319 val_loss=1.7716 acc=0.3350\n",
      "[Epoch 002] lr=0.02003 train_loss=1.6330 acc=0.3976 val_loss=1.3768 acc=0.5040\n",
      "[Epoch 003] lr=0.02004 train_loss=1.2427 acc=0.5581 val_loss=1.0574 acc=0.6186\n",
      "[Epoch 004] lr=0.02006 train_loss=0.9873 acc=0.6577 val_loss=0.8160 acc=0.7126\n",
      "[Epoch 005] lr=0.02007 train_loss=0.8657 acc=0.7064 val_loss=0.7684 acc=0.7338\n",
      "[Epoch 006] lr=0.02009 train_loss=0.7865 acc=0.7356 val_loss=0.7594 acc=0.7402\n",
      "[Epoch 007] lr=0.02010 train_loss=0.7325 acc=0.7554 val_loss=0.7155 acc=0.7544\n",
      "[Epoch 008] lr=0.02011 train_loss=0.6894 acc=0.7687 val_loss=0.6588 acc=0.7670\n",
      "[Epoch 009] lr=0.02013 train_loss=0.6623 acc=0.7764 val_loss=0.7419 acc=0.7546\n",
      "[Epoch 010] lr=0.02014 train_loss=0.6275 acc=0.7920 val_loss=0.7134 acc=0.7500\n",
      "[Epoch 011] lr=0.02016 train_loss=0.6133 acc=0.7954 val_loss=0.7271 acc=0.7578\n",
      "[Epoch 012] lr=0.02017 train_loss=0.5904 acc=0.8004 val_loss=0.5755 acc=0.8024\n",
      "[Epoch 013] lr=0.02018 train_loss=0.5720 acc=0.8091 val_loss=0.6495 acc=0.7818\n",
      "[Epoch 014] lr=0.02020 train_loss=0.5627 acc=0.8132 val_loss=0.7232 acc=0.7696\n",
      "[Epoch 015] lr=0.02021 train_loss=0.5455 acc=0.8159 val_loss=0.6324 acc=0.7928\n",
      "[Epoch 016] lr=0.02023 train_loss=0.5396 acc=0.8189 val_loss=0.6185 acc=0.7842\n",
      "[Epoch 017] lr=0.02024 train_loss=0.5332 acc=0.8207 val_loss=0.5744 acc=0.8004\n",
      "[Epoch 018] lr=0.02026 train_loss=0.5194 acc=0.8262 val_loss=0.4976 acc=0.8284\n",
      "[Epoch 019] lr=0.02027 train_loss=0.5056 acc=0.8317 val_loss=0.6248 acc=0.7912\n",
      "[Epoch 020] lr=0.02028 train_loss=0.5113 acc=0.8296 val_loss=0.7630 acc=0.7604\n",
      "[Epoch 021] lr=0.02030 train_loss=0.4989 acc=0.8326 val_loss=0.7573 acc=0.7586\n",
      "[Epoch 022] lr=0.02031 train_loss=0.5012 acc=0.8321 val_loss=0.6003 acc=0.7924\n",
      "[Epoch 023] lr=0.02033 train_loss=0.4927 acc=0.8360 val_loss=0.5756 acc=0.8014\n",
      "[Epoch 024] lr=0.02034 train_loss=0.4808 acc=0.8380 val_loss=0.5116 acc=0.8226\n",
      "[Epoch 025] lr=0.02036 train_loss=0.4789 acc=0.8411 val_loss=0.5414 acc=0.8100\n",
      "[Epoch 026] lr=0.02037 train_loss=0.4797 acc=0.8408 val_loss=0.6162 acc=0.7852\n",
      "[Epoch 027] lr=0.02038 train_loss=0.4777 acc=0.8399 val_loss=0.4571 acc=0.8420\n",
      "[Epoch 028] lr=0.02040 train_loss=0.4709 acc=0.8417 val_loss=0.5595 acc=0.8176\n",
      "[Epoch 029] lr=0.02041 train_loss=0.4670 acc=0.8429 val_loss=0.5318 acc=0.8198\n",
      "[Epoch 030] lr=0.02043 train_loss=0.4666 acc=0.8433 val_loss=0.5930 acc=0.8014\n",
      "[Epoch 031] lr=0.02044 train_loss=0.4668 acc=0.8433 val_loss=0.5018 acc=0.8280\n",
      "[Epoch 032] lr=0.02045 train_loss=0.4576 acc=0.8464 val_loss=0.4961 acc=0.8298\n",
      "[Epoch 033] lr=0.02047 train_loss=0.4620 acc=0.8466 val_loss=0.4765 acc=0.8398\n",
      "[Epoch 034] lr=0.02048 train_loss=0.4590 acc=0.8472 val_loss=0.5438 acc=0.8154\n",
      "[Epoch 035] lr=0.02050 train_loss=0.4594 acc=0.8470 val_loss=0.4867 acc=0.8354\n",
      "[Epoch 036] lr=0.02051 train_loss=0.4566 acc=0.8482 val_loss=0.5265 acc=0.8266\n",
      "[Epoch 037] lr=0.02053 train_loss=0.4537 acc=0.8475 val_loss=0.5411 acc=0.8196\n",
      "[Epoch 038] lr=0.02054 train_loss=0.4527 acc=0.8486 val_loss=0.4646 acc=0.8420\n",
      "[Epoch 039] lr=0.02055 train_loss=0.4527 acc=0.8515 val_loss=0.5470 acc=0.8144\n",
      "[Epoch 040] lr=0.02057 train_loss=0.4454 acc=0.8522 val_loss=0.4494 acc=0.8506\n",
      "[Epoch 041] lr=0.02058 train_loss=0.4446 acc=0.8508 val_loss=0.4752 acc=0.8334\n",
      "[Epoch 042] lr=0.02060 train_loss=0.4412 acc=0.8514 val_loss=0.4695 acc=0.8330\n",
      "[Epoch 043] lr=0.02061 train_loss=0.4431 acc=0.8531 val_loss=0.4832 acc=0.8346\n",
      "[Epoch 044] lr=0.02063 train_loss=0.4418 acc=0.8517 val_loss=0.6485 acc=0.7924\n",
      "[Epoch 045] lr=0.02064 train_loss=0.4453 acc=0.8528 val_loss=0.5109 acc=0.8254\n",
      "[Epoch 046] lr=0.02065 train_loss=0.4443 acc=0.8514 val_loss=0.5188 acc=0.8242\n",
      "[Epoch 047] lr=0.02067 train_loss=0.4400 acc=0.8520 val_loss=0.5195 acc=0.8234\n",
      "[Epoch 048] lr=0.02068 train_loss=0.4377 acc=0.8528 val_loss=0.4784 acc=0.8418\n",
      "[Epoch 049] lr=0.02070 train_loss=0.4295 acc=0.8556 val_loss=0.6431 acc=0.7836\n",
      "[Epoch 050] lr=0.02071 train_loss=0.4380 acc=0.8550 val_loss=0.4395 acc=0.8436\n",
      "[Epoch 051] lr=0.02072 train_loss=0.4326 acc=0.8562 val_loss=0.4774 acc=0.8340\n",
      "[Epoch 052] lr=0.02074 train_loss=0.4333 acc=0.8540 val_loss=0.5292 acc=0.8242\n",
      "[Epoch 053] lr=0.02075 train_loss=0.4389 acc=0.8540 val_loss=0.4787 acc=0.8366\n",
      "[Epoch 054] lr=0.02077 train_loss=0.4350 acc=0.8553 val_loss=0.4733 acc=0.8370\n",
      "[Epoch 055] lr=0.02078 train_loss=0.4403 acc=0.8533 val_loss=0.5251 acc=0.8310\n",
      "[Epoch 056] lr=0.02080 train_loss=0.4330 acc=0.8549 val_loss=0.5421 acc=0.8220\n",
      "[Epoch 057] lr=0.02081 train_loss=0.4323 acc=0.8561 val_loss=0.5629 acc=0.8080\n",
      "[Epoch 058] lr=0.02082 train_loss=0.4290 acc=0.8581 val_loss=0.4745 acc=0.8370\n",
      "[Epoch 059] lr=0.02084 train_loss=0.4370 acc=0.8550 val_loss=0.4727 acc=0.8394\n",
      "[Epoch 060] lr=0.02085 train_loss=0.4306 acc=0.8555 val_loss=0.4848 acc=0.8352\n",
      "FINAL TEST: loss=0.4200  top1_acc=0.8609\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc ▁▃▅▆▆▇▇▇▇▇██████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▆▅▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc ▁▃▅▆▇▇▇▇▇▇▇█▇▇▇██▇███████████████▇██████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss █▆▄▃▂▂▃▂▂▂▂▁▂▃▃▂▁▂▂▂▂▁▁▁▂▂▁▂▁▁▁▁▁▂▁▁▁▂▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.8506\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 60\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.8609\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.02085\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.85547\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 0.43065\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.8352\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 0.48485\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mi0050\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/ge3le8im\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251015_061229-ge3le8im/logs\u001b[0m\n",
      ">>> \n",
      "            python -m vgg6_cifar.scripts.train_experiment                 --data_dir ./data                 --out_dir ./runs/sweeps_ext/actrelu_bnTrue_bs32_e60_lr0.1_m0.9_nesterov-sgd                 --activation relu                 --batch_size 32                 --epochs 60                 --lr 0.1                 --momentum 0.9                 --optimizer nesterov-sgd                 --weight_decay 0.0005                 --label_smoothing 0.0                 --seed 42                 --aug_hflip                 --aug_crop                 --aug_cutout                 --aug_jitter                 --use_bn                 --amp                 --wandb --wandb_project vgg6-cifar10-assignment --wandb_group q2q5-sweeps --run_name i0051\n",
      "            \n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m setting up run 9feprnox (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251015_062232-9feprnox\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mi0051\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/9feprnox\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.02001 train_loss=1.9929 acc=0.2359 val_loss=1.6891 acc=0.3386\n",
      "[Epoch 002] lr=0.02003 train_loss=1.5032 acc=0.4484 val_loss=1.2552 acc=0.5464\n",
      "[Epoch 003] lr=0.02004 train_loss=1.1193 acc=0.6056 val_loss=0.9724 acc=0.6562\n",
      "[Epoch 004] lr=0.02006 train_loss=0.9210 acc=0.6822 val_loss=0.8168 acc=0.7086\n",
      "[Epoch 005] lr=0.02007 train_loss=0.8149 acc=0.7243 val_loss=0.7316 acc=0.7460\n",
      "[Epoch 006] lr=0.02009 train_loss=0.7406 acc=0.7503 val_loss=0.7200 acc=0.7520\n",
      "[Epoch 007] lr=0.02010 train_loss=0.6922 acc=0.7693 val_loss=0.6798 acc=0.7708\n",
      "[Epoch 008] lr=0.02011 train_loss=0.6547 acc=0.7801 val_loss=0.6483 acc=0.7800\n",
      "[Epoch 009] lr=0.02013 train_loss=0.6311 acc=0.7898 val_loss=0.6003 acc=0.7940\n",
      "[Epoch 010] lr=0.02014 train_loss=0.6037 acc=0.7990 val_loss=0.7410 acc=0.7496\n",
      "[Epoch 011] lr=0.02016 train_loss=0.5817 acc=0.8055 val_loss=0.7330 acc=0.7594\n",
      "[Epoch 012] lr=0.02017 train_loss=0.5658 acc=0.8115 val_loss=0.6139 acc=0.8022\n",
      "[Epoch 013] lr=0.02018 train_loss=0.5492 acc=0.8158 val_loss=0.6098 acc=0.7964\n",
      "[Epoch 014] lr=0.02020 train_loss=0.5391 acc=0.8203 val_loss=0.6575 acc=0.7866\n",
      "[Epoch 015] lr=0.02021 train_loss=0.5284 acc=0.8223 val_loss=0.6433 acc=0.7836\n",
      "[Epoch 016] lr=0.02023 train_loss=0.5160 acc=0.8272 val_loss=0.7530 acc=0.7574\n",
      "[Epoch 017] lr=0.02024 train_loss=0.5091 acc=0.8312 val_loss=0.5229 acc=0.8196\n",
      "[Epoch 018] lr=0.02026 train_loss=0.5026 acc=0.8319 val_loss=0.4844 acc=0.8366\n",
      "[Epoch 019] lr=0.02027 train_loss=0.4925 acc=0.8357 val_loss=0.5053 acc=0.8236\n",
      "[Epoch 020] lr=0.02028 train_loss=0.4852 acc=0.8373 val_loss=0.6094 acc=0.7932\n",
      "[Epoch 021] lr=0.02030 train_loss=0.4836 acc=0.8394 val_loss=0.6178 acc=0.7956\n",
      "[Epoch 022] lr=0.02031 train_loss=0.4849 acc=0.8381 val_loss=0.5298 acc=0.8198\n",
      "[Epoch 023] lr=0.02033 train_loss=0.4747 acc=0.8424 val_loss=0.6507 acc=0.7798\n",
      "[Epoch 024] lr=0.02034 train_loss=0.4696 acc=0.8422 val_loss=0.5221 acc=0.8316\n",
      "[Epoch 025] lr=0.02036 train_loss=0.4665 acc=0.8427 val_loss=0.4819 acc=0.8408\n",
      "[Epoch 026] lr=0.02037 train_loss=0.4584 acc=0.8441 val_loss=0.5475 acc=0.8140\n",
      "[Epoch 027] lr=0.02038 train_loss=0.4616 acc=0.8438 val_loss=0.4553 acc=0.8472\n",
      "[Epoch 028] lr=0.02040 train_loss=0.4581 acc=0.8476 val_loss=0.5983 acc=0.7986\n",
      "[Epoch 029] lr=0.02041 train_loss=0.4581 acc=0.8472 val_loss=0.4988 acc=0.8318\n",
      "[Epoch 030] lr=0.02043 train_loss=0.4550 acc=0.8478 val_loss=0.5162 acc=0.8280\n",
      "[Epoch 031] lr=0.02044 train_loss=0.4501 acc=0.8494 val_loss=0.5081 acc=0.8256\n",
      "[Epoch 032] lr=0.02045 train_loss=0.4435 acc=0.8536 val_loss=0.4600 acc=0.8442\n",
      "[Epoch 033] lr=0.02047 train_loss=0.4454 acc=0.8509 val_loss=0.4943 acc=0.8332\n",
      "[Epoch 034] lr=0.02048 train_loss=0.4406 acc=0.8538 val_loss=0.5198 acc=0.8204\n",
      "[Epoch 035] lr=0.02050 train_loss=0.4431 acc=0.8527 val_loss=0.5059 acc=0.8258\n",
      "[Epoch 036] lr=0.02051 train_loss=0.4391 acc=0.8531 val_loss=0.4845 acc=0.8346\n",
      "[Epoch 037] lr=0.02053 train_loss=0.4375 acc=0.8526 val_loss=0.5421 acc=0.8132\n",
      "[Epoch 038] lr=0.02054 train_loss=0.4421 acc=0.8533 val_loss=0.5192 acc=0.8208\n",
      "[Epoch 039] lr=0.02055 train_loss=0.4336 acc=0.8538 val_loss=0.5525 acc=0.8170\n",
      "[Epoch 040] lr=0.02057 train_loss=0.4348 acc=0.8546 val_loss=0.5054 acc=0.8330\n",
      "[Epoch 041] lr=0.02058 train_loss=0.4283 acc=0.8549 val_loss=0.5893 acc=0.8064\n",
      "[Epoch 042] lr=0.02060 train_loss=0.4354 acc=0.8546 val_loss=0.4714 acc=0.8460\n",
      "[Epoch 043] lr=0.02061 train_loss=0.4282 acc=0.8571 val_loss=0.5082 acc=0.8240\n",
      "[Epoch 044] lr=0.02063 train_loss=0.4313 acc=0.8551 val_loss=0.6114 acc=0.7982\n",
      "[Epoch 045] lr=0.02064 train_loss=0.4320 acc=0.8551 val_loss=0.5848 acc=0.8092\n",
      "[Epoch 046] lr=0.02065 train_loss=0.4240 acc=0.8582 val_loss=0.5554 acc=0.8130\n",
      "[Epoch 047] lr=0.02067 train_loss=0.4241 acc=0.8582 val_loss=0.5452 acc=0.8162\n",
      "[Epoch 048] lr=0.02068 train_loss=0.4257 acc=0.8575 val_loss=0.5040 acc=0.8310\n",
      "[Epoch 049] lr=0.02070 train_loss=0.4256 acc=0.8570 val_loss=0.4823 acc=0.8364\n",
      "[Epoch 050] lr=0.02071 train_loss=0.4222 acc=0.8595 val_loss=0.4991 acc=0.8310\n",
      "[Epoch 051] lr=0.02072 train_loss=0.4227 acc=0.8574 val_loss=0.5004 acc=0.8302\n",
      "[Epoch 052] lr=0.02074 train_loss=0.4204 acc=0.8573 val_loss=0.5114 acc=0.8300\n",
      "[Epoch 053] lr=0.02075 train_loss=0.4236 acc=0.8581 val_loss=0.4348 acc=0.8514\n",
      "[Epoch 054] lr=0.02077 train_loss=0.4245 acc=0.8561 val_loss=0.4398 acc=0.8466\n",
      "[Epoch 055] lr=0.02078 train_loss=0.4221 acc=0.8572 val_loss=0.4784 acc=0.8426\n",
      "[Epoch 056] lr=0.02080 train_loss=0.4217 acc=0.8584 val_loss=0.6078 acc=0.7952\n",
      "[Epoch 057] lr=0.02081 train_loss=0.4209 acc=0.8590 val_loss=0.8350 acc=0.7426\n",
      "[Epoch 058] lr=0.02082 train_loss=0.4169 acc=0.8605 val_loss=0.5241 acc=0.8300\n",
      "[Epoch 059] lr=0.02084 train_loss=0.4176 acc=0.8608 val_loss=0.5165 acc=0.8196\n",
      "[Epoch 060] lr=0.02085 train_loss=0.4180 acc=0.8604 val_loss=0.4382 acc=0.8474\n",
      "FINAL TEST: loss=0.4120  top1_acc=0.8621\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc ▁▃▅▆▆▇▇▇▇▇██████████████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▆▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc ▁▄▅▆▇▇▇▇▇▇▇▇▇██▇▇██████████████▇▇████▇▇█\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss █▄▃▃▂▃▂▂▂▂▁▁▂▂▂▁▁▂▁▂▁▁▁▁▂▂▁▁▂▂▂▁▁▁▁▁▂▃▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.8514\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 60\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.8621\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.02085\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.86042\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 0.41801\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.8474\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 0.43825\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mi0051\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/9feprnox\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251015_062232-9feprnox/logs\u001b[0m\n",
      ">>> \n",
      "            python -m vgg6_cifar.scripts.train_experiment                 --data_dir ./data                 --out_dir ./runs/sweeps_ext/actrelu_bnTrue_bs32_e60_lr0.1_m0.9_adam                 --activation relu                 --batch_size 32                 --epochs 60                 --lr 0.1                 --momentum 0.9                 --optimizer adam                 --weight_decay 0.0005                 --label_smoothing 0.0                 --seed 42                 --aug_hflip                 --aug_crop                 --aug_cutout                 --aug_jitter                 --use_bn                 --amp                 --wandb --wandb_project vgg6-cifar10-assignment --wandb_group q2q5-sweeps --run_name i0052\n",
      "            \n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251015_063236-biix9hbo\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mi0052\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/biix9hbo\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.02001 train_loss=2.8835 acc=0.1392 val_loss=2.1312 acc=0.1762\n",
      "[Epoch 002] lr=0.02003 train_loss=2.0860 acc=0.1838 val_loss=1.9827 acc=0.1888\n",
      "[Epoch 003] lr=0.02004 train_loss=2.0476 acc=0.1853 val_loss=2.1754 acc=0.1720\n",
      "[Epoch 004] lr=0.02006 train_loss=2.0313 acc=0.1876 val_loss=2.0268 acc=0.1962\n",
      "[Epoch 005] lr=0.02007 train_loss=2.0273 acc=0.1831 val_loss=1.9661 acc=0.2050\n",
      "[Epoch 006] lr=0.02009 train_loss=2.0193 acc=0.1879 val_loss=1.9524 acc=0.2040\n",
      "[Epoch 007] lr=0.02010 train_loss=2.0155 acc=0.1900 val_loss=2.1655 acc=0.1670\n",
      "[Epoch 008] lr=0.02011 train_loss=2.0149 acc=0.1895 val_loss=1.9699 acc=0.1932\n",
      "[Epoch 009] lr=0.02013 train_loss=2.0192 acc=0.1931 val_loss=2.0797 acc=0.1870\n",
      "[Epoch 010] lr=0.02014 train_loss=2.0108 acc=0.1929 val_loss=2.0624 acc=0.1994\n",
      "[Epoch 011] lr=0.02016 train_loss=2.0092 acc=0.1936 val_loss=2.0093 acc=0.2214\n",
      "[Epoch 012] lr=0.02017 train_loss=2.0079 acc=0.1931 val_loss=2.0113 acc=0.2106\n",
      "[Epoch 013] lr=0.02018 train_loss=2.0041 acc=0.1927 val_loss=1.9330 acc=0.2032\n",
      "[Epoch 014] lr=0.02020 train_loss=2.0167 acc=0.1935 val_loss=2.0548 acc=0.1636\n",
      "[Epoch 015] lr=0.02021 train_loss=2.0179 acc=0.1898 val_loss=1.9844 acc=0.2002\n",
      "[Epoch 016] lr=0.02023 train_loss=2.0107 acc=0.1898 val_loss=1.9429 acc=0.1882\n",
      "[Epoch 017] lr=0.02024 train_loss=2.0145 acc=0.1898 val_loss=1.9752 acc=0.1956\n",
      "[Epoch 018] lr=0.02026 train_loss=2.0087 acc=0.1922 val_loss=1.9275 acc=0.2082\n",
      "[Epoch 019] lr=0.02027 train_loss=2.0116 acc=0.1933 val_loss=2.0812 acc=0.2062\n",
      "[Epoch 020] lr=0.02028 train_loss=2.0147 acc=0.1932 val_loss=2.0592 acc=0.2074\n",
      "[Epoch 021] lr=0.02030 train_loss=2.0185 acc=0.1895 val_loss=1.9470 acc=0.2252\n",
      "[Epoch 022] lr=0.02031 train_loss=2.0111 acc=0.1946 val_loss=2.0148 acc=0.2120\n",
      "[Epoch 023] lr=0.02033 train_loss=2.0144 acc=0.1939 val_loss=1.9549 acc=0.2088\n",
      "[Epoch 024] lr=0.02034 train_loss=2.0159 acc=0.1916 val_loss=1.9538 acc=0.2000\n",
      "[Epoch 025] lr=0.02036 train_loss=2.0144 acc=0.1876 val_loss=2.0153 acc=0.2020\n",
      "[Epoch 026] lr=0.02037 train_loss=2.0105 acc=0.1923 val_loss=2.0176 acc=0.1842\n",
      "[Epoch 027] lr=0.02038 train_loss=2.0121 acc=0.1928 val_loss=1.9437 acc=0.2050\n",
      "^C\n",
      "Exception in thread Thread-57 (_pin_memory_loop):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/pin_memory.py\", line 54, in _pin_memory_loop\n",
      "    do_one_step()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/pin_memory.py\", line 31, in do_one_step\n",
      "    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/multiprocessing/reductions.py\", line 355, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/resource_sharer.py\", line 86, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 502, in Client\n",
      "    c = SocketClient(address)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 630, in SocketClient\n",
      "    s.connect(address)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/vgg6_cifar/scripts/train_experiment.py\", line 96, in <module>\n",
      "    main()\n",
      "  File \"/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/vgg6_cifar/scripts/train_experiment.py\", line 73, in main\n",
      "    tr_loss, tr_acc = train_one_epoch(model, train_loader, optimizer, device, scaler, criterion, epoch, writer)\n",
      "  File \"/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/vgg6_cifar/engine/trainer.py\", line 16, in train_one_epoch\n",
      "    scaler.scale(loss).backward(); scaler.step(optimizer); scaler.update()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py\", line 410, in step\n",
      "    self.unscale_(optimizer)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py\", line 307, in unscale_\n",
      "    optimizer_state[\"found_inf_per_device\"] = self._unscale_grads_(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py\", line 223, in _unscale_grads_\n",
      "    with torch.no_grad():\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/vgg6_cifar/scripts/train_experiment.py\", line 96, in <module>\n",
      "    main()\n",
      "  File \"/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/vgg6_cifar/scripts/train_experiment.py\", line 73, in main\n",
      "    tr_loss, tr_acc = train_one_epoch(model, train_loader, optimizer, device, scaler, criterion, epoch, writer)\n",
      "  File \"/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/vgg6_cifar/engine/trainer.py\", line 16, in train_one_epoch\n",
      "    scaler.scale(loss).backward(); scaler.step(optimizer); scaler.update()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py\", line 410, in step\n",
      "    self.unscale_(optimizer)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py\", line 307, in unscale_\n",
      "    optimizer_state[\"found_inf_per_device\"] = self._unscale_grads_(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py\", line 223, in _unscale_grads_\n",
      "    with torch.no_grad():\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python -m vgg6_cifar.scripts.sweep_grid \\\n",
    "  --data_dir ./data --base_out ./runs/sweeps_ext \\\n",
    "  --epochs_list 20,60,100 \\\n",
    "  --batch_sizes 32,64,128,512 \\\n",
    "  --lrs 0.1,0.01,0.001 \\\n",
    "  --momentums 0.1,0.9 \\\n",
    "  --optimizers sgd,nesterov-sgd,adam,adamw,rmsprop,nadam,adagrad \\\n",
    "  --activations relu,silu,gelu,tanh,sigmoid \\\n",
    "  --batch_norms true,false \\\n",
    "  --amp --wandb --seed 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75515d14-5979-465d-895f-6119c60738ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ./runs/act_gelu/scatter_valacc_vs_step.png\n"
     ]
    }
   ],
   "source": [
    "!python -m vgg6_cifar.scripts.plot_scatter_valacc_vs_step \\\n",
    "  --metrics_csv ./runs/act_gelu/metrics.csv \\\n",
    "  --out_png    ./runs/act_gelu/scatter_valacc_vs_step.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b9e214a-3b39-4c0e-a7c2-dcd9fa1e27d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved plots to: ./runs/act_gelu\n"
     ]
    }
   ],
   "source": [
    "!python -m vgg6_cifar.scripts.plot_curves \\\n",
    "  --metrics_csv ./runs/act_gelu/metrics.csv \\\n",
    "  --out_dir     ./runs/act_gelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "267db5ab-218b-44f7-a6c2-7b7e365fd1e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m setting up run lqui28zz (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251016_085333-lqui28zz\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mwhole-brook-1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/lqui28zz\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.00349 train_loss=1.5740 acc=0.4332 val_loss=1.3183 acc=0.5162\n",
      "[Epoch 002] lr=0.00350 train_loss=1.0844 acc=0.6127 val_loss=0.9839 acc=0.6398\n",
      "[Epoch 003] lr=0.00351 train_loss=0.9098 acc=0.6788 val_loss=0.8727 acc=0.6858\n",
      "[Epoch 004] lr=0.00352 train_loss=0.8115 acc=0.7177 val_loss=0.7606 acc=0.7212\n",
      "[Epoch 005] lr=0.00353 train_loss=0.7465 acc=0.7378 val_loss=0.7868 acc=0.7152\n",
      "[Epoch 006] lr=0.00354 train_loss=0.7039 acc=0.7534 val_loss=0.7135 acc=0.7432\n",
      "[Epoch 007] lr=0.00355 train_loss=0.6612 acc=0.7694 val_loss=0.8023 acc=0.7284\n",
      "[Epoch 008] lr=0.00356 train_loss=0.6317 acc=0.7834 val_loss=0.6529 acc=0.7698\n",
      "[Epoch 009] lr=0.00357 train_loss=0.6052 acc=0.7912 val_loss=0.6180 acc=0.7786\n",
      "[Epoch 010] lr=0.00358 train_loss=0.5791 acc=0.8004 val_loss=0.6077 acc=0.7876\n",
      "[Epoch 011] lr=0.00359 train_loss=0.5591 acc=0.8065 val_loss=0.6631 acc=0.7680\n",
      "[Epoch 012] lr=0.00360 train_loss=0.5400 acc=0.8140 val_loss=0.6086 acc=0.7844\n",
      "[Epoch 013] lr=0.00361 train_loss=0.5235 acc=0.8192 val_loss=0.5712 acc=0.7980\n",
      "[Epoch 014] lr=0.00362 train_loss=0.5088 acc=0.8249 val_loss=0.5979 acc=0.7860\n",
      "[Epoch 015] lr=0.00363 train_loss=0.4923 acc=0.8298 val_loss=0.5455 acc=0.8124\n",
      "[Epoch 016] lr=0.00364 train_loss=0.4815 acc=0.8345 val_loss=0.5424 acc=0.8108\n",
      "[Epoch 017] lr=0.00365 train_loss=0.4675 acc=0.8386 val_loss=0.5372 acc=0.8108\n",
      "[Epoch 018] lr=0.00366 train_loss=0.4602 acc=0.8420 val_loss=0.5165 acc=0.8190\n",
      "[Epoch 019] lr=0.00367 train_loss=0.4525 acc=0.8441 val_loss=0.4997 acc=0.8240\n",
      "[Epoch 020] lr=0.00368 train_loss=0.4426 acc=0.8484 val_loss=0.4925 acc=0.8230\n",
      "[Epoch 021] lr=0.00369 train_loss=0.4322 acc=0.8513 val_loss=0.4829 acc=0.8276\n",
      "[Epoch 022] lr=0.00370 train_loss=0.4209 acc=0.8553 val_loss=0.4850 acc=0.8262\n",
      "[Epoch 023] lr=0.00371 train_loss=0.4148 acc=0.8577 val_loss=0.5140 acc=0.8286\n",
      "[Epoch 024] lr=0.00372 train_loss=0.4086 acc=0.8587 val_loss=0.4783 acc=0.8338\n",
      "[Epoch 025] lr=0.00373 train_loss=0.3997 acc=0.8619 val_loss=0.5073 acc=0.8204\n",
      "[Epoch 026] lr=0.00374 train_loss=0.3907 acc=0.8653 val_loss=0.4862 acc=0.8282\n",
      "[Epoch 027] lr=0.00375 train_loss=0.3829 acc=0.8693 val_loss=0.4918 acc=0.8258\n",
      "[Epoch 028] lr=0.00376 train_loss=0.3766 acc=0.8699 val_loss=0.4718 acc=0.8334\n",
      "[Epoch 029] lr=0.00377 train_loss=0.3693 acc=0.8740 val_loss=0.4546 acc=0.8390\n",
      "[Epoch 030] lr=0.00378 train_loss=0.3649 acc=0.8747 val_loss=0.4564 acc=0.8426\n",
      "[Epoch 031] lr=0.00379 train_loss=0.3622 acc=0.8750 val_loss=0.4461 acc=0.8446\n",
      "[Epoch 032] lr=0.00380 train_loss=0.3532 acc=0.8779 val_loss=0.4749 acc=0.8390\n",
      "[Epoch 033] lr=0.00381 train_loss=0.3454 acc=0.8808 val_loss=0.4482 acc=0.8414\n",
      "[Epoch 034] lr=0.00382 train_loss=0.3390 acc=0.8831 val_loss=0.4642 acc=0.8350\n",
      "[Epoch 035] lr=0.00383 train_loss=0.3395 acc=0.8824 val_loss=0.4786 acc=0.8312\n",
      "[Epoch 036] lr=0.00384 train_loss=0.3271 acc=0.8896 val_loss=0.4485 acc=0.8410\n",
      "[Epoch 037] lr=0.00385 train_loss=0.3288 acc=0.8859 val_loss=0.4365 acc=0.8486\n",
      "[Epoch 038] lr=0.00386 train_loss=0.3185 acc=0.8891 val_loss=0.4405 acc=0.8440\n",
      "[Epoch 039] lr=0.00387 train_loss=0.3205 acc=0.8908 val_loss=0.4422 acc=0.8416\n",
      "[Epoch 040] lr=0.00388 train_loss=0.3114 acc=0.8940 val_loss=0.4108 acc=0.8574\n",
      "FINAL TEST: loss=0.3933  top1_acc=0.8664\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc ▁▄▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc ▁▄▄▅▅▆▅▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇███████▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss █▅▅▄▄▃▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▂▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.8574\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 40\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.8664\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.00388\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.89396\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 0.31135\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.8574\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 0.41084\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mwhole-brook-1\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/lqui28zz\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251016_085333-lqui28zz/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m vgg6_cifar.scripts.train_experiment \\\n",
    "    --data_dir      ./data \\\n",
    "    --out_dir       ./runs/final_best \\\n",
    "    --activation    gelu \\\n",
    "    --optimizer     adagrad \\\n",
    "    --lr            0.01392 \\\n",
    "    --batch_size    128 \\\n",
    "    --epochs        40 \\\n",
    "    --momentum      0 \\\n",
    "    --use_bn \\\n",
    "    --aug_hflip \\\n",
    "    --aug_crop \\\n",
    "    --aug_cutout \\\n",
    "    --aug_jitter \\\n",
    "    --amp \\\n",
    "    --wandb \\\n",
    "    --seed          42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e5066d8a-21b6-4d55-9598-66345c7605b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/usr/local/lib/python3.10/dist-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m505\u001b[0m (\u001b[33mcs24m505-iitmaana\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m setting up run evkbzcv6 (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m setting up run evkbzcv6 (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m setting up run evkbzcv6 (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m setting up run evkbzcv6 (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣟\u001b[0m setting up run evkbzcv6 (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/prj/qct/lv/computeai-scratch/Alok/M.tech/vgg6_cifar_full_epbundle/wandb/run-20251016_103459-evkbzcv6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mglamorous-darkness-6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/evkbzcv6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n",
      "CIFAR-10 normalization: mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616)\n",
      "Augmentations used for TRAIN: RandomCrop(32, padding=4), RandomHorizontalFlip(p=0.5), ColorJitter(0.2/0.2/0.2/0.02), RandomErasingSquare(~2%)\n",
      "VAL/TEST transforms: ToTensor + Normalize\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 001] lr=0.00201 train_loss=1.9484 acc=0.3121 val_loss=1.6919 acc=0.4380\n",
      "[Epoch 002] lr=0.00201 train_loss=1.6417 acc=0.4663 val_loss=1.5349 acc=0.5236\n",
      "[Epoch 003] lr=0.00202 train_loss=1.5140 acc=0.5376 val_loss=1.4374 acc=0.5752\n",
      "[Epoch 004] lr=0.00202 train_loss=1.4218 acc=0.5851 val_loss=1.4017 acc=0.5888\n",
      "[Epoch 005] lr=0.00203 train_loss=1.3438 acc=0.6236 val_loss=1.3129 acc=0.6290\n",
      "[Epoch 006] lr=0.00203 train_loss=1.2949 acc=0.6475 val_loss=1.2428 acc=0.6680\n",
      "[Epoch 007] lr=0.00204 train_loss=1.2500 acc=0.6725 val_loss=1.2181 acc=0.6820\n",
      "[Epoch 008] lr=0.00205 train_loss=1.2172 acc=0.6870 val_loss=1.2139 acc=0.6874\n",
      "[Epoch 009] lr=0.00205 train_loss=1.1899 acc=0.7031 val_loss=1.1704 acc=0.7102\n",
      "[Epoch 010] lr=0.00206 train_loss=1.1629 acc=0.7158 val_loss=1.1452 acc=0.7174\n",
      "[Epoch 011] lr=0.00206 train_loss=1.1418 acc=0.7264 val_loss=1.1249 acc=0.7296\n",
      "[Epoch 012] lr=0.00207 train_loss=1.1207 acc=0.7363 val_loss=1.0873 acc=0.7428\n",
      "[Epoch 013] lr=0.00207 train_loss=1.1013 acc=0.7444 val_loss=1.0804 acc=0.7510\n",
      "[Epoch 014] lr=0.00208 train_loss=1.0892 acc=0.7506 val_loss=1.0602 acc=0.7580\n",
      "[Epoch 015] lr=0.00209 train_loss=1.0709 acc=0.7589 val_loss=1.0525 acc=0.7648\n",
      "[Epoch 016] lr=0.00209 train_loss=1.0599 acc=0.7652 val_loss=1.0343 acc=0.7722\n",
      "[Epoch 017] lr=0.00210 train_loss=1.0441 acc=0.7714 val_loss=1.0392 acc=0.7674\n",
      "[Epoch 018] lr=0.00210 train_loss=1.0342 acc=0.7794 val_loss=1.0248 acc=0.7818\n",
      "[Epoch 019] lr=0.00211 train_loss=1.0233 acc=0.7852 val_loss=1.0010 acc=0.7884\n",
      "[Epoch 020] lr=0.00211 train_loss=1.0096 acc=0.7908 val_loss=0.9975 acc=0.7848\n",
      "[Epoch 021] lr=0.00212 train_loss=0.9994 acc=0.7942 val_loss=0.9936 acc=0.7964\n",
      "[Epoch 022] lr=0.00213 train_loss=0.9918 acc=0.8011 val_loss=0.9991 acc=0.7868\n",
      "[Epoch 023] lr=0.00213 train_loss=0.9831 acc=0.8031 val_loss=0.9755 acc=0.7994\n",
      "[Epoch 024] lr=0.00214 train_loss=0.9742 acc=0.8080 val_loss=0.9711 acc=0.7984\n",
      "[Epoch 025] lr=0.00214 train_loss=0.9632 acc=0.8140 val_loss=0.9746 acc=0.8034\n",
      "[Epoch 026] lr=0.00215 train_loss=0.9561 acc=0.8160 val_loss=0.9637 acc=0.8064\n",
      "[Epoch 027] lr=0.00215 train_loss=0.9471 acc=0.8201 val_loss=0.9487 acc=0.8086\n",
      "[Epoch 028] lr=0.00216 train_loss=0.9393 acc=0.8259 val_loss=0.9313 acc=0.8194\n",
      "[Epoch 029] lr=0.00216 train_loss=0.9342 acc=0.8272 val_loss=0.9352 acc=0.8180\n",
      "[Epoch 030] lr=0.00217 train_loss=0.9253 acc=0.8317 val_loss=0.9292 acc=0.8186\n",
      "[Epoch 031] lr=0.00218 train_loss=0.9199 acc=0.8320 val_loss=0.9255 acc=0.8158\n",
      "[Epoch 032] lr=0.00218 train_loss=0.9125 acc=0.8378 val_loss=0.9305 acc=0.8214\n",
      "[Epoch 033] lr=0.00219 train_loss=0.9071 acc=0.8391 val_loss=0.9123 acc=0.8300\n",
      "[Epoch 034] lr=0.00219 train_loss=0.9010 acc=0.8406 val_loss=0.9308 acc=0.8248\n",
      "[Epoch 035] lr=0.00220 train_loss=0.8953 acc=0.8471 val_loss=0.9139 acc=0.8308\n",
      "[Epoch 036] lr=0.00220 train_loss=0.8887 acc=0.8482 val_loss=0.9176 acc=0.8272\n",
      "[Epoch 037] lr=0.00221 train_loss=0.8852 acc=0.8501 val_loss=0.9020 acc=0.8318\n",
      "[Epoch 038] lr=0.00222 train_loss=0.8782 acc=0.8542 val_loss=0.9045 acc=0.8306\n",
      "[Epoch 039] lr=0.00222 train_loss=0.8723 acc=0.8561 val_loss=0.8909 acc=0.8360\n",
      "[Epoch 040] lr=0.00223 train_loss=0.8674 acc=0.8582 val_loss=0.8895 acc=0.8406\n",
      "[Epoch 041] lr=0.00223 train_loss=0.8665 acc=0.8580 val_loss=0.8928 acc=0.8396\n",
      "[Epoch 042] lr=0.00224 train_loss=0.8586 acc=0.8614 val_loss=0.8886 acc=0.8390\n",
      "[Epoch 043] lr=0.00224 train_loss=0.8512 acc=0.8650 val_loss=0.8885 acc=0.8376\n",
      "[Epoch 044] lr=0.00225 train_loss=0.8499 acc=0.8643 val_loss=0.8773 acc=0.8430\n",
      "[Epoch 045] lr=0.00226 train_loss=0.8462 acc=0.8677 val_loss=0.8759 acc=0.8444\n",
      "[Epoch 046] lr=0.00226 train_loss=0.8402 acc=0.8710 val_loss=0.8793 acc=0.8426\n",
      "[Epoch 047] lr=0.00227 train_loss=0.8360 acc=0.8740 val_loss=0.8833 acc=0.8408\n",
      "[Epoch 048] lr=0.00227 train_loss=0.8338 acc=0.8732 val_loss=0.8709 acc=0.8470\n",
      "[Epoch 049] lr=0.00228 train_loss=0.8305 acc=0.8756 val_loss=0.8688 acc=0.8514\n",
      "[Epoch 050] lr=0.00228 train_loss=0.8267 acc=0.8766 val_loss=0.8585 acc=0.8528\n",
      "[Epoch 051] lr=0.00229 train_loss=0.8225 acc=0.8807 val_loss=0.8660 acc=0.8486\n",
      "[Epoch 052] lr=0.00230 train_loss=0.8150 acc=0.8824 val_loss=0.8625 acc=0.8496\n",
      "[Epoch 053] lr=0.00230 train_loss=0.8156 acc=0.8810 val_loss=0.8671 acc=0.8474\n",
      "[Epoch 054] lr=0.00231 train_loss=0.8096 acc=0.8847 val_loss=0.8573 acc=0.8508\n",
      "[Epoch 055] lr=0.00231 train_loss=0.8070 acc=0.8864 val_loss=0.8653 acc=0.8472\n",
      "[Epoch 056] lr=0.00232 train_loss=0.8013 acc=0.8890 val_loss=0.8436 acc=0.8546\n",
      "[Epoch 057] lr=0.00232 train_loss=0.7993 acc=0.8891 val_loss=0.8475 acc=0.8592\n",
      "[Epoch 058] lr=0.00233 train_loss=0.7941 acc=0.8919 val_loss=0.8573 acc=0.8554\n",
      "[Epoch 059] lr=0.00234 train_loss=0.7948 acc=0.8914 val_loss=0.8538 acc=0.8532\n",
      "[Epoch 060] lr=0.00234 train_loss=0.7940 acc=0.8927 val_loss=0.8556 acc=0.8564\n",
      "[Epoch 061] lr=0.00235 train_loss=0.7881 acc=0.8956 val_loss=0.8269 acc=0.8680\n",
      "[Epoch 062] lr=0.00235 train_loss=0.7842 acc=0.8965 val_loss=0.8276 acc=0.8646\n",
      "[Epoch 063] lr=0.00236 train_loss=0.7822 acc=0.8982 val_loss=0.8356 acc=0.8620\n",
      "[Epoch 064] lr=0.00236 train_loss=0.7791 acc=0.8986 val_loss=0.8439 acc=0.8612\n",
      "[Epoch 065] lr=0.00237 train_loss=0.7750 acc=0.9006 val_loss=0.8302 acc=0.8660\n",
      "[Epoch 066] lr=0.00237 train_loss=0.7732 acc=0.9024 val_loss=0.8495 acc=0.8590\n",
      "[Epoch 067] lr=0.00238 train_loss=0.7671 acc=0.9066 val_loss=0.8415 acc=0.8608\n",
      "[Epoch 068] lr=0.00239 train_loss=0.7669 acc=0.9051 val_loss=0.8279 acc=0.8668\n",
      "[Epoch 069] lr=0.00239 train_loss=0.7618 acc=0.9075 val_loss=0.8278 acc=0.8662\n",
      "[Epoch 070] lr=0.00240 train_loss=0.7612 acc=0.9060 val_loss=0.8223 acc=0.8730\n",
      "[Epoch 071] lr=0.00240 train_loss=0.7586 acc=0.9091 val_loss=0.8230 acc=0.8658\n",
      "[Epoch 072] lr=0.00241 train_loss=0.7568 acc=0.9098 val_loss=0.8354 acc=0.8620\n",
      "[Epoch 073] lr=0.00241 train_loss=0.7536 acc=0.9115 val_loss=0.8167 acc=0.8686\n",
      "[Epoch 074] lr=0.00242 train_loss=0.7523 acc=0.9114 val_loss=0.8264 acc=0.8656\n",
      "[Epoch 075] lr=0.00243 train_loss=0.7481 acc=0.9137 val_loss=0.8276 acc=0.8658\n",
      "[Epoch 076] lr=0.00243 train_loss=0.7470 acc=0.9146 val_loss=0.8109 acc=0.8764\n",
      "[Epoch 077] lr=0.00244 train_loss=0.7442 acc=0.9148 val_loss=0.8255 acc=0.8688\n",
      "[Epoch 078] lr=0.00244 train_loss=0.7408 acc=0.9159 val_loss=0.8521 acc=0.8546\n",
      "[Epoch 079] lr=0.00245 train_loss=0.7408 acc=0.9173 val_loss=0.8175 acc=0.8690\n",
      "[Epoch 080] lr=0.00245 train_loss=0.7375 acc=0.9193 val_loss=0.8339 acc=0.8654\n",
      "[Epoch 081] lr=0.00246 train_loss=0.7336 acc=0.9201 val_loss=0.8070 acc=0.8798\n",
      "[Epoch 082] lr=0.00247 train_loss=0.7313 acc=0.9226 val_loss=0.8131 acc=0.8684\n",
      "[Epoch 083] lr=0.00247 train_loss=0.7295 acc=0.9218 val_loss=0.8123 acc=0.8686\n",
      "[Epoch 084] lr=0.00248 train_loss=0.7277 acc=0.9232 val_loss=0.8487 acc=0.8540\n",
      "[Epoch 085] lr=0.00248 train_loss=0.7252 acc=0.9245 val_loss=0.8077 acc=0.8706\n",
      "[Epoch 086] lr=0.00249 train_loss=0.7247 acc=0.9258 val_loss=0.8049 acc=0.8740\n",
      "[Epoch 087] lr=0.00249 train_loss=0.7229 acc=0.9250 val_loss=0.8297 acc=0.8644\n",
      "[Epoch 088] lr=0.00250 train_loss=0.7187 acc=0.9275 val_loss=0.8110 acc=0.8748\n",
      "[Epoch 089] lr=0.00251 train_loss=0.7194 acc=0.9275 val_loss=0.8257 acc=0.8692\n",
      "[Epoch 090] lr=0.00251 train_loss=0.7163 acc=0.9294 val_loss=0.8083 acc=0.8744\n",
      "[Epoch 091] lr=0.00252 train_loss=0.7124 acc=0.9306 val_loss=0.8147 acc=0.8724\n",
      "[Epoch 092] lr=0.00252 train_loss=0.7110 acc=0.9314 val_loss=0.8299 acc=0.8622\n",
      "[Epoch 093] lr=0.00253 train_loss=0.7081 acc=0.9322 val_loss=0.8051 acc=0.8706\n",
      "[Epoch 094] lr=0.00253 train_loss=0.7070 acc=0.9324 val_loss=0.8162 acc=0.8732\n",
      "[Epoch 095] lr=0.00254 train_loss=0.7055 acc=0.9336 val_loss=0.8040 acc=0.8766\n",
      "[Epoch 096] lr=0.00255 train_loss=0.7036 acc=0.9334 val_loss=0.8073 acc=0.8742\n",
      "[Epoch 097] lr=0.00255 train_loss=0.7044 acc=0.9336 val_loss=0.8037 acc=0.8758\n",
      "[Epoch 098] lr=0.00256 train_loss=0.6999 acc=0.9355 val_loss=0.8032 acc=0.8780\n",
      "[Epoch 099] lr=0.00256 train_loss=0.6977 acc=0.9369 val_loss=0.8053 acc=0.8736\n",
      "[Epoch 100] lr=0.00257 train_loss=0.6944 acc=0.9396 val_loss=0.8198 acc=0.8676\n",
      "[Epoch 101] lr=0.00257 train_loss=0.6921 acc=0.9394 val_loss=0.8061 acc=0.8734\n",
      "[Epoch 102] lr=0.00258 train_loss=0.6942 acc=0.9394 val_loss=0.8063 acc=0.8742\n",
      "[Epoch 103] lr=0.00259 train_loss=0.6924 acc=0.9388 val_loss=0.7871 acc=0.8808\n",
      "[Epoch 104] lr=0.00259 train_loss=0.6884 acc=0.9413 val_loss=0.8233 acc=0.8644\n",
      "[Epoch 105] lr=0.00260 train_loss=0.6851 acc=0.9425 val_loss=0.7981 acc=0.8788\n",
      "[Epoch 106] lr=0.00260 train_loss=0.6870 acc=0.9424 val_loss=0.8062 acc=0.8766\n",
      "[Epoch 107] lr=0.00261 train_loss=0.6841 acc=0.9432 val_loss=0.8158 acc=0.8698\n",
      "[Epoch 108] lr=0.00261 train_loss=0.6846 acc=0.9417 val_loss=0.8153 acc=0.8702\n",
      "[Epoch 109] lr=0.00262 train_loss=0.6817 acc=0.9439 val_loss=0.7886 acc=0.8844\n",
      "[Epoch 110] lr=0.00263 train_loss=0.6819 acc=0.9443 val_loss=0.8090 acc=0.8722\n",
      "[Epoch 111] lr=0.00263 train_loss=0.6785 acc=0.9460 val_loss=0.8125 acc=0.8678\n",
      "[Epoch 112] lr=0.00264 train_loss=0.6786 acc=0.9449 val_loss=0.7981 acc=0.8788\n",
      "[Epoch 113] lr=0.00264 train_loss=0.6769 acc=0.9454 val_loss=0.7969 acc=0.8778\n",
      "[Epoch 114] lr=0.00265 train_loss=0.6754 acc=0.9480 val_loss=0.7837 acc=0.8834\n",
      "[Epoch 115] lr=0.00265 train_loss=0.6722 acc=0.9479 val_loss=0.8032 acc=0.8764\n",
      "[Epoch 116] lr=0.00266 train_loss=0.6703 acc=0.9500 val_loss=0.7923 acc=0.8854\n",
      "[Epoch 117] lr=0.00266 train_loss=0.6701 acc=0.9483 val_loss=0.8053 acc=0.8762\n",
      "[Epoch 118] lr=0.00267 train_loss=0.6699 acc=0.9477 val_loss=0.7947 acc=0.8814\n",
      "[Epoch 119] lr=0.00268 train_loss=0.6660 acc=0.9513 val_loss=0.8033 acc=0.8770\n",
      "[Epoch 120] lr=0.00268 train_loss=0.6667 acc=0.9504 val_loss=0.8000 acc=0.8766\n",
      "[Epoch 121] lr=0.00269 train_loss=0.6665 acc=0.9497 val_loss=0.7946 acc=0.8746\n",
      "[Epoch 122] lr=0.00269 train_loss=0.6636 acc=0.9520 val_loss=0.7870 acc=0.8874\n",
      "[Epoch 123] lr=0.00270 train_loss=0.6623 acc=0.9522 val_loss=0.7823 acc=0.8860\n",
      "[Epoch 124] lr=0.00270 train_loss=0.6623 acc=0.9515 val_loss=0.7958 acc=0.8764\n",
      "[Epoch 125] lr=0.00271 train_loss=0.6597 acc=0.9534 val_loss=0.7835 acc=0.8874\n",
      "[Epoch 126] lr=0.00272 train_loss=0.6587 acc=0.9543 val_loss=0.7945 acc=0.8804\n",
      "[Epoch 127] lr=0.00272 train_loss=0.6563 acc=0.9556 val_loss=0.7850 acc=0.8820\n",
      "[Epoch 128] lr=0.00273 train_loss=0.6569 acc=0.9558 val_loss=0.7975 acc=0.8742\n",
      "[Epoch 129] lr=0.00273 train_loss=0.6574 acc=0.9533 val_loss=0.7858 acc=0.8788\n",
      "[Epoch 130] lr=0.00274 train_loss=0.6530 acc=0.9573 val_loss=0.7882 acc=0.8836\n",
      "[Epoch 131] lr=0.00274 train_loss=0.6525 acc=0.9563 val_loss=0.7858 acc=0.8846\n",
      "[Epoch 132] lr=0.00275 train_loss=0.6515 acc=0.9568 val_loss=0.7957 acc=0.8764\n",
      "[Epoch 133] lr=0.00276 train_loss=0.6525 acc=0.9575 val_loss=0.7901 acc=0.8874\n",
      "[Epoch 134] lr=0.00276 train_loss=0.6492 acc=0.9579 val_loss=0.7933 acc=0.8812\n",
      "[Epoch 135] lr=0.00277 train_loss=0.6505 acc=0.9552 val_loss=0.7841 acc=0.8856\n",
      "[Epoch 136] lr=0.00277 train_loss=0.6469 acc=0.9608 val_loss=0.7726 acc=0.8862\n",
      "[Epoch 137] lr=0.00278 train_loss=0.6438 acc=0.9604 val_loss=0.7907 acc=0.8796\n",
      "[Epoch 138] lr=0.00278 train_loss=0.6444 acc=0.9602 val_loss=0.7929 acc=0.8806\n",
      "[Epoch 139] lr=0.00279 train_loss=0.6428 acc=0.9611 val_loss=0.7795 acc=0.8824\n",
      "[Epoch 140] lr=0.00280 train_loss=0.6419 acc=0.9613 val_loss=0.7871 acc=0.8800\n",
      "[Epoch 141] lr=0.00280 train_loss=0.6412 acc=0.9617 val_loss=0.7760 acc=0.8854\n",
      "[Epoch 142] lr=0.00281 train_loss=0.6415 acc=0.9608 val_loss=0.7908 acc=0.8828\n",
      "[Epoch 143] lr=0.00281 train_loss=0.6402 acc=0.9620 val_loss=0.8024 acc=0.8790\n",
      "[Epoch 144] lr=0.00282 train_loss=0.6386 acc=0.9636 val_loss=0.7975 acc=0.8788\n",
      "[Epoch 145] lr=0.00282 train_loss=0.6378 acc=0.9631 val_loss=0.7832 acc=0.8840\n",
      "[Epoch 146] lr=0.00283 train_loss=0.6369 acc=0.9631 val_loss=0.7689 acc=0.8924\n",
      "[Epoch 147] lr=0.00284 train_loss=0.6370 acc=0.9639 val_loss=0.8046 acc=0.8768\n",
      "[Epoch 148] lr=0.00284 train_loss=0.6349 acc=0.9646 val_loss=0.7699 acc=0.8916\n",
      "[Epoch 149] lr=0.00285 train_loss=0.6337 acc=0.9639 val_loss=0.7804 acc=0.8860\n",
      "[Epoch 150] lr=0.00285 train_loss=0.6344 acc=0.9642 val_loss=0.7827 acc=0.8838\n",
      "[Epoch 151] lr=0.00286 train_loss=0.6303 acc=0.9665 val_loss=0.7948 acc=0.8798\n",
      "[Epoch 152] lr=0.00286 train_loss=0.6304 acc=0.9662 val_loss=0.7783 acc=0.8856\n",
      "[Epoch 153] lr=0.00287 train_loss=0.6305 acc=0.9664 val_loss=0.7905 acc=0.8856\n",
      "[Epoch 154] lr=0.00287 train_loss=0.6286 acc=0.9671 val_loss=0.7827 acc=0.8872\n",
      "[Epoch 155] lr=0.00288 train_loss=0.6283 acc=0.9668 val_loss=0.7708 acc=0.8888\n",
      "[Epoch 156] lr=0.00289 train_loss=0.6285 acc=0.9663 val_loss=0.7705 acc=0.8900\n",
      "[Epoch 157] lr=0.00289 train_loss=0.6284 acc=0.9664 val_loss=0.7791 acc=0.8840\n",
      "[Epoch 158] lr=0.00290 train_loss=0.6232 acc=0.9697 val_loss=0.7833 acc=0.8882\n",
      "[Epoch 159] lr=0.00290 train_loss=0.6264 acc=0.9670 val_loss=0.7755 acc=0.8820\n",
      "[Epoch 160] lr=0.00291 train_loss=0.6246 acc=0.9683 val_loss=0.7907 acc=0.8836\n",
      "[Epoch 161] lr=0.00291 train_loss=0.6232 acc=0.9694 val_loss=0.7757 acc=0.8874\n",
      "[Epoch 162] lr=0.00292 train_loss=0.6236 acc=0.9682 val_loss=0.7669 acc=0.8892\n",
      "[Epoch 163] lr=0.00293 train_loss=0.6228 acc=0.9684 val_loss=0.7837 acc=0.8828\n",
      "[Epoch 164] lr=0.00293 train_loss=0.6215 acc=0.9695 val_loss=0.7806 acc=0.8878\n",
      "[Epoch 165] lr=0.00294 train_loss=0.6204 acc=0.9712 val_loss=0.7888 acc=0.8848\n",
      "[Epoch 166] lr=0.00294 train_loss=0.6184 acc=0.9706 val_loss=0.7654 acc=0.8882\n",
      "[Epoch 167] lr=0.00295 train_loss=0.6191 acc=0.9706 val_loss=0.7681 acc=0.8934\n",
      "[Epoch 168] lr=0.00295 train_loss=0.6210 acc=0.9693 val_loss=0.7784 acc=0.8904\n",
      "[Epoch 169] lr=0.00296 train_loss=0.6180 acc=0.9716 val_loss=0.7762 acc=0.8850\n",
      "[Epoch 170] lr=0.00297 train_loss=0.6171 acc=0.9717 val_loss=0.7760 acc=0.8890\n",
      "[Epoch 171] lr=0.00297 train_loss=0.6164 acc=0.9723 val_loss=0.7818 acc=0.8858\n",
      "[Epoch 172] lr=0.00298 train_loss=0.6159 acc=0.9718 val_loss=0.7747 acc=0.8872\n",
      "[Epoch 173] lr=0.00298 train_loss=0.6156 acc=0.9715 val_loss=0.7841 acc=0.8876\n",
      "[Epoch 174] lr=0.00299 train_loss=0.6157 acc=0.9714 val_loss=0.7582 acc=0.8928\n",
      "[Epoch 175] lr=0.00299 train_loss=0.6157 acc=0.9717 val_loss=0.7707 acc=0.8886\n",
      "[Epoch 176] lr=0.00300 train_loss=0.6141 acc=0.9719 val_loss=0.7810 acc=0.8876\n",
      "[Epoch 177] lr=0.00301 train_loss=0.6133 acc=0.9720 val_loss=0.7794 acc=0.8932\n",
      "[Epoch 178] lr=0.00301 train_loss=0.6135 acc=0.9725 val_loss=0.7744 acc=0.8912\n",
      "[Epoch 179] lr=0.00302 train_loss=0.6118 acc=0.9733 val_loss=0.7730 acc=0.8864\n",
      "[Epoch 180] lr=0.00302 train_loss=0.6125 acc=0.9727 val_loss=0.7848 acc=0.8824\n",
      "[Epoch 181] lr=0.00303 train_loss=0.6110 acc=0.9734 val_loss=0.7739 acc=0.8898\n",
      "[Epoch 182] lr=0.00303 train_loss=0.6100 acc=0.9742 val_loss=0.7574 acc=0.8930\n",
      "[Epoch 183] lr=0.00304 train_loss=0.6111 acc=0.9730 val_loss=0.7792 acc=0.8896\n",
      "[Epoch 184] lr=0.00305 train_loss=0.6099 acc=0.9731 val_loss=0.7765 acc=0.8884\n",
      "[Epoch 185] lr=0.00305 train_loss=0.6070 acc=0.9755 val_loss=0.7656 acc=0.8900\n",
      "[Epoch 186] lr=0.00306 train_loss=0.6076 acc=0.9752 val_loss=0.7710 acc=0.8856\n",
      "[Epoch 187] lr=0.00306 train_loss=0.6060 acc=0.9754 val_loss=0.7701 acc=0.8882\n",
      "[Epoch 188] lr=0.00307 train_loss=0.6070 acc=0.9758 val_loss=0.7730 acc=0.8890\n",
      "[Epoch 189] lr=0.00307 train_loss=0.6070 acc=0.9749 val_loss=0.7641 acc=0.8916\n",
      "[Epoch 190] lr=0.00308 train_loss=0.6059 acc=0.9754 val_loss=0.7811 acc=0.8858\n",
      "[Epoch 191] lr=0.00309 train_loss=0.6064 acc=0.9747 val_loss=0.7768 acc=0.8888\n",
      "[Epoch 192] lr=0.00309 train_loss=0.6050 acc=0.9750 val_loss=0.7583 acc=0.8984\n",
      "[Epoch 193] lr=0.00310 train_loss=0.6043 acc=0.9761 val_loss=0.7718 acc=0.8896\n",
      "[Epoch 194] lr=0.00310 train_loss=0.6027 acc=0.9765 val_loss=0.7762 acc=0.8860\n",
      "[Epoch 195] lr=0.00311 train_loss=0.6032 acc=0.9760 val_loss=0.7667 acc=0.8904\n",
      "[Epoch 196] lr=0.00311 train_loss=0.6028 acc=0.9757 val_loss=0.7725 acc=0.8876\n",
      "[Epoch 197] lr=0.00312 train_loss=0.6015 acc=0.9772 val_loss=0.7664 acc=0.8908\n",
      "[Epoch 198] lr=0.00313 train_loss=0.6024 acc=0.9765 val_loss=0.7825 acc=0.8866\n",
      "[Epoch 199] lr=0.00313 train_loss=0.6021 acc=0.9767 val_loss=0.7793 acc=0.8794\n",
      "[Epoch 200] lr=0.00314 train_loss=0.6003 acc=0.9777 val_loss=0.7594 acc=0.8942\n",
      "FINAL TEST: loss=0.7482  top1_acc=0.9026\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣟\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▂▂▂▂▂▂▂▂▃▃▃▃▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▆▆▆▆▆▇████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         lr ▁▁▁▁▁▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇█████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  train_acc ▁▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇███████████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▆▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    val_acc ▁▂▄▅▆▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇███████████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss █▆▄▄▄▃▃▂▂▂▂▂▂▂▂▂▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        best_val_acc 0.8984\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               epoch 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: final_test_top1_acc 0.9026\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  lr 0.00314\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           train_acc 0.97773\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          train_loss 0.60029\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              use_bn True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             val_acc 0.8942\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            val_loss 0.75943\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mglamorous-darkness-6\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment/runs/evkbzcv6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cs24m505-iitmaana/vgg6-cifar10-assignment\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251016_103459-evkbzcv6/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m vgg6_cifar.scripts.train_experiment \\\n",
    "    --data_dir      ./data \\\n",
    "    --out_dir       ./runs/final_best_test3 \\\n",
    "    --activation    gelu \\\n",
    "    --optimizer     nesterov-sgd \\\n",
    "    --lr            0.01 \\\n",
    "    --batch_size    128 \\\n",
    "    --epochs        200 \\\n",
    "    --momentum      0.9 \\\n",
    "--weight_decay 5e-4 --label_smoothing 0.1 \\\n",
    "    --use_bn \\\n",
    "    --aug_hflip \\\n",
    "    --aug_crop \\\n",
    "    --aug_cutout \\\n",
    "    --aug_jitter \\\n",
    "    --amp \\\n",
    "    --wandb \\\n",
    "    --seed 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d94dfb1-b141-4d0e-8667-95effba9f60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a0f81d7-6051-4381-bba0-0d8c97a00c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vgg6_cifar.models.vgg6 import VGG6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fb1058df-225d-4806-898b-5292a0c22934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = VGG6(num_classes=10)\n",
    "checkpoint = torch.load('./runs/final_best_test2/best.pt')\n",
    "model.load_state_dict(checkpoint['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a09fab92-45c5-40dd-a4c9-e8d105caef40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG6(\n",
       "  (features): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Dropout(p=0.3, inplace=False)\n",
       "    (2): Linear(in_features=4096, out_features=256, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Dropout(p=0.3, inplace=False)\n",
       "    (5): Linear(in_features=256, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ce5a4349-d7ca-4016-83a8-3d109e3c4345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # Add normalization if used during training\n",
    "])\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0239d10d-96b9-41e4-a780-dc7ccc0b7a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 88.63%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Test Accuracy: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc7fa43-a50a-4532-8d97-b0fd3137292d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
